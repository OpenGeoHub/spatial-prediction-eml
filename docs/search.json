[{"path":"index.html","id":"introduction","chapter":"Introduction","heading":"Introduction","text":"","code":""},{"path":"index.html","id":"ensemble-machine-learning","chapter":"Introduction","heading":"Ensemble Machine Learning","text":" Rmarkdown tutorial provides practical instructions, illustrated sample\ndataset, use Ensemble Machine Learning generate predictions (maps) \n2D, 3D, 2D+T (spatiotemporal) training (point) datasets. show functionality \nautomated benchmarking spatial/spatiotemporal prediction problems, \nuse primarily mlr framework spatial packages terra, rgdal similar..Ensembles predictive models combine predictions two learners\n(Seni & Elder, 2010; C. Zhang & Ma, 2012). specific benefits using Ensemble learners :Performance: can help improve average prediction performance individual contributing learner ensemble.Robustness: can help reduce extrapolation / overshooting effects individual learners.Unbiasness: can help determine model-free estimate prediction errors.Even flexible best performing learners Random Forest neural\nnetworks always carry bias sense fitting produces recognizable\npatterns limited properties algorithm. case \nensembles, modeling algorithm becomes secondary, even though improvements\naccuracy often minor compared best individual learner, \ngood chance final EML model less prone overshooting \nextrapolation problems.principle three ways apply ensembles (C. Zhang & Ma, 2012):bagging: learn parallel, combine using deterministic principle (e.g. weighted averaging),boosting: learn sequentially adaptive way, combine using deterministic principle,stacking: learn parallel, fit meta-model predict ensemble estimates,“meta-model” additional model basically combines individual\n“base learners”. tutorial focus stacking approach Ensemble ML.several packages R implement Ensemble ML, example:SuperLearner package,caretEnsemble package,h2o.stackedEnsemble package,mlr mlr3 packages,Ensemble ML also available Python scikit-learn library.tutorial focus primarily using mlr package,\n.e. wrapper functions mlr implemented landmap package.","code":""},{"path":"index.html","id":"using-geographical-distances-to-improve-spatial-interpolation","chapter":"Introduction","heading":"Using geographical distances to improve spatial interpolation","text":"Machine Learning long time considered suboptimal spatial\ninterpolation problems, comparison classical geostatistical techniques\nkriging, basically ignores spatial dependence structure \ndata. incorporate spatial dependence structures machine learning, one\ncan now add -called “geographical features”: buffer distance, oblique\ndistances, /distances watershed, features. shown \nimprove prediction performance produce maps visually appear \nproduced kriging (T. Hengl, Nussbaum, Wright, Heuvelink, & Gräler, 2018).Use geographical features machine learning spatial predictions explained detail :Behrens, T., Schmidt, K., Viscarra Rossel, R. ., Gries, P., Scholten, T., & MacMillan, R. . (2018). Spatial modelling Euclidean distance fields machine learning. European journal soil science, 69(5), 757-770.Hengl, T., Nussbaum, M., Wright, M. N., Heuvelink, G. B., & Gräler, B. (2018). Random forest generic framework predictive modeling spatial spatio-temporal variables. PeerJ, 6, e5518. https://doi.org/10.7717/peerj.5518Møller, . B., Beucher, . M., Pouladi, N., Greve, M. H. (2020). Oblique geographic coordinates covariates digital soil mapping. SOIL, 6, 269–289, https://doi.org/10.5194/soil-6-269-2020Sekulić, ., Kilibarda, M., Heuvelink, G.B., Nikolić, M., Bajat, B. (2020). Random Forest Spatial Interpolation. Remote Sens. 12, 1687. https://doi.org/10.3390/rs12101687In case number covariates / features becomes large, assuming \ncovariates diverse, points equally spread area \ninterest, probably need using geographical distances model\ntraining unique combinations features become large can\nused represent geographical position (T. Hengl et al., 2018).","code":""},{"path":"index.html","id":"installing-the-landmap-package","chapter":"Introduction","heading":"Installing the landmap package","text":"install recent landmap package Github use:","code":"\nlibrary(devtools)\ninstall_github(\"envirometrix/landmap\")"},{"path":"index.html","id":"important-literature","chapter":"Introduction","heading":"Important literature","text":"introduction Spatial Data Science Machine Learning R \nrecommend studying first:Becker, M. et al.: “mlr3 book”;Bivand, R., Pebesma, E. Gómez-Rubio, V.: “Applied Spatial Data Analysis R”;Irizarry, R..: “Introduction Data Science: Data Analysis Prediction Algorithms R”;Kuhn, M.: “caret package”;Molnar, C.: “Interpretable Machine Learning: Guide Making Black Box Models Explainable”;Lovelace, R., Nowosad, J. Muenchow, J.: “Geocomputation R”;introduction Predictive Soil Mapping using R refer https://soilmapper.org.Machine Learning python resampling can best implemented via \nscikit-learn library, matches \nfunctionality available via mlr package R.","code":""},{"path":"index.html","id":"license","chapter":"Introduction","heading":"License","text":"work licensed Creative Commons Attribution-ShareAlike 4.0 International License.","code":""},{"path":"index.html","id":"acknowledgements","chapter":"Introduction","heading":"Acknowledgements","text":" tutorial based “R Data Science”\nbook Hadley Wickham contributors.OpenLandMap collaborative effort many people\ncontributed data, software, fixes improvements via pull request.OpenGeoHub independent --profit research\nfoundation promoting Open Source Open Data solutions. tools developed\nprimarily need Geo-harmonizer project enable creation \nnext-generation environmental layers continental Europe (Bonannella et al., 2022?; Witjes et al., 2022?).\nEnvirometriX Ltd. commercial branch group\nresponsible designing soil sampling designs AgriCapture\nsimilar soil monitoring projects.OpenDataScience.eu project co-financed European Union (CEF Telecom project 2018-EU-IA-0095).","code":""},{"path":"introduction-to-spatial-and-spatiotemporal-data.html","id":"introduction-to-spatial-and-spatiotemporal-data","chapter":"1 Introduction to spatial and spatiotemporal data","heading":"1 Introduction to spatial and spatiotemporal data","text":"reading work--progress Spatial spatiotemporal interpolation using Ensemble Machine Learning. chapter currently draft version, peer-review publication pending. can find polished first edition https://opengeohub.github.io/spatial-prediction-eml/.","code":""},{"path":"introduction-to-spatial-and-spatiotemporal-data.html","id":"spatial-data-and-spatial-interpolation","chapter":"1 Introduction to spatial and spatiotemporal data","heading":"1.1 Spatial data and spatial interpolation","text":"Spatial /geospatial data data spatially referenced (\ncase geographical data referenced Earth surface) .e. \\(X\\) \\(Y\\) coordinates\nknown. implementation GPS Earth Observation technology,\nalmost everything becoming spatial data hence tools Geographical\nInformation Systems (GIS) spatial analysis tools process, analyze \nvisualize geospatial data becoming essential.Spatial interpolation /Spatial Prediction process estimating values\ntarget variable whole area interest using input training\npoint data, algorithm values covariates new locations (Mitas & Mitasova, 1999).\nInterpolation results images maps, can used decision making similar.\ndifference interpolation prediction: prediction\ncan imply interpolation extrapolation. commonly use \nterm spatial prediction tutorial, even though term spatial interpolation\nwidely accepted (Mitas & Mitasova, 1999). geostatistics, e.g. \ncase ordinary kriging, interpolation corresponds cases \nlocation estimated surrounded sampling locations within\nspatial auto-correlation range (Brown, 2015; Diggle & Ribeiro Jr, 2007). Prediction outside practical range\n(.e. prediction error exceeds global variance) referred \nspatial extrapolation. words, extrapolation prediction locations\nenough statistical evidence (based statistical model)\nmake significant predictions.","code":""},{"path":"introduction-to-spatial-and-spatiotemporal-data.html","id":"spatial-interpolation-using-ensemble-machine-learning","chapter":"1 Introduction to spatial and spatiotemporal data","heading":"1.2 Spatial interpolation using Ensemble Machine Learning","text":"Ensemble Machine Learning (Ensemble ML) approach modeling , instead using \nsingle best learner, use multiple strong learners combine \npredictive capabilities single union. can lead higher\naccuracy robustness (Seni & Elder, 2010), also helps deriving\nmodel-free estimate prediction errors nonparametric techniques \nbootstrapping (C. Zhang & Ma, 2012). way can help decrease methodological disadvantages \nindividual learners shown previous example synthetic data.Ensemble ML can used fit models generate predictions using points data\nway ordinary kriging used generate interpolations. Ensemble ML\npredictive mapping 2D 3D discussed detail first chapter\ntutorial. Spatiotemporal interpolation using EML (2D+T, 3D+T) \norder magnitude computational (Gasch et al., 2015) follows logic.Ensemble ML based stacking implemented mlr package (Bischl et al., 2016)\ncan initiated via makeStackedLearner function e.g.:base learner predictions computed 5-fold cross-validation\n(repeated re-fitting) used determine meta-learner. algorithm\nknown “Super Learner” algorithm (Polley & van der Laan, 2010).case spatial prediction, also want block training points based \nspatial proximity prevent producing bias predictions. \nknow range spatial dependence model residuals similar .e. \nsomething can derived fitting variogram, limit minimum\nspatial distance training validation points avoid overfitting \nsimilar (info refer Spatial Sampling tutorial).automate fitting Ensemble Machine Learning models purpose \nspatial interpolation / prediction, one can now use landmap package combines:derivation geographical distances,conversion grids principal components,automated filling gaps gridded data,automated fitting variogram determination spatial auto-correlation structure,spatial overlay,model training using spatial Cross-Validation (Lovelace, Nowosad, & Muenchow, 2019),model stacking .e. fitting final EML,concept automating spatial interpolation level almost \nhuman interaction required referred “automated mapping” automated\nspatial interpolation (Pebesma et al., 2011).","code":"\nm = mlr::makeStackedLearner(base.learners = lrns, \n              super.learner = \"regr.ml\", method = \"stack.cv\")"},{"path":"introduction-to-spatial-and-spatiotemporal-data.html","id":"spatiotemporal-data","chapter":"1 Introduction to spatial and spatiotemporal data","heading":"1.3 Spatiotemporal data","text":"Spatiotemporal data practically data referenced space \ntime. implies following coordinates known:geographic location (longitude latitude projected \\(X,Y\\) coordinates);spatial location accuracy size block / volume case bulking samples;height ground surface (elevation);start end time measurement (year, month, day, hour, minute etc.);Consider example daily temperature measured meteorological station.\nfollowing coordinates:means measurement fully spatiotemporally referenced \n\\(X,Y\\) location defined, delta.xy location accuracy known, begin \nend time measurement specified (case temporal support 1 day).Analysis spatiotemporal data somewhat different pure spatial\nanalysis. Time course just another spatial dimension .e. \nspecific properties different statistical assumptions methods apply \nspatiotemporal data. introduction spatiotemporal data R please\nrefer spacetime package tutorial (Pebesma, 2012).Conceptually speaking, spatiotemporal datasets corresponding\ndatabases can matched two major groups features (Erwig, Gu, Schneider, & Vazirgiannis, 1999): (1)\nmoving dynamic objects (discrete vector geometries), (2)\ndynamic regions (fields continuous features). Distinct objects (entities)\npeople, animals, vehicles similar best represented using\nvectors trajectories (movement time), fields commonly\nrepresented using gridded structures. case working \nfields, basically map either:dynamic changes quantity density material chemical element,energy flux similar physical measurements,dynamic changes probability occurrence feature object,Spatiotemporal data can best visualized 2D+T plots space-time cubes. One\nexample spacetime cube following plot Fig. 1.1 (T. Hengl, Heuvelink, Perčec-Tadić, & Pebesma, 2012; T. Hengl, Roudier, Beaudette, & Pebesma, 2015).\nFigure 1.1: Space-time cube visualized R: () cloud plot showing location meteorological stations Croatia, (b) illustration spatial temporal support space-time cube.\nplot shows distribution meteorological stations Croatia, \nrepeated measurements time. dataset used \nuse-case examples produce spatiotemporal predictions daily temperatures.","code":"\ntemp = 22\nlat = 44.56123\nlon = 19.27734\ndelta.xy = 30\nbegin.time = \"2013-09-02 00:00:00 CEST\"\nend.time = \"2013-09-03 00:00:00 CEST\""},{"path":"introduction-to-spatial-and-spatiotemporal-data.html","id":"time-series-analysis","chapter":"1 Introduction to spatial and spatiotemporal data","heading":"1.4 Time-series analysis","text":"Field statistics dealing modeling changes variables time,\nincluding predicting values beyond training data (forecasting) \ntime-series analysis. systematic guides run time-series\nanalysis R can found .variable varies time (time-series curves) can often drastically different\nchanges space (spatial patterns). general, one can say , many\nenvironmental variables, variation values time can separated \ncomponents :Long-term component (trend) determined long-term geological \nextraterrestrial processes,Seasonal monthly /daily component (seasonality) determined Earth rotation \nincoming sun radiation,Variation component can due chaotic behavior /\nlocal factors (hence autocorrelated), andPure noise .e. measurement errors similar,\nFigure 1.2: Illustration decomposition time-series : (1) trend, (2) seasonality, (3) random.\nConsider example case land surface temperature. long-term\ncomponent determined variations Earth’s orbit /Sun’s\nenergy output resulting gradual drops rises global mean\ntemperature (glacials interglacials).\nFig. 1.3 shows example global temperature reconstruction proxy data Marcott, Shakun, Clark, & Mix (2013).\nFigure 1.3: Global temperature reconstruction. shows global temperature varies long-term term scale. Graph : Klaus Bitterman.\nSeasonal .e. monthly daily components variation land surface temperature\nalso quite systematic. basically determined Earth’s rotation\nangles Sun relation Earth’s surface. relatively stable\npattern looks like sinusoidal curves similar. plot shows variation\nvalues soil moisture soil temperature one meteo station USA\nacross multiple years (Gasch et al., 2015).\nFigure 1.4: Sensor values five depths (0.3, 0.6, 0.9, 1.2, 1.5 m) one station Cook Agronomy Farm January 2011–January 2014. black line indicates locally fitted splines.\ndata set Fig. 1.4 discussed case\nstudies demonstrate 3D+T spatiotemporal modeling (Gasch et al., 2015).\nsee later, seasonal daily monthly part variation\nsystematic can modeling using latitude, altitude time/day year.","code":""},{"path":"introduction-to-spatial-and-spatiotemporal-data.html","id":"visualizing-spatiotemporal-data","chapter":"1 Introduction to spatial and spatiotemporal data","heading":"1.5 Visualizing spatiotemporal data","text":"Spatial data usually visualized using static interactive maps (see e.g. mapview /tmap package).\nSpatiotemporal data (2D+T) complex visualize 2D data, 3D+T data\ncan even require special software (T. Hengl et al., 2015) \nusers can make seamless interpretation.three possible groups ways visualize spatiotemporal\ndata:Using static images showing trend parameters together \ntime-series plots selected representative point locations.Using time-slices series visualizations \nspatial domain changing time (time-lapses).Using animations interactive plots time-sliders\nallowing users choose speed direction animation.introduction visualizing spatiotemporal time-series data\nrefer Lamigueiro (2014). complex visualization \nspatiotemporal / dynamic geographic features possible using \nhttps://geemap.org/ package (Python package interactive mapping Google Earth Engine, ipyleaflet, ipywidgets).OpenLandMap.org also multiple temporal datasets users can interactive \ntime-dimension using time-slider implemented OpenLayers Geoserver (M. Kilibarda & Protić, 2019).\nFigure 1.5: Visualization land cover change using animation www.OpenLandMap.org.\n","code":""},{"path":"introduction-to-spatial-and-spatiotemporal-data.html","id":"spatiotemporal-interpolation","chapter":"1 Introduction to spatial and spatiotemporal data","heading":"1.6 Spatiotemporal interpolation","text":"Spatiotemporal interpolation /prediction implies point\nsamples used interpolate within spacetime cube. \nobviously assumes enough point measurements available, spread \nspace time. show tutorial Machine Learning can\nused interpolate values within spacetime cube using real case-studies.\nSpatiotemporal interpolation using various kriging methods implemented \ngstat package (Bivand, Pebesma, & Rubio, 2013), addressed tutorial.success spatiotemporal interpolation (terms prediction accuracy),\nkey recognize systematic component variation spacetime, \nusually possible can find relationship target variable \nEO data available time-series covers spacetime cube\ninterest. establish significant relation dynamic target \ndynamic covariates, can use fitted model predict anywhere spacetime cube.-depth discussion spatiotemporal data R please refer \nWikle, Zammit-Mangion, & Cressie (2019). -depth discussion spatial spatiotemporal\nblocking purpose modeling building cross-validation refer \nRoberts et al. (2017).","code":""},{"path":"introduction-to-spatial-and-spatiotemporal-data.html","id":"modeling-seasonal-components","chapter":"1 Introduction to spatial and spatiotemporal data","heading":"1.7 Modeling seasonal components","text":"Seasonality characteristic target variable follow cyclical\npatterns trigonometric functions. repeating patterns can happen\ndifferent time-scales:inter-annually,monthly based season (spring, summer, autumn, winter),daily,hourly .e. day-time night-time patterns,monthly daily seasonal component variation determined Earth’s rotation\nSun’s angle. Milan Kilibarda et al. (2014) shown seasonal\ncomponent e.g. geometric Earth surface minimum maximum daily temperature\ncan modeled, universally anywhere globe, using following formula:day day year, fi latitude, number 18 represents\ncoldest day northern warmest day southern hemisphere,\nelev elevation meter, 0.6 vertical temperature gradient per\n100-m, sign denotes signum function extracts sign real number.formula accounts different seasons southern northern\nhemisphere can basically applied gridded surfaces compute expected\ntemperature given day. simple example min daily temperature :plot function five consecutive years, get something similar \nspline-fitted functions previous plot:\nFigure 1.6: Geometric temperature function plot given latitude.\n","code":"\ntemp.from.geom <- function(fi, day, a=30.419375, \n                           b=-15.539232, elev=0, t.grad=0.6) {\n  f = ifelse(fi==0, 1e-10, fi)\n  costeta = cos( (day-18 )*pi/182.5 + 2^(1-sign(fi) ) *pi) \n  cosfi = cos(fi*pi/180 )\n  A = cosfi\n  B = (1-costeta ) * abs(sin(fi*pi/180 ) )\n  x = a*A + b*B - t.grad * elev / 100\n  return(x)\n}\ntemp.from.geom(fi=52, day=120)\n#> [1] 8.73603\ndays = seq(1:(5*365))\nplot(temp.from.geom(fi=52, day=days))"},{"path":"introduction-to-spatial-and-spatiotemporal-data.html","id":"predictive-mapping-using-spatial-and-spatiotemporal-ml-in-r","chapter":"1 Introduction to spatial and spatiotemporal data","heading":"1.8 Predictive mapping using spatial and spatiotemporal ML in R","text":"Standard spatiotemporal ML predictive mapping typically includes \nfollowing steps (T. Hengl & MacMillan, 2019; T. Hengl et al., 2018):Prepare training (points) data data cube covariates\nideally analysis-ready datacube.Overlay points create regression-matrix.Fine-tune initial model, reduce complexity\nproduce production-ready prediction model.Run mapping accuracy assessment determine prediction uncertainty\nincluding per pixel uncertainty.Generate predictions save maps.Visualize predictions using web-GIS solutions.\nFigure 1.7: General Machine Learning framework recommended predictive mapping vegetation / ecological / soil variables. Assuming full automation modeling, 2nd-round samples can used gradually improve mapping accuracy.\n","code":""},{"path":"introduction-to-spatial-and-spatiotemporal-data.html","id":"extrapolation-and-over-fitting-problems-of-ml-methods","chapter":"1 Introduction to spatial and spatiotemporal data","heading":"1.9 Extrapolation and over-fitting problems of ML methods","text":"Machine Learning defacto become next-generation applied predictive modeling\nframework. ML techniques Random Forest (RF) proven \n-perform vs simple linear statistical methods, especially \ndatasets large, complex target variable follows complex\nrelationship covariates (T. Hengl et al., 2018). Random Forest comes cost\nhowever. four main practical disadvantages RF:Depending data assumptions data, can -fit values\nwithout analyst even noticing .predicts well within feature space enough training\ndata. Extrapolation .e. prediction outside training space can\nlead poor performance (Meyer & Pebesma, 2021).can computationally expensive computational load increasing\nexponentially number covariates.requires quality training data highly sensitive blunders \ntypos data.Read extrapolation problems Random Forest post.following section demonstrate indeed RF can overfit\ndata can serious problems predicting extrapolation\nspace. Consider example following small synthetic dataset assuming\nsimple linear relationship (see original post Dylan Beaudette):fit simple Ordinary Least Square model data get:see model explains 85% variation data \nRMSE estimated model (residual standard error) matches well \nnoise component inserted purpose using rnorm function.fit Random Forest model data get:Next, can estimate prediction errors using method Lu & Hardin (2021),\navailable via forestError package:shows RF estimates higher RMSE linear model. However, \nvisualize two models see indeed RF\nalgorithm seems -fit specific data:\nFigure 1.8: Difference model fits sythetic data: lm vs RF. case know RF (blue line) overfitting -estimating prediction error extrapolation space. Dotted line shows respective 1 std. prediction interval RF.\nRF basically tries fit relationship even pure noise component variation\n(know pure noise generated using rnorm function).\nobvious -fitting want model something purely random.Extrapolation maybe much problem example \nprediction intervals forestError package expressed \nrealistically predictions deviate linear structure \ndata. Assuming , prediction, one eventually collect\nground-truth data RF model , probably show \nprediction error / prediction intervals completely . traditional\nstatisticians consider -narrow -optimistic \nfitted line -fit, hence pipeline -optimistic\nprediction uncertainty can result decision makers -confident,\nleading wrong decisions, consequently making users losing confidence RF.\n-depth discussion extrapolation problems Area Applicability\nMachine Learning models please refer Meyer & Pebesma (2021).possible solution problem , instead using one\nlearners, use multiple learners apply robust cross-validation \nprevents target model -fitting. can implemented efficiently,\nexample, using mlr package (Bischl et al., 2016). can run Ensemble\nModel applying following four steps. First, define task interest\ncombination learners .e. -called base learners:case use basically four different models: RF (ranger),\nlinear model (glm), Gradient boosting (gamboost) Support Vector\nMachine (kvsm). Second, train Ensemble model using stacking approach:results show ranger ksvm basically -perform \nfact significant specific data .e. probably omitted \nmodeling.Note , stacking multiple learners use separate model\n(meta-learner) case simple linear model. use \nsimple model assume non-linear relationships \nalready modeled via complex models ranger, gamboost\n/ksvm.Next, need estimate mapping accuracy prediction errors \nEnsemble predictions. trivial simple derived formulas.\nneed use non-parametric approach basically can computational.\ncomputationally interesting approach first estimate (global) mapping\naccuracy, adjust prediction variance multiple base learners:shows variance learners 10 times smaller \nactual CV variance. , proves many learners try fit\ndata closely variance different base learners often\nsmoothed .Next, can predict values prediction errors new locations:plot results fitting linear model vs EML:\nFigure 1.9: Difference model fits sythetic data: lm vs Ensemble ML.\nplot , see prediction error intervals \nextrapolation space now wider (compare Fig. 1.8), reflects much better\nexpect used forestError package.summary: appears combining linear non-linear tree-based\nmodels Ensemble ML framework helps : decrease -fitting produce\nrealistic predictions uncertainty / prediction intervals. Ensemble ML\nframework correctly identifies linear models important \nrandom forest similar. Hopefully, provides enough evidence convince \nEnsemble ML potentially interesting use generic solution \nspatial spatiotemporal interpolation extrapolation.","code":"\nset.seed(200)\nn = 100\nx <- 1:n\ny <- x + rnorm(n = 50, mean = 15, sd = 15)\nm0 <- lm(y ~ x)\nsummary(m0)\n#> \n#> Call:\n#> lm(formula = y ~ x)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -27.6093  -6.4396   0.6437   6.7742  26.7192 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) 14.84655    2.37953   6.239 1.12e-08 ***\n#> x            0.97910    0.04091  23.934  < 2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 11.81 on 98 degrees of freedom\n#> Multiple R-squared:  0.8539, Adjusted R-squared:  0.8524 \n#> F-statistic: 572.9 on 1 and 98 DF,  p-value: < 2.2e-16\nlibrary(randomForest)\n#> randomForest 4.6-14\n#> Type rfNews() to see new features/changes/bug fixes.\n#> \n#> Attaching package: 'randomForest'\n#> The following object is masked from 'package:ranger':\n#> \n#>     importance\nrf = randomForest::randomForest(data.frame(x=x), y, nodesize = 5, keep.inbag = TRUE)\nrf\n#> \n#> Call:\n#>  randomForest(x = data.frame(x = x), y = y, nodesize = 5, keep.inbag = TRUE) \n#>                Type of random forest: regression\n#>                      Number of trees: 500\n#> No. of variables tried at each split: 1\n#> \n#>           Mean of squared residuals: 202.6445\n#>                     % Var explained: 78.34\nlibrary(forestError)\nrmse <- function(a, b) {  sqrt(mean((a - b)^2)) }\ndat <- data.frame(x,y)\nnewdata <- data.frame(\n  x = -100:200\n)\nnewdata$y.lm <- predict(m0, newdata = newdata)\n## prediction error from forestError:\nquantiles = c((1-.682)/2, 1-(1-.682)/2)\npr.rf = forestError::quantForestError(rf, X.train=data.frame(x=x), \n                                      X.test=data.frame(x = -100:200), \n                                      Y.train=y, alpha = (1-(quantiles[2]-quantiles[1])))\nnewdata$y.rf <- predict(rf, newdata = newdata)\nrmse.lm <- round(rmse(y, predict(m0)), 1)\nrmse.rf <- round(rmse(y, predict(rf)), 1)\nrmse.lm; rmse.rf\n#> [1] 11.7\n#> [1] 14.2\nleg.txt <- sprintf(\"%s (%s)\", c('lm', 'RF'), c(rmse.lm, rmse.rf))\npar(mar = c(0, 0, 0, 0), fg = 'black', bg = 'white')\nplot(y ~ x, xlim = c(-25, 125), ylim = c(-50, 150), type = 'n', axes = FALSE)\ngrid()\npoints(y ~ x, cex = 1, pch = 16, las = 1)\nlines(y.lm ~ x, data = newdata, col = 2, lwd = 2)\nlines(y.rf ~ x, data = newdata, col = 4, lwd = 2)\nlines(newdata$x, pr.rf$estimates$lower_0.318, lty=2,col=4)\nlines(newdata$x, pr.rf$estimates$upper_0.318, lty=2,col=4)\nlegend('bottom', legend = leg.txt, lwd = 2, lty = 1, col = c(2, 4, 3), horiz = TRUE, title = 'RMSE')\nlibrary(mlr)\nlibrary(kernlab)\n#> \n#> Attaching package: 'kernlab'\n#> The following objects are masked from 'package:raster':\n#> \n#>     buffer, rotated\nlibrary(mboost)\n#> Loading required package: parallel\n#> Loading required package: stabs\n#> \n#> Attaching package: 'stabs'\n#> The following object is masked from 'package:mlr':\n#> \n#>     subsample\n#> This is mboost 2.9-2. See 'package?mboost' and 'news(package  = \"mboost\")'\n#> for a complete list of changes.\n#> \n#> Attaching package: 'mboost'\n#> The following object is masked from 'package:glmnet':\n#> \n#>     Cindex\n#> The following objects are masked from 'package:raster':\n#> \n#>     cv, extract\nlibrary(landmap)\nSL.library = c(\"regr.ranger\", \"regr.glm\", \"regr.gamboost\", \"regr.ksvm\")\nlrns <- lapply(SL.library, mlr::makeLearner)\ntsk <- mlr::makeRegrTask(data = dat, target = \"y\")\ninit.m <- mlr::makeStackedLearner(lrns, method = \"stack.cv\", super.learner = \"regr.lm\", resampling=mlr::makeResampleDesc(method = \"CV\"))\neml = train(init.m, tsk)\nsummary(eml$learner.model$super.model$learner.model)\n#> \n#> Call:\n#> stats::lm(formula = f, data = d)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -28.0474  -7.0473  -0.4075   7.2528  28.6083 \n#> \n#> Coefficients:\n#>               Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)   -0.86179    3.01456  -0.286  0.77560    \n#> regr.ranger   -0.27294    0.24292  -1.124  0.26402    \n#> regr.glm       4.75714    1.09051   4.362 3.27e-05 ***\n#> regr.gamboost -3.55134    1.14764  -3.094  0.00259 ** \n#> regr.ksvm      0.08578    0.28482   0.301  0.76394    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 11.25 on 95 degrees of freedom\n#> Multiple R-squared:  0.8714, Adjusted R-squared:  0.8659 \n#> F-statistic: 160.9 on 4 and 95 DF,  p-value: < 2.2e-16\nnewdata$y.eml = predict(eml, newdata = newdata)$data$response\nm.train = eml$learner.model$super.model$learner.model$model\nm.terms = eml$learner.model$super.model$learner.model$terms\neml.MSE0 = matrixStats::rowSds(as.matrix(m.train[,all.vars(m.terms)[-1]]), na.rm=TRUE)^2\neml.MSE = deviance(eml$learner.model$super.model$learner.model)/df.residual(eml$learner.model$super.model$learner.model)\n## correction factor / mass-preservation of MSE\neml.cf = eml.MSE/mean(eml.MSE0, na.rm = TRUE)\neml.cf\n#> [1] 9.328646\npred = mlr::getStackedBaseLearnerPredictions(eml, newdata=data.frame(x = -100:200))\nrf.sd = sqrt(matrixStats::rowSds(as.matrix(as.data.frame(pred)), na.rm=TRUE)^2 * eml.cf)\nrmse.eml <- round(sqrt(eml.MSE), 1)\nleg.txt <- sprintf(\"%s (%s)\", c('lm', 'EML'), c(rmse.lm, rmse.eml))\npar(mar = c(0, 0, 0, 0), fg = 'black', bg = 'white')\nplot(y ~ x, xlim = c(-25, 125), ylim = c(-50, 150), type = 'n', axes = FALSE)\ngrid()\npoints(y ~ x, cex = 1, pch = 16, las = 1)\nlines(y.lm ~ x, data = newdata, col = 2, lwd = 2)\nlines(y.eml ~ x, data = newdata, col = 4, lwd = 2)\nlines(newdata$x, newdata$y.eml+rmse.eml+rf.sd, lty=2, col=4)\nlines(newdata$x, newdata$y.eml-(rmse.eml+rf.sd), lty=2, col=4)\nlegend('bottom', legend = leg.txt, lwd = 2, lty = 1, col = c(2, 4, 3), horiz = TRUE, title = 'RMSE')"},{"path":"spatial-interpolation-using-ensemble-ml.html","id":"spatial-interpolation-using-ensemble-ml","chapter":"2 Spatial interpolation using Ensemble ML","heading":"2 Spatial interpolation using Ensemble ML","text":"reading work--progress Spatial spatiotemporal interpolation using Ensemble Machine Learning. chapter currently draft version, peer-review publication pending. can find polished first edition https://opengeohub.github.io/spatial-prediction-eml/.","code":""},{"path":"spatial-interpolation-using-ensemble-ml.html","id":"spatial-interpolation-using-ml-and-buffer-distances-to-points","chapter":"2 Spatial interpolation using Ensemble ML","heading":"2.1 Spatial interpolation using ML and buffer distances to points","text":"relatively simple approach interpolate values point data using e.g. \nRandom Forest use buffer distances points covariates. can\nuse meuse dataset testing (T. Hengl et al., 2018):creates 155 gridded maps .e. one map per training point. maps \ndistances can now used predict target variable running:Using model can generate plot predictions using:\nFigure 2.1: Values Zinc predicted using RF buffer distances.\nresulting predictions produce patterns much similar \nproduce used ordinary kriging similar. Note however RFsp\nmodel: (1) fit variogram, (2) model essence -\nparameterized basically covariates training points.","code":"\nlibrary(rgdal)\nlibrary(ranger)\nlibrary(raster)\nlibrary(plotKML)\ndemo(meuse, echo=FALSE)\ngrid.dist0 <- landmap::buffer.dist(meuse[\"zinc\"], meuse.grid[1], \n                                   classes=as.factor(1:nrow(meuse)))\ndn0 <- paste(names(grid.dist0), collapse=\"+\")\nfm0 <- as.formula(paste(\"zinc ~ \", dn0))\nov.zinc <- over(meuse[\"zinc\"], grid.dist0)\nrm.zinc <- cbind(meuse@data[\"zinc\"], ov.zinc)\nm.zinc <- ranger(fm0, rm.zinc, num.trees=150, seed=1)\nm.zinc\n#> Ranger result\n#> \n#> Call:\n#>  ranger(fm0, rm.zinc, num.trees = 150, seed = 1) \n#> \n#> Type:                             Regression \n#> Number of trees:                  150 \n#> Sample size:                      155 \n#> Number of independent variables:  155 \n#> Mtry:                             12 \n#> Target node size:                 5 \n#> Variable importance mode:         none \n#> Splitrule:                        variance \n#> OOB prediction error (MSE):       67501.48 \n#> R squared (OOB):                  0.4990359\nop <- par(oma=c(0,0,0,1), mar=c(0,0,4,3))\nzinc.rfd <- predict(m.zinc, grid.dist0@data)$predictions\nmeuse.grid$zinc.rfd = zinc.rfd\nplot(raster(meuse.grid[\"zinc.rfd\"]), col=R_pal[[\"rainbow_75\"]][4:20],\n         main=\"Predictions RF on buffer distances\", axes=FALSE, box=FALSE)\npoints(meuse, pch=\"+\", cex=.8)\npar(op)"},{"path":"spatial-interpolation-using-ensemble-ml.html","id":"spatial-interpolation-using-ml-and-geographical-distances-to-neighbors","chapter":"2 Spatial interpolation using Ensemble ML","heading":"2.2 Spatial interpolation using ML and geographical distances to neighbors","text":"Deriving buffer distances points obviously suitable \nlarge point datasets. Sekulić, Kilibarda, Heuvelink, Nikolić, & Bajat (2020) describe alternative, scalable\nmethod uses closest neighbors (values) covariates predict\ntarget variable. can implemented using meteo package:produces 20 grids showing assigned values 1st 10th\nneighbor distances. can plot values based first neighbor,\ncorresponds using e.g. Voronoi polygons:\nFigure 2.2: Values first neighbor meuse dataset.\nNext, can estimate values training points, time\nremove duplicates using rm.dupl = TRUE:Finally, can fit model predict values purely based spatial\nautocorrelation values (1st 10th nearest neighbour):produce predictions can run:\nFigure 2.3: Values first neighbor meuse dataset.\nsummary, based Figs. 2.1 2.3,\ncan conclude predictions produced using nearest neighbors (Fig. 2.3) show quite different patterns \npredictions based buffer distances (Fig. 2.1). method Sekulić et al. (2020)\n(Random Forest Spatial Interpolation) RFSI probably interesting general applications \nalso added spatiotemporal data problems. also reflects closely idea using\nspatial autocorrelation values used kriging since values neighbors \ndistances neighbors used covariates. hand, RFSI seem \nproduce predictions contain also short range variability (noisy) \npredictions might appear look like geostatistical simulations.","code":"\nlibrary(meteo)\n#> Warning: replacing previous import 'caret::MAE' by 'DescTools::MAE' when loading\n#> 'meteo'\n#> Warning: replacing previous import 'caret::RMSE' by 'DescTools::RMSE' when\n#> loading 'meteo'\nnearest_obs <- meteo::near.obs(locations = meuse.grid, \n                               locations.x.y = c(\"x\",\"y\"), \n                               observations = meuse, observations.x.y=c(\"x\",\"y\"), \n                               zcol = \"zinc\", n.obs = 10, rm.dupl = TRUE)\n#> Warning in if (class(knn1$nn.idx) != \"integer\") {: the condition has length > 1\n#> and only the first element will be used\nstr(nearest_obs)\n#> 'data.frame':    3103 obs. of  20 variables:\n#>  $ dist1 : num  168.2 112 139.9 172 56.4 ...\n#>  $ dist2 : num  204 165 164 173 127 ...\n#>  $ dist3 : num  239 183 210 230 139 ...\n#>  $ dist4 : num  282 268 246 241 265 ...\n#>  $ dist5 : num  370 331 330 335 297 ...\n#>  $ dist6 : num  407 355 370 380 306 ...\n#>  $ dist7 : num  429 406 391 388 390 ...\n#>  $ dist8 : num  504 448 473 472 393 ...\n#>  $ dist9 : num  523 476 484 496 429 ...\n#>  $ dist10: num  524 480 488 497 431 ...\n#>  $ obs1  : num  1022 1022 1022 640 1022 ...\n#>  $ obs2  : num  640 640 640 1022 1141 ...\n#>  $ obs3  : num  1141 1141 1141 257 640 ...\n#>  $ obs4  : num  257 257 257 1141 257 ...\n#>  $ obs5  : num  346 346 346 346 346 346 346 346 346 346 ...\n#>  $ obs6  : num  406 406 406 269 406 406 406 269 257 406 ...\n#>  $ obs7  : num  269 269 269 406 269 ...\n#>  $ obs8  : num  1096 1096 1096 281 1096 ...\n#>  $ obs9  : num  347 347 347 347 504 347 347 279 269 504 ...\n#>  $ obs10 : num  281 504 281 279 347 504 279 347 347 347 ...\nmeuse.gridF = meuse.grid\nmeuse.gridF@data = nearest_obs\nspplot(meuse.gridF[11])\n## training points\nnearest_obs.dev <- meteo::near.obs(locations = meuse, \n                                   locations.x.y = c(\"x\",\"y\"), \n                                   observations = meuse, \n                                   observations.x.y=c(\"x\",\"y\"), \n                                   zcol = \"zinc\", n.obs = 10, rm.dupl = TRUE)\n#> Warning in if (class(knn1$nn.idx) != \"integer\") {: the condition has length > 1\n#> and only the first element will be used\nmeuse@data <- cbind(meuse@data, nearest_obs.dev)\nfm.RFSI <- as.formula(paste(\"zinc ~ \", paste(paste0(\"dist\", 1:10), collapse=\"+\"), \"+\", paste(paste0(\"obs\", 1:10), collapse=\"+\")))\nfm.RFSI\n#> zinc ~ dist1 + dist2 + dist3 + dist4 + dist5 + dist6 + dist7 + \n#>     dist8 + dist9 + dist10 + obs1 + obs2 + obs3 + obs4 + obs5 + \n#>     obs6 + obs7 + obs8 + obs9 + obs10\nrf_RFSI <- ranger(fm.RFSI, data=meuse@data, importance = \"impurity\", num.trees = 85, keep.inbag = TRUE)\nrf_RFSI\n#> Ranger result\n#> \n#> Call:\n#>  ranger(fm.RFSI, data = meuse@data, importance = \"impurity\", num.trees = 85,      keep.inbag = TRUE) \n#> \n#> Type:                             Regression \n#> Number of trees:                  85 \n#> Sample size:                      155 \n#> Number of independent variables:  20 \n#> Mtry:                             4 \n#> Target node size:                 5 \n#> Variable importance mode:         impurity \n#> Splitrule:                        variance \n#> OOB prediction error (MSE):       65997.22 \n#> R squared (OOB):                  0.5101999\nout = predict(rf_RFSI, meuse.gridF@data)\nmeuse.grid$zinc.rfsi = out$predictions\nop <- par(oma=c(0,0,0,1), mar=c(0,0,4,3))\nplot(raster(meuse.grid[\"zinc.rfsi\"]), col=R_pal[[\"rainbow_75\"]][4:20],\n     main=\"Predictions RFSI\", axes=FALSE, box=FALSE)\npoints(meuse, pch=\"+\", cex=.8)\npar(op)\n#dev.off()"},{"path":"spatial-interpolation-using-ensemble-ml.html","id":"interpolation-of-numeric-values-using-spatial-regression","chapter":"2 Spatial interpolation using Ensemble ML","heading":"2.3 Interpolation of numeric values using spatial regression","text":"load packages used tutorial:testing use meuse data set. can fit 2D model interpolate zinc\nconcentration based sampling points, distance river flooding frequency\nmaps using:runs number steps including derivation geographical distances (Møller, Beucher, Pouladi, & Greve, 2020),\nderivation principal components (make sure features numeric complete),\nfitting variogram using geoR package (Diggle & Ribeiro Jr, 2007), spatial overlay,\ntraining individual learners training super learner. principle, \nparameter need set manually train.spLearner lambda = 1\nrequired estimate variogram: case target variable \nlog-normally distributed, hence geoR package needs transformation\nparameter set lambda = 1.Note default meta-learner train.spLearner linear model \nfive independently fitted learners c(\"regr.ranger\", \"regr.xgboost\", \"regr.ksvm\", \"regr.nnet\", \"regr.cvglmnet\"). can check success training based 5-fold\nspatial Cross-Validation using:shows model explains 65% variability target variable\nregr.ranger learner (Wright & Ziegler, 2017) strongest learner. Average\nmapping error RMSE = 213, hence models somewhat accurate \nused buffer distances.predict values grids use:Note , default, predict two outputs:Mean prediction: .e. best unbiased prediction response;Prediction errors: usually predicted lower upper 67% quantiles (1 std.) based forestError (Lu & Hardin, 2021);otherwise specified, derivation prediction error (Root Mean Square\nPrediction Error), bias lower upper prediction intervals implemented\ndefault via forestError\nalgorithm. method explained detail Lu & Hardin (2021).also produce prediction intervals using quantreg Random Forest\nalgorithm (Meinshausen, 2006) implemented ranger package, \nstandard deviation bootstraped models, although using method Lu & Hardin (2021) recommended.determine prediction errors without drastically increasing computing time,\nbasically fit independent random forest model using five base-learners\nsetting quantreg = TRUE:prediction error methods non-parameteric users can choose \nprobability output via quantiles argument. example, default\nquantiles set produce prediction intervals .682 range, \n1-standard-deviation range case Gaussian distribution.\nDeriving prediction errors, however, can come computational large number\nfeatures trees random forest, mind EML comes \nexponentially increased computing time.can plot predictions prediction errors next using:\nFigure 2.4: Predicted zinc content based meuse data set.\nshows prediction errors (right plot) highest:model getting away training points (spatial extrapolation),individual points high values can explained covariates,measured values response variable general high,can also plot lower upper prediction intervals .682\nprobability range using:\nFigure 2.5: Lower (q.lwr) upper (q.upr) prediction intervals zinc content based meuse data set.\n","code":"\nlibrary(landmap)\nlibrary(rgdal)\nlibrary(geoR)\n#> --------------------------------------------------------------\n#>  Analysis of Geostatistical Data\n#>  For an Introduction to geoR go to http://www.leg.ufpr.br/geoR\n#>  geoR version 1.8-1 (built on 2020-02-08) is now loaded\n#> --------------------------------------------------------------\nlibrary(plotKML)\nlibrary(raster)\nlibrary(glmnet)\nlibrary(xgboost)\nlibrary(kernlab)\n#> \n#> Attaching package: 'kernlab'\n#> The following objects are masked from 'package:raster':\n#> \n#>     buffer, rotated\nlibrary(deepnet)\nlibrary(forestError)\nlibrary(mlr)\ndemo(meuse, echo=FALSE)\nm <- train.spLearner(meuse[\"zinc\"], covariates=meuse.grid[,c(\"dist\",\"ffreq\")], \n                     lambda = 1, parallel=FALSE)\n#> # weights:  103\n#> initial  value 51302358.935975 \n#> final  value 19491244.345324 \n#> converged\n#> # weights:  103\n#> initial  value 45220763.375836 \n#> final  value 16169762.129496 \n#> converged\n#> # weights:  103\n#> initial  value 50723533.722266 \n#> final  value 18557431.400000 \n#> converged\n#> # weights:  103\n#> initial  value 52179393.837805 \n#> final  value 19673925.525180 \n#> converged\n#> # weights:  103\n#> initial  value 48966129.905012 \n#> final  value 19237393.850000 \n#> converged\n#> # weights:  103\n#> initial  value 50327217.343373 \n#> final  value 19238858.992857 \n#> converged\n#> # weights:  103\n#> initial  value 50106999.812372 \n#> final  value 19072846.949640 \n#> converged\n#> # weights:  103\n#> initial  value 48513819.381025 \n#> final  value 18339886.345324 \n#> converged\n#> # weights:  103\n#> initial  value 48861791.596477 \n#> final  value 18454493.171429 \n#> converged\n#> # weights:  103\n#> initial  value 48476787.002316 \n#> final  value 18430903.600000 \n#> converged\n#> # weights:  103\n#> initial  value 54933965.304156 \n#> final  value 20750447.509677 \n#> converged\nsummary(m@spModel$learner.model$super.model$learner.model)\n#> \n#> Call:\n#> stats::lm(formula = f, data = d)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -470.94  -97.78  -15.73   64.59 1049.38 \n#> \n#> Coefficients:\n#>                Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)   2146.8982   968.9798   2.216 0.028234 *  \n#> regr.ranger      0.6822     0.1912   3.569 0.000483 ***\n#> regr.xgboost     0.5249     0.4071   1.289 0.199244    \n#> regr.nnet       -4.5017     2.0482  -2.198 0.029499 *  \n#> regr.ksvm        0.4241     0.2148   1.975 0.050145 .  \n#> regr.cvglmnet   -0.2757     0.1648  -1.673 0.096441 .  \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 199.9 on 149 degrees of freedom\n#> Multiple R-squared:  0.7132, Adjusted R-squared:  0.7036 \n#> F-statistic:  74.1 on 5 and 149 DF,  p-value: < 2.2e-16\nmeuse.y <- predict(m)\n#> Predicting values using 'getStackedBaseLearnerPredictions'...TRUE\n#> Deriving model errors using forestError package...TRUEzinc ~ regr.ranger + regr.xgboost + regr.nnet + regr.ksvm + regr.cvglmnet\npar(mfrow=c(1,2), oma=c(0,0,0,1), mar=c(0,0,4,3))\nplot(raster(meuse.y$pred[\"response\"]), col=R_pal[[\"rainbow_75\"]][4:20],\n     main=\"Predictions spLearner\", axes=FALSE, box=FALSE)\npoints(meuse, pch=\"+\", cex=.8)\nplot(raster(meuse.y$pred[\"model.error\"]), col=rev(bpy.colors()),\n     main=\"Prediction errors\", axes=FALSE, box=FALSE)\npoints(meuse, pch=\"+\", cex=.8)\npts = list(\"sp.points\", meuse, pch = \"+\", col=\"black\")\nspplot(meuse.y$pred[,c(\"q.lwr\",\"q.upr\")], col.regions=R_pal[[\"rainbow_75\"]][4:20],\n       sp.layout = list(pts),\n       main=\"Prediction intervals (alpha = 0.318)\")"},{"path":"spatial-interpolation-using-ensemble-ml.html","id":"model-fine-tuning-and-feature-selection","chapter":"2 Spatial interpolation using Ensemble ML","heading":"2.4 Model fine-tuning and feature selection","text":"function tune.spLearner can used optimize spLearner object :fine-tuning model parameters, especially ranger mtry XGBoost parameters,reduce number features running feature selection via mlr::makeFeatSelWrapper function,package landmap currently requires two base learners used include regr.ranger \nregr.xgboost, least 3 base learners total. model can optimized using:reports RMSE different mtry reports features left removed. Note turn fine-tuning XGboost using xg.skip = TRUE takes order magnitude time. summary, specific case, fine-tuned model much accurate, comes less features:Note fine-tuning feature selection can quite computational \nhighly recommended start smaller subsets data measure processing\ntime. Note function mlr::makeFeatSelWrapper can result errors \ncovariates low variance follow zero-inflated distribution.\nReducing number features via feature selection fine-tuning Random\nForest mtry XGboost parameters, however, can result significantly higher\nprediction speed can also help improve accuracy.","code":"\nm0 <- tune.spLearner(m, xg.skip=TRUE, parallel=FALSE)\nstr(m0@spModel$features)chr [1:11] \"PC2\" \"PC3\" \"PC4\" \"rX_0\" \"rY_0\" \"rY_0.2\" \"rX_0.5\" \"rY_1\" \"rY_1.4\" \"rY_2.9\" \"rY_3.1\"\nsummary(m0@spModel$learner.model$super.model$learner.model)Residuals:\n    Min      1Q  Median      3Q     Max \n-404.09 -139.03  -42.05   64.69 1336.47 \n\nCoefficients:\n                Estimate Std. Error t value Pr(>|t|)   \n(Intercept)   2091.87119  661.70995   3.161  0.00190 **\nregr.ranger      0.14278    0.24177   0.591  0.55570   \nregr.xgboost     0.92283    0.53131   1.737  0.08448 . \nregr.nnet       -4.34961    1.38703  -3.136  0.00206 **\nregr.ksvm        0.66590    0.25027   2.661  0.00865 **\nregr.cvglmnet   -0.08703    0.13808  -0.630  0.52944   \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 245.2 on 149 degrees of freedom\nMultiple R-squared:  0.5683,    Adjusted R-squared:  0.5538 \nF-statistic: 39.23 on 5 and 149 DF,  p-value: < 2.2e-16"},{"path":"spatial-interpolation-using-ensemble-ml.html","id":"estimation-of-prediction-intervals","chapter":"2 Spatial interpolation using Ensemble ML","heading":"2.5 Estimation of prediction intervals","text":"can also print lower upper prediction interval every location using e.g.:q.lwr lower q.upr 68% probability upper quantile value. shows 68% probability interval location x=181072, y=333611 734–1241 means prediction error (±1 s.d.), location, 250. Compare actual value sampled location:average prediction error whole area :somewhat lower RMSE derived cross-validation, \nalso predicted values fact low (skewed distribution),\nEML seems many problems predicting low values.Note also, example , refit model using exactly \nsettings might get somewhat different maps different values. \nexpected number training points covariates low, stacking\ndone using (random) 5-fold Cross-validation, hence results always\nslightly different. resulting models maps, however, \nsignificantly different indicate Ensemble ML unstable.\ncase larger datasets (≫1000 points), differences predictions\nbecome less less visible.","code":"\nsp::over(meuse[1,], meuse.y$pred)\n#>   response model.error model.bias    q.lwr    q.upr\n#> 1 999.8759    214.1839   27.39722 749.5775 1289.188\nmeuse@data[1,\"zinc\"]\n#> [1] 1022\nsummary(meuse.y$pred$model.error)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>   44.63   77.04  167.69  159.58  215.06  480.11"},{"path":"spatial-interpolation-using-ensemble-ml.html","id":"predictions-using-log-transformed-target-variable","chapter":"2 Spatial interpolation using Ensemble ML","heading":"2.6 Predictions using log-transformed target variable","text":"purpose spatial prediction make accurate predictions low(er)\nvalues response, can train model transformed variable:summary model usually somewhat higher R-square, best learners stay :can next predict back-transform values:\nFigure 2.6: Predicted zinc content based meuse data set log-transformation.\npredictions (Figs. 2.4 2.6) show similar\npatterns prediction error maps quite different case. Nevertheless,\nproblem areas seem match maps (see Figs. 2.4 2.6 right part).\ncompare distributions two predictions can also see predictions differ much:\nFigure 2.7: Difference distributions observed predicted.\nobserved high values somewhat smoothed median value \n, hence can conclude two EML models predict target\nvariable without bias. estimate prediction intervals using log-transformed\nvariable can use:Note log-transformation needed non-linear learner \nranger /Xgboost, often good idea focus prediction \nget better accuracy lower values (Tomislav Hengl et al., 2021). example, objective spatial\ninterpolation map soil nutrient deficiencies, log-transformation \ngood idea produce slightly better accuracy lower values.Another advantage using log-transformation log-normal variables \nprediction intervals likely symmetric, derivation \nprediction error (±1 s.d.) can derived :","code":"\nmeuse$log.zinc = log1p(meuse$zinc)\nm2 <- train.spLearner(meuse[\"log.zinc\"], covariates=meuse.grid[,c(\"dist\",\"ffreq\")], parallel=FALSE)\n#> # weights:  103\n#> initial  value 4998.882579 \n#> final  value 73.222851 \n#> converged\n#> # weights:  103\n#> initial  value 5683.969732 \n#> final  value 68.561330 \n#> converged\n#> # weights:  103\n#> initial  value 4834.044628 \n#> final  value 68.897172 \n#> converged\n#> # weights:  103\n#> initial  value 5706.570659 \n#> final  value 72.649748 \n#> converged\n#> # weights:  103\n#> initial  value 2349.857144 \n#> final  value 73.103072 \n#> converged\n#> # weights:  103\n#> initial  value 6184.275769 \n#> final  value 74.169414 \n#> converged\n#> # weights:  103\n#> initial  value 4451.463365 \n#> final  value 69.440176 \n#> converged\n#> # weights:  103\n#> initial  value 4852.414287 \n#> final  value 72.774801 \n#> converged\n#> # weights:  103\n#> initial  value 5902.359343 \n#> final  value 72.079999 \n#> converged\n#> # weights:  103\n#> initial  value 6689.386512 \n#> final  value 72.803948 \n#> converged\n#> # weights:  103\n#> initial  value 5607.506170 \n#> final  value 79.790191 \n#> converged\nsummary(m2@spModel$learner.model$super.model$learner.model)\n#> \n#> Call:\n#> stats::lm(formula = f, data = d)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -1.00361 -0.18873 -0.05022  0.14092  1.41474 \n#> \n#> Coefficients:\n#>               Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)   25.75549   10.10511   2.549 0.011822 *  \n#> regr.ranger    0.74984    0.19969   3.755 0.000248 ***\n#> regr.xgboost   0.31617    0.33255   0.951 0.343264    \n#> regr.nnet     -4.44329    1.70769  -2.602 0.010206 *  \n#> regr.ksvm      0.28476    0.19541   1.457 0.147146    \n#> regr.cvglmnet -0.07384    0.15905  -0.464 0.643137    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.3574 on 149 degrees of freedom\n#> Multiple R-squared:  0.7615, Adjusted R-squared:  0.7535 \n#> F-statistic: 95.14 on 5 and 149 DF,  p-value: < 2.2e-16\nmeuse.y2 <- predict(m2)\n#> Predicting values using 'getStackedBaseLearnerPredictions'...TRUE\n#> Deriving model errors using forestError package...TRUE\n## back-transform:\nmeuse.y2$pred$response.t = expm1(meuse.y2$pred$response)\nlibrary(ggridges)\nlibrary(viridis)\n#> Loading required package: viridisLite\nlibrary(ggplot2)\n#> \n#> Attaching package: 'ggplot2'\n#> The following object is masked from 'package:kernlab':\n#> \n#>     alpha\nzinc.df = data.frame(zinc=c(sp::over(meuse, meuse.y$pred[\"response\"])[,1], \n                            sp::over(meuse, meuse.y2$pred[\"response.t\"])[,1],\n                            meuse$zinc\n))\nzinc.df$type = as.vector(sapply(c(\"predicted\", \"log.predicted\", \"observed\"), function(i){rep(i, nrow(meuse))}))\nggplot(zinc.df, aes(x = zinc, y = type, fill = ..x..)) +\n  geom_density_ridges_gradient(scale = 0.95, rel_min_height = 0.01, gradient_lwd = 1.) +\n  scale_x_continuous(expand = c(0.01, 0)) +\n  ## scale_x_continuous(trans='log2') +\n  scale_y_discrete(expand = c(0.01, 0.01)) +\n  scale_fill_viridis(name = \"Zinc\", option = \"C\") +\n  labs(title = \"Distributions comparison\") +\n  theme_ridges(font_size = 13, grid = TRUE) + theme(axis.title.y = element_blank())\n#> Picking joint bandwidth of 110\nx = sp::over(meuse[1,], meuse.y2$pred)\nexpm1(x$q.lwr); expm1(x$q.upr)\n#> [1] 837.8133\n#> [1] 1638.359pe = (q.upr - q.lwr)/2"},{"path":"spatial-interpolation-using-ensemble-ml.html","id":"spatial-prediction-of-soil-types-factor-variable","chapter":"2 Spatial interpolation using Ensemble ML","heading":"2.7 Spatial prediction of soil types (factor-variable)","text":"Ensemble Machine Learning can also used interpolate factor type variables\ne.g. soil types. example Ebergotzen dataset available \npackage plotKML (T. Hengl et al., 2015):case target variable TAXGRSC soil types based German soil\nclassification system. changes modeling problem regression \nclassification. recommend using following learners :model training prediction however looks regression:generate predictions use:","code":"\nlibrary(plotKML)\ndata(eberg_grid)\ngridded(eberg_grid) <- ~x+y\nproj4string(eberg_grid) <- CRS(\"+init=epsg:31467\")\ndata(eberg)\ncoordinates(eberg) <- ~X+Y\nproj4string(eberg) <- CRS(\"+init=epsg:31467\")\nsummary(eberg$TAXGRSC)\n#>     Auenboden     Braunerde          Gley         HMoor    Kolluvisol \n#>            71           790            86             1           186 \n#>          Moor Parabraunerde  Pararendzina       Pelosol    Pseudogley \n#>             1           704           215           252           487 \n#>        Ranker       Regosol      Rendzina          NA's \n#>            20           376            23           458\nsl.c <- c(\"classif.ranger\", \"classif.xgboost\", \"classif.nnTrain\")\nX <- eberg_grid[c(\"PRMGEO6\",\"DEMSRT6\",\"TWISRT6\",\"TIRAST6\")]\nif(!exists(\"mF\")){\n  mF <- train.spLearner(eberg[\"TAXGRSC\"], covariates=X, parallel=FALSE)\n}\n#> Converting PRMGEO6 to indicators...\n#> Converting covariates to principal components...\n#> Deriving oblique coordinates...TRUE\n#> Subsetting observations to 79% complete cases...TRUE\n#> Skipping variogram modeling...TRUE\n#> Estimating block size ID for spatial Cross Validation...TRUE\n#> Using learners: classif.ranger, classif.xgboost, classif.nnTrain...TRUE\n#> Fitting a spatial learner using 'mlr::makeClassifTask'...TRUE\nif(!exists(\"TAXGRSC\")){\n  TAXGRSC <- predict(mF)\n}\n#> Predicting values using 'getStackedBaseLearnerPredictions'...TRUE\n#> Deriving model errors using sd of sign. learners...TRUE"},{"path":"spatial-interpolation-using-ensemble-ml.html","id":"classification-accuracy","chapter":"2 Spatial interpolation using Ensemble ML","heading":"2.8 Classification accuracy","text":"default landmap package predict hard classes probabilities per class. can check average accuracy classification using:shows 25% classes miss-classified classification\nconfusion especially high Braunerde class. Note result \nbased internal training. Normally one repeat process\nseveral times using 5-fold similar (.e. fit EML, predict errors using resampled\nvalues , repeat).Predicted probabilities, however, interesting also show\nEML possibly problems transition zones multiple classes:\nFigure 2.8: Predicted soil types based EML.\nmaps show also case geographical distances play role, \noverall, features (DTM derivatives parnt material) seem important.addition map probabilities per class, also derived errors per\nprobability, case can computed standard deviation \nprobabilities produced individual learners (note: classification problems\ntechniques quantreg random forest currently exist):\nFigure 2.9: Predicted errors per soil types based s.d. individual learners.\nprobability space, instead using RMSE similar measures, often\nrecommended use measures log-loss \ncorrectly quantifies difference observed predicted probability.\nrule thumb, log-loss values 0.35 indicate poor accuracy predictions,\nthreshold number critically low log-loss also depends number\nclasses. plot can note , general, average error \nmaps relatively low e.g. 0.07:still many pixels confusion classes prediction\nerrors high. Recommended strategy improve map generate sampling\nplan using average prediction error /Confusion Index map, collect\nnew observations & measurements refit prediction models.","code":"\nnewdata = mF@vgmModel$observations@data\nsel.e = complete.cases(newdata[,mF@spModel$features])\nnewdata = newdata[sel.e, mF@spModel$features]\npred = predict(mF@spModel, newdata=newdata)\npred$data$truth = mF@vgmModel$observations@data[sel.e, \"TAXGRSC\"]\nprint(calculateConfusionMatrix(pred))\n#>                predicted\n#> true            Auenboden Braunerde Gley Kolluvisol Parabraunerde Pararendzina\n#>   Auenboden            34         6    0          0             5            0\n#>   Braunerde             0       623    0          1            17            7\n#>   Gley                  4         9   38          3             9            0\n#>   Kolluvisol            0        13    1         99            17            1\n#>   Parabraunerde         0        34    0          3           460            0\n#>   Pararendzina          0        19    0          0             3          147\n#>   Pelosol               0        11    0          1             1            2\n#>   Pseudogley            0        54    2          4            24            3\n#>   Ranker                0        10    0          0             6            0\n#>   Regosol               0        66    0          0            10            1\n#>   Rendzina              0         2    0          0             0            0\n#>   -err.-                4       224    3         12            92           14\n#>                predicted\n#> true            Pelosol Pseudogley Ranker Regosol Rendzina -err.-\n#>   Auenboden           0          3      0       0        0     14\n#>   Braunerde           9          5      0       6        1     46\n#>   Gley                0          5      0       0        0     30\n#>   Kolluvisol          1          3      0       3        0     39\n#>   Parabraunerde       2         10      0       4        0     53\n#>   Pararendzina        6          1      0       0        0     29\n#>   Pelosol           157          4      0       1        0     20\n#>   Pseudogley          4        307      0      13        0    104\n#>   Ranker              1          0      0       0        0     17\n#>   Regosol             4         12      0     220        0     93\n#>   Rendzina            0          0      0       0       20      2\n#>   -err.-             27         43      0      27        1    447\nplot(stack(TAXGRSC$pred[grep(\"prob.\", names(TAXGRSC$pred))]),\n     col=SAGA_pal[[\"SG_COLORS_YELLOW_RED\"]], zlim=c(0,1))\nplot(stack(TAXGRSC$pred[grep(\"error.\", names(TAXGRSC$pred))]),\n     col=SAGA_pal[[\"SG_COLORS_YELLOW_BLUE\"]], zlim=c(0,0.45))\nsummary(TAXGRSC$pred$error.Parabraunerde)\n#>     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n#> 0.001558 0.029074 0.041220 0.066121 0.064179 0.339019"},{"path":"spatial-interpolation-in-3d-using-ensemble-ml.html","id":"spatial-interpolation-in-3d-using-ensemble-ml","chapter":"3 Spatial interpolation in 3D using Ensemble ML","heading":"3 Spatial interpolation in 3D using Ensemble ML","text":"reading work--progress Spatial spatiotemporal interpolation using Ensemble Machine Learning. chapter currently draft version, peer-review publication pending. can find polished first edition https://opengeohub.github.io/spatial-prediction-eml/.","code":""},{"path":"spatial-interpolation-in-3d-using-ensemble-ml.html","id":"mapping-concentrations-of-geochemical-elements","chapter":"3 Spatial interpolation in 3D using Ensemble ML","heading":"3.1 Mapping concentrations of geochemical elements","text":"Ensemble ML can also used mapping soil variables 3D. Consider example\nGeochemical minerological data set USA48 (Smith, Woodruff, Solano, Ellefsen, & Karl, 2014). public data set\nproduced maintained USA Geological Survey contains laboratory measurements\nchemical elements minerals soil 4,857 sites (three depths 0 5 cm,\nhorizon C horizon; Fig. 3.1).previously imported overlaid sampling points vs large stack \ncovariate layers using e.g.:can load regression matrix contains target variables covariates:total 4818 unique locations:individual records can browsed directly via https://mrdata.usgs.gov/ngdb/soil/,\nexample single record includes:\nFigure 3.1: Example geochemical sample observations measurements coordiates site.\nCovariates prepared help interpolation geochemicals include:Distance cities (Nelson et al., 2019);MODIS LST (monthly daytime nighttime);MODIS EVI (long-term monthly values);Lights night images;Snow occurrence probability;Soil property class maps USA48 (Ramcharan et al., 2018);Terrain / hydrological indices;can focus predict concentration lead (Pb). Pb seems change\ndepth seems consistent different land cover classes:\nFigure 3.2: Distribution Pb function land cover classes.\naiming producing predictions geochemical elements different\ndepths soil, can also use depth soil sample one covariates.\nmakes prediction system 3D model can thus used predict \nnew 3D location (\\(X, Y, d\\)). fit RF model data can use:results R-square 0.67. many training samples exactly\ncoordinates (site, three depths), assume model \n-fitting .e. --bag accuracy probably -optimistic.\nInstead can fit Ensemble model block points within 30 30-km blocks:shows somewhat lower R-square 0.44, time whole sites \ntaken hence seems somewhat realistic estimate \nmapping accuracy (Roberts et al., 2017). accuracy plot shows model \nproblems predicting higher values, overall matches observed values:\nFigure 3.3: Accuracy plot Pb concentration soil fitted using Ensemble ML.\nVariables important explaining distribution target variable (based variable importance)\nseem soil depth (hnz_depth), soil type maps, annual day time temperature travel time cities:\nFigure 3.4: Variable importance 3D prediction model Pb concentrations.\nplot travel time cities vs Pb concentrations, can clearly see \nPb negatively correlated travel time cities (following log-log linear relationship):\nFigure 3.5: Distribution Pb function travel time cities different depths.\n","code":"\n## Training data ----\nngs.m <- read.csv(\"./training_data/ds_801_all.csv\")\nngs.m$olc_id = olctools::encode_olc(ngs.m$latitude, ngs.m$longitude, 11)\n## Spatial overlay ----\nsel.pnts = !duplicated(ngs.m$olc_id)\nsummary(sel.pnts)\nngs.m.pnts = ngs.m[which(sel.pnts), c(\"olc_id\", \"longitude\", \"latitude\")]\ncoordinates(ngs.m.pnts) <- ~ longitude + latitude\nproj4string(ngs.m.pnts) <- \"+init=epsg:4326\"\nngs.xy = spTransform(ngs.m.pnts, crs(mask))\ntif.lst = list.files(\"./1km\", glob2rx(\"*.tif$\"), full.names = TRUE)\n## 262 layers\nov.tmp = parallel::mclapply(1:length(tif.lst), function(j){ terra::extract(terra::rast(tif.lst[j]), terra::vect(ngs.xy)) }, mc.cores = 80)\nov.tmp = dplyr::bind_cols(lapply(ov.tmp, function(i){i[,2]}))\nnames(ov.tmp) = tools::file_path_sans_ext(basename(tif.lst))\nov.tmp$olc_id = ngs.xy$olc_id\nreg.matrix = plyr::join(ngs.m, ov.tmp)\nsaveRDS.gz(reg.matrix, \"./input/ds801_geochem1km.rds\")\nds801 = readRDS(\"./input/ds801_geochem1km.rds\")\ndim(ds801)\n#> [1] 14275   407\nstr(levels(as.factor(ds801$olc_id)))\n#>  chr [1:4818] \"75WXFVQG+WH6\" \"75WXJHVG+62X\" \"75WXJVM9+995\" \"75WXRG2G+5PV\" ...\nopenair::scatterPlot(ds801[ds801$pb_ppm<140,], x = \"hzn_depth\", y = \"pb_ppm\", method = \"hexbin\", col = \"increment\", log.x=TRUE, log.y=TRUE, xlab=\"Depth\", ylab=\"Pb [ppm]\", z.lim=c(0,100), type=\"landcover1\")\n#> Warning: removing 11 missing rows due to landcover1\nds801$log.pb = log1p(ds801$pb_ppm)\npr.vars = c(readRDS(\"./input/pb.pr.vars.rds\"), \"hzn_depth\")\nsel.pb = complete.cases(ds801[,c(\"log.pb\", pr.vars)])\nmrf = ranger::ranger(y=ds801$log.pb[sel.pb], x=ds801[sel.pb, pr.vars], \n            num.trees = 85, importance = 'impurity')\nmrf\n#> Ranger result\n#> \n#> Call:\n#>  ranger::ranger(y = ds801$log.pb[sel.pb], x = ds801[sel.pb, pr.vars],      num.trees = 85, importance = \"impurity\") \n#> \n#> Type:                             Regression \n#> Number of trees:                  85 \n#> Sample size:                      14264 \n#> Number of independent variables:  188 \n#> Mtry:                             13 \n#> Target node size:                 5 \n#> Variable importance mode:         impurity \n#> Splitrule:                        variance \n#> OOB prediction error (MSE):       0.09636483 \n#> R squared (OOB):                  0.670695\nif(!exists(\"eml.pb\")){\n  lrn.rf = mlr::makeLearner(\"regr.ranger\", num.trees=85, importance=\"impurity\",\n                            num.threads = parallel::detectCores())\n  lrns.pb <- list(lrn.rf, mlr::makeLearner(\"regr.xgboost\"), mlr::makeLearner(\"regr.cvglmnet\"))\n  tsk0.pb <- mlr::makeRegrTask(data = ds801[sel.pb, c(\"log.pb\", pr.vars)], \n                               target = \"log.pb\", blocking = as.factor(ds801$ID[sel.pb]))\n  init.pb <- mlr::makeStackedLearner(lrns.pb, method=\"stack.cv\", super.learner=\"regr.lm\", \n                                      resampling=mlr::makeResampleDesc(method=\"CV\", blocking.cv=TRUE))\n  parallelMap::parallelStartSocket(parallel::detectCores())\n  eml.pb = train(init.pb, tsk0.pb)\n  parallelMap::parallelStop()\n}\n#> [10:57:29] WARNING: amalgamation/../src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.\nsummary(eml.pb$learner.model$super.model$learner.model)\n#> \n#> Call:\n#> stats::lm(formula = f, data = d)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -2.3979 -0.1990 -0.0117  0.1699  6.2948 \n#> \n#> Coefficients:\n#>               Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)   -0.64745    0.04296 -15.069  < 2e-16 ***\n#> regr.ranger    0.85552    0.02152  39.755  < 2e-16 ***\n#> regr.xgboost   0.26550    0.05337   4.974 6.62e-07 ***\n#> regr.cvglmnet  0.25631    0.02024  12.665  < 2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.4075 on 14260 degrees of freedom\n#> Multiple R-squared:  0.4328, Adjusted R-squared:  0.4327 \n#> F-statistic:  3627 on 3 and 14260 DF,  p-value: < 2.2e-16\nt.pb = quantile(ds801$log.pb, c(0.001, 0.01, 0.999), na.rm=TRUE)\nplot_hexbin(varn=\"log.pb\", breaks=c(t.pb[1], seq(t.pb[2], t.pb[3], length=25)), \n      meas=eml.pb$learner.model$super.model$learner.model$model$log.pb, \n      pred=eml.pb$learner.model$super.model$learner.model$fitted.values,\n      main=\"Pb [EML]\")\nlibrary(ggplot2)\n#> \n#> Attaching package: 'ggplot2'\n#> The following object is masked from 'package:latticeExtra':\n#> \n#>     layer\nxl.pb <- as.data.frame(mlr::getFeatureImportance(eml.pb[[\"learner.model\"]][[\"base.models\"]][[1]])$res)\nxl.pb$relative_importance = 100*xl.pb$importance/sum(xl.pb$importance)\nxl.pb = xl.pb[order(xl.pb$relative_importance, decreasing = T),]\nxl.pb$variable = paste0(c(1:nrow(xl.pb)), \". \", xl.pb$variable)\nggplot(data = xl.pb[1:20,], aes(x = reorder(variable, relative_importance), y = relative_importance)) +\n  geom_bar(fill = \"steelblue\",\n           stat = \"identity\") +\n  coord_flip() +\n  labs(title = \"Variable importance\",\n       x = NULL,\n       y = NULL) +\n  theme_bw() + theme(text = element_text(size=15))\nopenair::scatterPlot(ds801[ds801$pb_ppm<140,], x = \"travel_time_to_cities_1_usa48\", y = \"pb_ppm\", method = \"hexbin\", col = \"increment\", log.x=TRUE, log.y=TRUE, xlab=\"Travel time to cities 1\", ylab=\"Pb [ppm]\", type=\"hzn_depth\")\n#> Warning: removing 11 missing rows due to hzn_depth"},{"path":"spatial-interpolation-in-3d-using-ensemble-ml.html","id":"predictions-in-3d","chapter":"3 Spatial interpolation in 3D using Ensemble ML","heading":"3.2 Predictions in 3D","text":"produce predictions can focus area around Chicago conglomeration. can\nload covariate layers using:contains layers used training. can generate predictions \nadding depth column, write predictions GeoTIFFs:finally gives following pattern:\nFigure 3.6: Predictions Pb concentration different soil depths based Ensemble ML. Points indicate training points used build predictive mapping model. Red color indicates high values. Values Pb clearly drop soil depth.\nBased results, can said general:Distribution Pb across USA seem controlled mixture factors\nincluding climatic factors, soil-terrain units anthropogenic factors (travel distance cities);Overall big urban areas show significantly higher concentrations heavy\nmetals relationship log-log linear;Soil depth geochemical elements comes overall important\ncovariate hence mapping soil variables 3D fully justified;","code":"\ng1km = readRDS(\"./input/chicago_grid1km.rds\")\nfor(k in c(5, 30, 60)){\n  out.tif = paste0(\"./output/pb_ppm_\", k, \"cm_1km.tif\")\n  if(!file.exists(out.tif)){\n    g1km$hzn_depth = k\n    sel.na = complete.cases(g1km)\n    newdata = g1km[sel.na, eml.pb$features]\n    pred = predict(eml.pb, newdata=newdata)\n    g1km.sp = SpatialPixelsDataFrame(as.matrix(g1km[sel.na,c(\"x\",\"y\")]), \n                data=pred$data, proj4string=CRS(\"EPSG:5070\"))\n    g1km.sp$pred = expm1(g1km.sp$response)\n    rgdal::writeGDAL(g1km.sp[\"pred\"], out.tif, type=\"Int16\", mvFlag=-32768, options=c(\"COMPRESS=DEFLATE\"))\n    #gc()\n  }\n}"},{"path":"spatial-interpolation-in-3d-using-ensemble-ml.html","id":"advantages-and-limitations-of-running-3d-predictive-mapping","chapter":"3 Spatial interpolation in 3D using Ensemble ML","heading":"3.3 Advantages and limitations of running 3D predictive mapping","text":"summary 3D soil mapping relatively straight forward implement especially\nmapping soil variables soil profiles (multiple samples per soil layer).\nmodeling target variable depth, good idea plot\nrelationship target variable depth different settings\n(Fig. 3.2). 3D soil mapping based Machine Learning \nnow increasingly common (T. Hengl & MacMillan, 2019; Sothe, Gonsamo, Arabian, & Snider, 2022).limitation 3D predictive mapping size data .e. data volumes\nincreasing proportionally number slices need predict (case three).\nAlso, show points exactly coordinates might result e.g. \nRandom Forest overfitting emphasizing covariate layers possibly\nless important, hence important use blocking parameter \nseparates training validation points.Soil depth many soil variables important explanatory variables, \ncases correlate target variable, probably\nalso need 3D soil mapping. case depth significantly\ncorrelated, one simply first aggregate values fixed block depth \nconvert modeling 3D 2D.","code":""},{"path":"spatiotemporal-ml.html","id":"spatiotemporal-ml","chapter":"4 Spatiotemporal interpolation using Ensemble ML","heading":"4 Spatiotemporal interpolation using Ensemble ML","text":"reading work--progress Spatial spatiotemporal interpolation using Ensemble Machine Learning. chapter currently draft version, peer-review publication pending. can find polished first edition https://opengeohub.github.io/spatial-prediction-eml/.","code":""},{"path":"spatiotemporal-ml.html","id":"spatiotemporal-interpolation-of-daily-temperatures","chapter":"4 Spatiotemporal interpolation using Ensemble ML","heading":"4.1 Spatiotemporal interpolation of daily temperatures","text":"previous examples demonstrated effects -fitting \nEnsemble ML helps decrease overfitting extrapolation problems\nusing synthetic data. can now look real-life cases example\ndaily temperatures measured several years Croatia described T. Hengl et al. (2012).\ndata sets consist two parts: (1) measurements daily temperatures \nmeteo stations, (2) list gridded covariates (Fig. 4.1).\nFigure 4.1: Temporal dynamics mean-daily temperatures sample meteorological stations. shows seasonality effects (smoothed line) daily oscillations.\ncan load point data using:typical format spatiotemporal meteorological data locations\nstations one table, measurements daily temperatures (MDTEMP) \n. column cday cumulative day since 1970, allows us\npresent time linear scale .e. using numeric value instead dates.gridded data includes: () static covariates (relief data), \n(b) dynamic time-series data (MODIS LST). load static covariates use:dynamic time-series data stored local folder (input/LST2006HR) \nindividual files can list using:see 46 images year 2006 daytime 46 images \nnight time estimates LST. want load rasters R\nmight experience RAM problems. can first overlay points see\nvariables can help mapping daily temperatures.static covariates run overlay :spatiotemporal data (MODIS LST time-series) need run overlay \nspacetime cube. means need match points using x,y,t\ncoordinates grids covering x,y,t fields. speed spacetime overlay\nuse custom function extract_st, basically builds top \nterra package. First, need define begin, end times GeoTIFF:now know spacetime coordinates points grids, can run\noverlay parallel speed computing:case also exclude values LST.day equal 0 \nbasically missing values GeoTIFFs. repeat overlay operation\nnight light images:result spacetime overlay simple long table matching exactly meteo-data table.\nnext bind results overlay using static dynamic covariates:also add geometric component temperature based sphere formulas:now produced spatiotemporal regression matrix can used fit\nprediction model daily temperature. model form:next fit Ensemble ML using process described previous sections:Train model using subset points:shows daily temperatures can predicted relatively high R-square,\nalthough residual values still significant (ranging -1.8 1.8 degrees):variable importance analysis shows important variable predicting\ndaily temperatures , fact, night-time MODIS LST:\nFigure 4.2: Variable importance modeling spacetime daily temperatures.\ncan use fitted spacetime EML model generate predictions e.g. \nfour consecutive days August. First, import MODIS LST month interest:Second, interpolate values 8–day periods fill gaps EO data\nusing simple linear interpolation (MODIS images available every 8 days):Now can make predictions target days August 2006 using (run\noperation loop avoid RAM overload):plot predictions can either put predictions spacetime package\nclass (see gstat tutorial), simply plot using sp package:\nFigure 4.3: Predictions spacetime daily temperature August 2006.\nsummary, example shows fit spatiotemporal EML using\nalso seasonality component together EO data. can hence\nconsidered complete framework spatiotemporal interpolation static,\ndynamic covariates latitude / elevation used model training.","code":"\nlibrary(rgdal)\nhrmeteo = readRDS(\"input/hrtemp2006_meteo.rds\")\nstr(hrmeteo)\n#> List of 2\n#>  $ meteo   :'data.frame':    44895 obs. of  5 variables:\n#>   ..$ IDSTA : chr [1:44895] \"GL001\" \"GL001\" \"GL001\" \"GL001\" ...\n#>   ..$ DATE  : chr [1:44895] \"2006-1-1\" \"2006-1-2\" \"2006-1-3\" \"2006-1-4\" ...\n#>   ..$ MDTEMP: num [1:44895] 1.6 0.7 1.5 0.3 -0.1 1 0.3 -1.9 -5.4 -3.6 ...\n#>   ..$ cday  : num [1:44895] 13148 13149 13150 13151 13152 ...\n#>   .. ..- attr(*, \"tzone\")= chr \"\"\n#>   ..$ x     : num [1:44895] NA NA NA NA NA NA NA NA NA NA ...\n#>  $ stations:'data.frame':    152 obs. of  3 variables:\n#>   ..$ IDSTA: chr [1:152] \"GL001\" \"GL002\" \"GL003\" \"GL004\" ...\n#>   ..$ X    : num [1:152] 670760 643073 673778 752344 767729 ...\n#>   ..$ Y    : num [1:152] 5083464 5086417 5052001 4726567 4717878 ...\nidsta.pnts = hrmeteo$stations\ncoordinates(idsta.pnts) = ~ X + Y\nhrgrid1km = readRDS(\"input/hrgrid1km.rds\")\n#plot(hrgrid1km[1])\nproj4string(idsta.pnts) = proj4string(hrgrid1km)\nstr(hrgrid1km@data)\n#> 'data.frame':    238630 obs. of  4 variables:\n#>  $ HRdem : int  1599 1426 1440 1764 1917 1912 1707 1550 1518 1516 ...\n#>  $ HRdsea: num  93 89.6 89.8 93.6 95 ...\n#>  $ Lat   : num  46.5 46.5 46.5 46.5 46.5 ...\n#>  $ Lon   : num  13.2 13.2 13.2 13.2 13.2 ...\nLST.listday <- dir(\"input/LST2006HR\", pattern=glob2rx(\"LST2006_**_**.LST_Day_1km.tif\"), full.names = TRUE)\nLST.listnight <- dir(\"input/LST2006HR\", pattern=glob2rx(\"LST2006_**_**.LST_Night_1km.tif\"), full.names = TRUE)\nstr(LST.listday)\n#>  chr [1:46] \"input/LST2006HR/LST2006_01_01.LST_Day_1km.tif\" ...\nidsta.ov <- sp::over(idsta.pnts, hrgrid1km)\nidsta.ov$IDSTA = idsta.pnts$IDSTA\nstr(idsta.ov)\n#> 'data.frame':    152 obs. of  5 variables:\n#>  $ HRdem : int  161 134 202 31 205 563 80 96 116 228 ...\n#>  $ HRdsea: num  198.5 181.7 192.9 0 1.5 ...\n#>  $ Lat   : num  45.9 45.9 45.6 42.7 42.6 ...\n#>  $ Lon   : num  17.2 16.8 17.2 18.1 18.3 ...\n#>  $ IDSTA : chr  \"GL001\" \"GL002\" \"GL003\" \"GL004\" ...\nlibrary(terra)\n#> terra version 0.8.11 (beta-release)\n#> \n#> Attaching package: 'terra'\n#> The following object is masked from 'package:rgdal':\n#> \n#>     project\nsource(\"mlst_functions.R\")\nhrmeteo$meteo$x = plyr::join(hrmeteo$meteo, hrmeteo$stations, by=\"IDSTA\")$X\nhrmeteo$meteo$y = plyr::join(hrmeteo$meteo, hrmeteo$stations, by=\"IDSTA\")$Y\n## generate row ID:\nhrmeteo$meteo$row.id = 1:nrow(hrmeteo$meteo)\nhrmeteo$meteo$Date = as.Date(hrmeteo$meteo$DATE, format = \"%Y-%m-%d\")\n## strip dates from filename:\nbegin.tif1.lst = as.Date(paste0(\"2006-\", substr(basename(LST.listday), 9, 10), \n                                \"-\", substr(basename(LST.listday), 12, 13)))-4\nend.tif1.lst = as.Date(paste0(\"2006-\", substr(basename(LST.listday), 9, 10), \n                              \"-\", substr(basename(LST.listday), 12, 13)))+4\nmc.cores = parallel::detectCores()\nov.pnts <- parallel::mclapply(1:length(LST.listday), function(i){ \n  extract_st(tif=LST.listday[i], hrmeteo$meteo, date=\"Date\", \n             crs = proj4string(hrgrid1km),        \n             date.tif.begin=begin.tif1.lst[i], \n             date.tif.end=end.tif1.lst[i], \n             coords=c(\"x\",\"y\"), variable.name=\"LST.day\") }, \n  mc.cores=mc.cores)\nov.pnts = ov.pnts[!sapply(ov.pnts, is.null)]\nov.tifs1 = plyr::join_all(ov.pnts, by=\"row.id\", type=\"full\")\nstr(ov.tifs1)\n#> 'data.frame':    44895 obs. of  2 variables:\n#>  $ LST.day: num  0 0 0 0 0 0 0 0 0 0 ...\n#>  $ row.id : int  1 2 3 4 5 366 367 368 369 370 ...\nov.tifs1$LST.day = ifelse(ov.tifs1$LST.day == 0, NA, ov.tifs1$LST.day)\nbegin.tif2.lst = as.Date(paste0(\"2006-\", substr(basename(LST.listnight), 9, 10), \n                                \"-\", substr(basename(LST.listnight), 12, 13)))-4\nend.tif2.lst = as.Date(paste0(\"2006-\", substr(basename(LST.listnight), 9, 10), \n                              \"-\", substr(basename(LST.listnight), 12, 13)))+4\nov.pnts <- parallel::mclapply(1:length(LST.listnight), function(i){ \n  extract_st(tif=LST.listnight[i], hrmeteo$meteo, date=\"Date\", \n             crs = proj4string(hrgrid1km),        \n             date.tif.begin=begin.tif2.lst[i], \n             date.tif.end=end.tif2.lst[i], \n             coords=c(\"x\",\"y\"), variable.name=\"LST.night\") }, \n  mc.cores=mc.cores)\nov.pnts = ov.pnts[!sapply(ov.pnts, is.null)]\nov.tifs2 = plyr::join_all(ov.pnts, by=\"row.id\", type=\"full\")\nstr(ov.tifs2)\n#> 'data.frame':    44895 obs. of  2 variables:\n#>  $ LST.night: num  13344 13344 13344 13344 13344 ...\n#>  $ row.id   : int  1 2 3 4 5 366 367 368 369 370 ...\nov.tifs2$LST.night = ifelse(ov.tifs2$LST.night == 0, NA, ov.tifs2$LST.night)\nhrmeteo.rm = plyr::join_all(list(hrmeteo$meteo, ov.tifs1, ov.tifs2))\n#> Joining by: row.id\n#> Joining by: row.id\nhrmeteo.rm = plyr::join(hrmeteo.rm, idsta.ov)\n#> Joining by: IDSTA\nhrmeteo.rm$temp.mean <- temp.from.geom(fi=hrmeteo.rm$Lat, \n                   as.numeric(strftime(hrmeteo.rm$Date, format = \"%j\")), \n                   a=37.03043, b=-15.43029, elev=hrmeteo.rm$HRdem, t.grad=0.6)\nfm.tmp <- MDTEMP ~ temp.mean + LST.day + LST.night + HRdsea\nlibrary(mlr)\n#> Loading required package: ParamHelpers\n#> 'mlr' is in maintenance mode since July 2019. Future development\n#> efforts will go into its successor 'mlr3' (<https://mlr3.mlr-org.com>).\n#> \n#> Attaching package: 'mlr'\n#> The following object is masked from 'package:terra':\n#> \n#>     resample\nlrn.rf = mlr::makeLearner(\"regr.ranger\", num.trees=150, importance=\"impurity\",\n                          num.threads = parallel::detectCores())\nlrns.st <- list(lrn.rf, mlr::makeLearner(\"regr.nnet\"), mlr::makeLearner(\"regr.gamboost\"))\nsel = complete.cases(hrmeteo.rm[,all.vars(fm.tmp)])\nhrmeteo.rm = hrmeteo.rm[sel,]\n#summary(sel)\nsubs <- runif(nrow(hrmeteo.rm))<.2\ntsk0.st <- mlr::makeRegrTask(data = hrmeteo.rm[subs,all.vars(fm.tmp)], \n                             target = \"MDTEMP\", blocking = as.factor(hrmeteo.rm$IDSTA[subs]))\ntsk0.st\n#> Supervised task: hrmeteo.rm[subs, all.vars(fm.tmp)]\n#> Type: regr\n#> Target: MDTEMP\n#> Observations: 7573\n#> Features:\n#>    numerics     factors     ordered functionals \n#>           4           0           0           0 \n#> Missings: FALSE\n#> Has weights: FALSE\n#> Has blocking: TRUE\n#> Has coordinates: FALSE\ninit.TMP <- mlr::makeStackedLearner(lrns.st, method=\"stack.cv\", super.learner=\"regr.lm\", \n                                    resampling=mlr::makeResampleDesc(method=\"CV\", blocking.cv=TRUE))\nparallelMap::parallelStartSocket(parallel::detectCores())\neml.TMP = train(init.TMP, tsk0.st)\n#> # weights:  19\n#> initial  value 1752597.611462 \n#> final  value 503633.983959 \n#> converged\nparallelMap::parallelStop()\nsummary(eml.TMP$learner.model$super.model$learner.model)\n#> \n#> Call:\n#> stats::lm(formula = f, data = d)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -16.2439  -1.8106   0.0204   1.8004  14.9611 \n#> \n#> Coefficients:\n#>               Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)   35.06219    8.10904   4.324 1.55e-05 ***\n#> regr.ranger    0.70575    0.02770  25.474  < 2e-16 ***\n#> regr.nnet     -2.72276    0.62920  -4.327 1.53e-05 ***\n#> regr.gamboost  0.29540    0.02799  10.553  < 2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 2.888 on 7569 degrees of freedom\n#> Multiple R-squared:  0.8747, Adjusted R-squared:  0.8746 \n#> F-statistic: 1.761e+04 on 3 and 7569 DF,  p-value: < 2.2e-16\nlibrary(ggplot2)\nxl <- as.data.frame(mlr::getFeatureImportance(eml.TMP[[\"learner.model\"]][[\"base.models\"]][[1]])$res)\nxl$relative_importance = 100*xl$importance/sum(xl$importance)\nxl = xl[order(xl$relative_importance, decreasing = TRUE),]\nxl$variable = paste0(c(1:length(xl$variable)), \". \", xl$variable)\nggplot(data = xl[1:4,], aes(x = reorder(variable, relative_importance), y = relative_importance)) +\n  geom_bar(fill = \"steelblue\",\n           stat = \"identity\") +\n  coord_flip() +\n  labs(title = \"Variable importance\",\n       x = NULL,\n       y = NULL) +\n  theme_bw() + theme(text = element_text(size=15))\nhrpred1km = hrgrid1km\nsel.tifs1 = LST.listday[grep(\"_08_\", LST.listday)]\nsel.tifs2 = LST.listnight[grep(\"_08_\", LST.listnight)]\n## read to R in parallel\nx1 = as.data.frame( parallel::mclapply(sel.tifs1, \n      function(i){x <- readGDAL(i)$band1; x <- ifelse(x<1, NA, x); return(x)}, \n                                       mc.cores = mc.cores))\nx2 = as.data.frame( parallel::mclapply(sel.tifs2, \n      function(i){x <- readGDAL(i)$band1; x <- ifelse(x<1, NA, x); return(x)}, \n                                       mc.cores = mc.cores))\nnames(x1)  <- basename(sel.tifs1); names(x2) <- basename(sel.tifs2)\ndates.lst = as.Date(\"2006-08-13\")+1:8\nin.dates = c(\"2006-08-05\", \"2006-08-13\", \"2006-08-21\", \"2006-08-29\")\nin.days = as.numeric(strftime(as.Date(c(in.dates)), format = \"%j\"))\n## interpolate values for missing dates in spacetime\nlibrary(parallel)\ncl <- makeCluster(detectCores())\nclusterExport(cl, c(\"in.days\", \"dates.lst\"))\nt1s = parallel::parApply(cl, x1, 1, \n      function(y) { try( approx(in.days, as.vector(y), xout=as.numeric(strftime(dates.lst, format = \"%j\")))$y ) })\nt2s = parallel::parApply(cl, x2, 1, \n      function(y) { try( approx(in.days, as.vector(y), xout=as.numeric(strftime(dates.lst, format = \"%j\")))$y ) })\nstopCluster(cl)\n## remove missing pixels\nx.t1s = parallel::mclapply(t1s, length, mc.cores = mc.cores)\nt1s[which(!x.t1s==8)] <- list(rep(NA, 8))\nt1s = do.call(rbind.data.frame, t1s)\nnames(t1s) = paste0(\"LST.day_\", dates.lst)\nx.t2s = parallel::mclapply(t2s, length, mc.cores = mc.cores)\nt2s[which(!x.t2s==8)] <- list(rep(NA, 8))\nt2s = do.call(rbind.data.frame, t2s)\nnames(t2s) = paste0(\"LST.night_\", dates.lst)\nfor(j in paste(dates.lst)){\n  out.tif = paste0(\"output/MDTEMP_\", j, \".tif\")\n  if(!file.exists(out.tif)){\n    hrpred1km@data[,\"LST.day\"] = t1s[,paste0(\"LST.day_\", j)]\n    hrpred1km@data[,\"LST.night\"] = t2s[,paste0(\"LST.night_\", j)]\n    hrpred1km$temp.mean = temp.from.geom(fi=hrpred1km$Lat, \n                     as.numeric(strftime(as.Date(j), format = \"%j\")), \n                     a=37.03043, b=-15.43029, elev=hrpred1km$HRdem, t.grad=0.6)\n    sel.pix = complete.cases(hrpred1km@data[,eml.TMP$features])\n    out = predict(eml.TMP, newdata=hrpred1km@data[sel.pix,eml.TMP$features])\n    hrpred1km@data[,paste0(\"MDTEMP_\", j)] = NA\n    hrpred1km@data[sel.pix, make.names(paste0(\"MDTEMP_\", j))] = out$data$response * 10\n    writeGDAL(hrpred1km[make.names(paste0(\"MDTEMP_\", j))], out.tif, mvFlag = -32768,\n              type = \"Int16\", options = c(\"COMPRESS=DEFLATE\"))\n  } else {\n    hrpred1km@data[,make.names(paste0(\"MDTEMP_\", j))] = readGDAL(out.tif)$band1\n  }\n}\n#> output/MDTEMP_2006-08-14.tif has GDAL driver GTiff \n#> and has 487 rows and 490 columns\n#> output/MDTEMP_2006-08-15.tif has GDAL driver GTiff \n#> and has 487 rows and 490 columns\n#> output/MDTEMP_2006-08-16.tif has GDAL driver GTiff \n#> and has 487 rows and 490 columns\n#> output/MDTEMP_2006-08-17.tif has GDAL driver GTiff \n#> and has 487 rows and 490 columns\n#> output/MDTEMP_2006-08-18.tif has GDAL driver GTiff \n#> and has 487 rows and 490 columns\n#> output/MDTEMP_2006-08-19.tif has GDAL driver GTiff \n#> and has 487 rows and 490 columns\n#> output/MDTEMP_2006-08-20.tif has GDAL driver GTiff \n#> and has 487 rows and 490 columns\n#> output/MDTEMP_2006-08-21.tif has GDAL driver GTiff \n#> and has 487 rows and 490 columns\nst.pts = list(\"sp.points\", idsta.pnts, pch = \"+\", col=\"black\")\nspplot(hrpred1km[make.names(paste0(\"MDTEMP_\", dates.lst[c(1,4,8)]))], \n       col.regions=plotKML::R_pal[[\"rainbow_75\"]][4:20],\n       at = seq(143, 239, length.out=17),\n       sp.layout = list(st.pts),\n       main=\"Prediction daily temperature\")\n#dev.off()"},{"path":"spatiotemporal-ml.html","id":"spatiotemporal-distribution-of-fagus-sylvatica","chapter":"4 Spatiotemporal interpolation using Ensemble ML","heading":"4.2 Spatiotemporal distribution of Fagus sylvatica","text":"next example show fit spatiotemporal model\nusing biological data: occurrences Fagus\nsylvatica Europe. \ndomain Species Distribution modeling, except case \nmodel distribution target species also spacetime. training (point) data \ncompiled purpose OpenDataScience.eu project, \ncleaned overlaid vs time-series Landsat GLAD images climatic\nvariables (Bonannella et al., 2022?; Witjes et al., 2022?).\ndetails data refer also eumap repository.can load snapshot data using:subset larger\ndataset \nused produce predictions distribution key forest tree\nspecies Europe (can browse data via https://maps.opendatascience.eu).\nfirst columns dataset show:header columns :id: unique ID point;year: year obsevation;postprocess: column can value yearly spacetime identify\ntemporal reference observation comes original\ndataset result post-processing (yearly originals,\nspacetime post-processed points);Tile_ID: extracted 30-km tiling system;easting: easting coordinate observation point;northing: northing coordinate observation point;Atlas_class: contains name tree species NULL ’s \nabsence point coming LUCAS;lc1: contains original LUCAS land cover classes NULL ’s \npresence point;columns EO ecological covariates use \nmodeling distribution Fagus sylvatica. can plot distribution points EU using:\nFigure 4.4: Distribution occurrence locations Fagus sylvatica. training points also referrenced time hence can used run spacetime overlay.\nprevious examples, first define target model formula. \nremove model columns used prediction:speed-fitting models prepared two functions wrap modeling steps. First step\nconsists fine-tuning base learners:function runs hyperparameter optimization binary classification models (classif.ranger, classif.xgboost, classif.glmnet) later used create ensemble model.\nintermediate models (fine-tuned RF XGboost) written local folder output RDS files.\noutput function RDS file including fine-tuned models formula.second step fitting ensemble model based stacked regularization:function fits separately previously used classification models: algorithm\nclassify independently probability training point classified 0\n(-occurring) 1 (occurring). probability outputs model used training dataset\nmeta-learner. use GLM model binomial link function (logistic regression):variable importance analysis (RF component) shows \nimportant covariates mapping distribution Fagus sylvatica \nlandsat images (expected considering high spatial resolution)\nspatial distribution tree species (tree species commonly found\ntogether Fagus sylvatica field):\nFigure 4.5: Variable importance spatiotemporal model used predict distribution Fagus sylvatica.\nproduce spacetime predictions tiles (30-m spatial\nresolution) can run:can compare predictions probability occurrence \ntarget species two years next using:case seems drastic changes distribution \ntarget species time, also expected forest species distribution\nchange scale 50 100 year scale years.\nchanges distribution species, however, can detected nevertheless.\ncan due abrupt events pest-pandemics, fires, floods landslides\nclear cutting forests course.can also plot images using plotKML package can open \nvisualize predictions also Google Earth similar:case attach prediction also TimeSpan.begin TimeSpan.end\nmeans Google Earth recognize temporal reference predictions.\nOpening predictions Google Earth allows us interpretation produced maps also\nanalyze much changes vegetation cover connected relief,\nproximity urban areas, possible fire / flood events similar.\nFigure 4.6: Spacetime predictions distribution Fagus Sylvatica visualized time-series data Google Earth.\ncan check ensemble model performs classification task compared \ncomponent models running resampling operation learner meta-learner.\nexample use 5-fold spatial cross validation repeated 5 times. first create new learners\nassign hyperparameters obtained fine-tuning operation:create classification task (previously included functions), select \nmetric want evaluate models resampling strategy. using logarithmic loss \nperformance metric example. , everything set resampling operation, \nrun parallel:meta-learner, access predicted probabilities component models stored inside\nensemble model use training data meta-learner resampling task. \nneed create additional task:Logloss robust metric assessing performances classification algorithms \nprobability space. However, less intuitive represent. better compare performances across\nmodels, standardize logloss values according following equation:\\(R^2_\\text{logloss} = 1 -\\frac{Logloss_\\text{m}}{Logloss_\\text{r}}\\)\\(Logloss_\\text{m}\\) logloss value model \\(Logloss_\\text{r}\\) \nlogloss value model performs better random guess. use value \n\\(Logloss_\\text{r}\\) baseline predictive performances. Values \\(R^2_\\text{logloss}\\) close 1\nindicate high predictive performances, values close 0 indicate poor predictive performances:example, can see ensemble model outperforms, although slightly, \nmodels. also interesting notice Random forest best component\nmodel look ensemble model coefficients, terms performances Xgboost\nactually closest ensemble.","code":"\nlibrary(data.table)\nlibrary(dplyr)\nlibrary(mlr)\nlibrary(sp)\nfs.rm = readRDS('input/fagus_sylvatica_st.rds')\nocc.pnts = fs.rm[,c(\"Atlas_class\",\"easting\",\"northing\")]\ncoordinates(occ.pnts) = ~ easting + northing\nproj4string(occ.pnts) = \"+init=epsg:3035\"\nocc.pnts = spTransform(occ.pnts, CRS(\"+init=epsg:4326\"))\nhead(fs.rm[,1:10])\n#>        id year postprocess Tile_ID easting northing Atlas_class lc1\n#> 1  217127 2015   spacetime    4579 2925585  1639819           0    \n#> 2 1078455 2007   spacetime   12496 3551500  2886500           1    \n#> 3  332623 2012      yearly    8806 5650000  2286000           0 D20\n#> 4  488689 2018      yearly   14583 4116000  3230000           0 B16\n#> 5 2977316 2009   spacetime   23273 5376500  4601500           0    \n#> 6  649000 2002   spacetime    8163 3286500  2211500           1    \n#>   Abies_alba_distr.tiff Abies_spp._distr.tiff\n#> 1                     0                     0\n#> 2                   100                     0\n#> 3                     0                     0\n#> 4                   100                     0\n#> 5                     0                     0\n#> 6                     0                     0\nlibrary(rnaturalearth)\nlibrary(raster)\neurope <- rnaturalearth::ne_countries(scale=10, continent = 'europe')\neurope <- raster::crop(europe, extent(-24.8,35.2,31,68.5))\nop = par(mar=c(0,0,0,0))\nplot(europe, col=\"lightgrey\", border=\"darkgrey\", axes=FALSE)\npoints(occ.pnts[occ.pnts$Atlas_class==1,], pch=\"+\", cex=.8)\npar(op)\ncovs = grep(\"id|year|postprocess|Tile_ID|easting|northing|Atlas_class|lc1\", \n          colnames(fs.rm), value = TRUE, invert = TRUE)\nfm.fs = stats::as.formula(paste(\"Atlas_class ~ \", paste(covs, collapse=\"+\")))\nfs.rm$Atlas_class = factor(fs.rm$Atlas_class)\nall.vars(fm.fs)[1:5]\n#> [1] \"Atlas_class\"                    \"Abies_alba_distr.tiff\"         \n#> [3] \"Abies_spp._distr.tiff\"          \"Acer_campestre_distr.tiff\"     \n#> [5] \"Acer_pseudoplatanus_distr.tiff\"\nsource(\"mlst_functions.R\")\nfs.rm0 = fs.rm %>% sample_n(5000)\ntnd.ml = tune_learners(data = fs.rm0, formula = fm.fs, blocking = factor(fs.rm$Tile_ID))\neml.fs = train_sp_eml(data = fs.rm0, tune_result = tnd.ml, blocking = as.factor(fs.rm$Tile_ID))\n#eml.fs = readRDS(\"output/eml_model.rds\")\nsummary(eml.fs$learner.model$super.model$learner.model)\n#> \n#> Call:\n#> stats::glm(formula = f, family = \"binomial\", data = getTaskData(.task, \n#>     .subset), weights = .weights, model = FALSE)\n#> \n#> Deviance Residuals: \n#>     Min       1Q   Median       3Q      Max  \n#> -3.0986  -0.1150  -0.1123  -0.1122   3.1439  \n#> \n#> Coefficients:\n#>                 Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)      -5.0655     0.1776 -28.527  < 2e-16 ***\n#> classif.ranger    3.7088     0.6548   5.664 1.48e-08 ***\n#> classif.xgboost   3.3720     0.5377   6.272 3.57e-10 ***\n#> classif.glmnet    2.7881     0.2986   9.337  < 2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for binomial family taken to be 1)\n#> \n#>     Null deviance: 5314.27  on 4999  degrees of freedom\n#> Residual deviance:  729.32  on 4996  degrees of freedom\n#> AIC: 737.32\n#> \n#> Number of Fisher Scoring iterations: 7\nlibrary(ggplot2)\nxl <- as.data.frame(mlr::getFeatureImportance(eml.fs[[\"learner.model\"]][[\"base.models\"]][[1]])$res)\nxl$relative_importance = 100*xl$importance/sum(xl$importance)\nxl = xl[order(xl$relative_importance, decreasing = T),]\nxl$variable = paste0(c(1:238), \". \", xl$variable)\nggplot(data = xl[1:20,], aes(x = reorder(variable, relative_importance), y = relative_importance)) +\n  geom_bar(fill = \"steelblue\",\n           stat = \"identity\") +\n  coord_flip() +\n  labs(title = \"Variable importance\",\n       x = NULL,\n       y = NULL) +\n  theme_bw() + theme(text = element_text(size=10))\nm1 = predict_tiles(input = \"9690.2012\",  model = eml.fs)\nm2 = predict_tiles(input = \"9690.2016\",  model = eml.fs)\nm3 = predict_tiles(input = \"9690.2020\",  model = eml.fs)\nm1$Prob.2012 = m1$Prob\nm1$Prob.2016 = m2$Prob\nm1$Prob.2020 = m3$Prob\npts = list(\"sp.points\", spTransform(occ.pnts[occ.pnts$Atlas_class==1,], CRS(\"+init=epsg:3035\")), \n           pch = \"+\", col=\"black\")\nspplot(m1[,c(\"Prob.2012\",\"Prob.2016\",\"Prob.2020\")], names.attr = c(\"Prob.2010-2014\", \"Prob.2014-2018\", \"Prob.2018-2020\"), col.regions=rev(bpy.colors()),\n  sp.layout = list(pts),\n  main=\"Realized distribution of Fagus sylvatica\")\nlibrary(plotKML)\nfor(j in c(2012,2016,2020)){\n  kml(m1, colour=m1@data[,paste0(\"Prob.\", j)], file.name=paste0(\"prob.\", j, \".kml\"),\n      raster_name = paste0(\"prob.\", j, \".png\"),\n      colour_scale = SAGA_pal[[\"SG_COLORS_YELLOW_BLUE\"]], \n      z.lim=c(0,100),\n      TimeSpan.begin = as.Date(paste0(j, \"-01-01\")), TimeSpan.end = as.Date(paste0(j, \"-12-31\")))\n}\nlearners = list(makeLearner(\"classif.ranger\", predict.type = \"prob\", num.trees = 85),\n                makeLearner(\"classif.xgboost\", predict.type = \"prob\"),\n                makeLearner(\"classif.glmnet\", predict.type = \"prob\"))\nlearners[[1]] = mlr::setHyperPars(learners[[1]], par.vals = mlr::getHyperPars(tnd.ml[[1]]$learner))\nlearners[[2]] = mlr::setHyperPars(learners[[2]], par.vals = mlr::getHyperPars(tnd.ml[[2]]$learner))\nlearners[[3]] = mlr::setHyperPars(learners[[3]], par.vals = list(s = min(tnd.ml[[3]]$learner.model$lambda)))#> Starting parallelization in mode=socket with cpus=32.\n#> Exporting objects to slaves for mode socket: .mlr.slave.options\n#> Resampling: repeated cross-validation\n#> Measures:             logloss\n#> Mapping in parallel: mode = socket; level = mlr.resample; cpus = 32; elements = 25.\n#> \n#> Aggregated Result: logloss.test.mean=0.1131968\n#> \n#> Exporting objects to slaves for mode socket: .mlr.slave.options\n#> Resampling: repeated cross-validation\n#> Measures:             logloss\n#> Mapping in parallel: mode = socket; level = mlr.resample; cpus = 32; elements = 25.\n#> \n#> Aggregated Result: logloss.test.mean=0.0818324\n#> \n#> Exporting objects to slaves for mode socket: .mlr.slave.options\n#> Resampling: repeated cross-validation\n#> Measures:             logloss\n#> Mapping in parallel: mode = socket; level = mlr.resample; cpus = 32; elements = 25.\n#> \n#> Aggregated Result: logloss.test.mean=0.1695897\n#> #> Exporting objects to slaves for mode socket: .mlr.slave.options\n#> Resampling: repeated cross-validation\n#> Measures:             logloss\n#> Mapping in parallel: mode = socket; level = mlr.resample; cpus = 32; elements = 25.\n#> \n#> Aggregated Result: logloss.test.mean=0.0743475\n#> \n#> Stopped parallelization. All cleaned up.#>        RF       XGB       GLM       EML \n#> 0.7887610 0.8472908 0.6835250 0.8612585"},{"path":"spatiotemporal-ml.html","id":"summary-notes","chapter":"4 Spatiotemporal interpolation using Ensemble ML","heading":"4.3 Summary notes","text":"tutorial reviewed aspects spatial \nspatiotemporal data demonstrated use ML, specifically\nEnsemble ML, train spatiotemporal models produce time-series \npredictions. also shown, using synthetic real-life datasets, \nincorrectly setting training cross-validation can lead \n-fitting problems. done help users realize Machine Learning,\nhowever trivial might seem, click--button process solid knowledge\nunderstanding advanced statistics (regression, hypothesis testing,\nsampling resampling, probability theory) still required.spatiotemporal models, recommend combining covariates \ncan represent long-term accumulated effects climate, together \ncovariates can represent daily monthly oscillation variables soil\nmoisture, temperatures similar. design modeling system,\nhighly recommend trying understand ecology processes behind\nvariable interest first, designing modeling system best reflect expert\nknowledge. example RF -fitting data Gasch et al. (2015) shows -depth\nunderstanding problem can help design modeling framework prevent\n-fitting problems similar. Witjes et al. (2022?) Bonannella et al. (2022?) describe comprehensive\nframework spatiotemporal ML can even run large datasets.time-series EO data exists, can also incorporated \nmapping algorithm shown three case studies. spacetime overlays \nrecommend using Cloud-Optimized GeoTIFFs terra package (Hijmans, 2019) helps speed-overlays. options efficient overlay stars gdalcubes package (Appel & Pebesma, 2019).Spatiotemporal datasets can order magnitude larger, hence\nimportant, implementing analysis spatiotemporal data, \nconsider computing optimization, typically implies:Running operations parallel;Separating fine-tuning parameter optimization (best run \nsubset save computing time) predictions,Using tiling systems run overlay, predictions visualizations,Finally, recommend following generic steps fit spatiotemporal models:Define target interest design modeling framework understanding\necology processes behind variable interest.Prepare training (points) data data cube covariates\nreferenced spacetime.Overlay points spacetime, create spatiotemporal\nregression- classification-matrix.Add seasonal components, fine-tune initial model, reduce complexity\nmuch possible, produce production-ready spatiotemporal prediction model\n(usually using Ensemble Machine Learning).Run mapping accuracy assessment determine prediction uncertainty\nincluding per pixel uncertainty.Generate predictions spacetime — create time-series \npredictions.(optional) Run change-detection / trend analysis try detect\nmain drivers positive / negative trends (Witjes et al., 2022?).Deploy predictions Cloud-Optimized GeoTIFF produce final\nreport mapping accuracy, variable importance.Ensemble ML framework used clearly offers many benefits, also comes\ncost order magnitude higher computational load. Also\ninterpretation models can cumbersome multiple\nlearners plus meta-learner, often becomes difficult \ntrack-back individual relationship variables. help increase\nconfidence produced models, recommend studying Interpretable Machine\nLearning methods\n(Molnar, 2020), running additional model diagnostics, \nintensively plotting data space spacetime feature space.Note mlr package discontinued, examples\nmight become unstable time. advised instead use\nnew mlr3 package.","code":""},{"path":"spatiotemporal-machine-learning-for-species-distribution-modeling.html","id":"spatiotemporal-machine-learning-for-species-distribution-modeling","chapter":"5 Spatiotemporal Machine Learning for Species Distribution Modeling","heading":"5 Spatiotemporal Machine Learning for Species Distribution Modeling","text":"reading work--progress Spatial spatiotemporal interpolation using Ensemble Machine Learning. chapter currently draft version, peer-review publication pending. can find polished first edition https://opengeohub.github.io/spatial-prediction-eml/.","code":""},{"path":"spatiotemporal-machine-learning-for-species-distribution-modeling.html","id":"species-distribution-modeling","chapter":"5 Spatiotemporal Machine Learning for Species Distribution Modeling","heading":"5.1 Species Distribution Modeling","text":"Species Distribution Modeling (SDM) /Mapping aims\nexplaining mapping distribution species function ecological, environmental conditions\n/human influence. Typical steps SDM include (Hijmans, 2019):Prepare locations occurrence species species density;Prepare environmental predictor variables (climate, terrain, surface water);Fit SDM model can used either predict natural habitat / Niche /occurrence probability;Predict habitat / occurrence probability across region interest (perhaps future past climate).Modeling species distribution different mapping quantitative soil properties\n/land surface temperature described previous chapters. Species training data often comes\nspecific properties include (Fois, Cuena-Lombraña, Fenu, & Bacchetta, 2018; Martinez-Minaya, Cameletti, Conesa, & Pennino, 2018):Dealing occurrence-records: biologists / ecologists often record species observed;Species dynamics often complex: species migratory birds change location seasonaly;Modeling distribution species birds similar animals insects\nspacetime context highly complex various levels chaotic behavior apply;recent years increasing interest using Machine Learning \nspecies distribution modeling, especially model disease outbreaks.\nreview ML methods SDM available also J. Zhang & Li (2017).\nchapter describe scalable framework predicting species occurrences\nbased Ensemble Machine Learning described previous chapter.\ntarget variables interest case SDM usually () probability occurrence\n/(b) species density (number individuals per area) /(c) habitat\nsuitability indices. covariates SDM use time-series \nEarth Observation images similar climatic terrain-based images.Extending spatiotemporal Ensemble ML modeling species distribution trivial.\norder able interpolate species distribution, probability occurrence\n/density species space-time using Ensemble ML, can simply\nimport model occurrence-data miss quantities states.\nneed provide instead run several steps make data suited ML.\nexample, absence training points available, can use various\nmethod derive likely locations certain species occur .e. \nhighly unlikely occur due ecological limitations minimum winter temperature\nminimum rainfall similar. referred pseudo-absence training points.\nHence, first show generate pseudo-absence data using maxlike\npackage, enough occurrence \nabsence records available, apply standard ML steps.","code":""},{"path":"spatiotemporal-machine-learning-for-species-distribution-modeling.html","id":"tiger-mosquito-over-europe","chapter":"5 Spatiotemporal Machine Learning for Species Distribution Modeling","heading":"5.2 Tiger Mosquito over Europe","text":"tiger mosquito (Aedes albopictus) vector species many different viruses\nincluding responsible dengue fever, Zika chikungunya. natural habitat\nspecies limited past, however, recent time, species spread\nmany countries transport goods international travel e.g. shipping\nroutes similar (Benedict, Levine, Hawley, & Lounibos, 2007; Da Re, Montecino-Latorre, Vanwambeke, & Marcantonio, 2021). R package dynamAedes\ncontains stochastic, time-discrete spatially-explicit population dynamical models Aedes sp.\ninvasive species (Da Re, Van Bortel, et al., 2021).can obtain occurrences Aedes albopictus either using rgbif::occ_data\nfunction, downloading CSV file GBIF website.\nlocal copy occurrences Europe can loaded using:visualize mosquito point data Europe can use e.g.:\nFigure 5.1: Spatiotemporal visualization GBIF occurrence records Tiger mosquito.\nshows mosquito seems concentrated southern Europe,\nprimarily along coast-line, although adults spotted also \nNorthern Europe. Note also mosquito seems continuously spreading\nacross Europe, however, sure also just \nrecords GBIF coming last 5 years.","code":"\n## occ = rgbif::occ_data(taxonKey=1651430, hasCoordinate = TRUE, year = '2000,2022')\nocc = readRDS(\"./input/gbif_aedes_albopictus_mood.rds\")\nstr(occ)\n#> 'data.frame':    13834 obs. of  9 variables:\n#>  $ occurrenceID                 : chr  \"https://observation.org/observation/226030224\" \"https://observation.org/observation/229897504\" \"https://observation.org/observation/222737616\" \"https://observation.org/observation/221356176\" ...\n#>  $ scientificName               : chr  \"Aedes albopictus (Skuse, 1894)\" \"Aedes albopictus (Skuse, 1894)\" \"Aedes albopictus (Skuse, 1894)\" \"Aedes albopictus (Skuse, 1894)\" ...\n#>  $ individualCount              : num  1 1 1 1 10 125 22 3 2 2 ...\n#>  $ Date                         : Date, format: \"2021-09-21\" \"2021-07-22\" ...\n#>  $ coordinateUncertaintyInMeters: num  43 25 356 4 NA NA NA NA NA NA ...\n#>  $ decimalLongitude             : num  3.206 -0.349 4.863 -0.647 12.726 ...\n#>  $ decimalLatitude              : num  42 43.3 45.8 38.1 42.1 ...\n#>  $ Year                         : int  2021 2021 2021 2021 2020 2020 2020 2020 2020 2020 ...\n#>  $ olc_c                        : chr  \"8FH5X6G4+VF3\" \"8CMX8M42+Q8G\" \"8FQ6QVW7+M8J\" \"8CCX49J3+95Q\" ...\nlibrary(spacetime)\nlibrary(plotKML)\nsp_ST <- STIDF(SpatialPoints(occ[,c(\"decimalLongitude\", \"decimalLatitude\")], proj4string = \"EPSG:4326\"), \n               occ$Date, data.frame(individualCount=occ$individualCount))\ndata(SAGA_pal)\n## plot in Google Earth:\nplotKML(sp_ST, colour_scale=SAGA_pal[[1]])"},{"path":"spatiotemporal-machine-learning-for-species-distribution-modeling.html","id":"generating-pseudo-absence-data","chapter":"5 Spatiotemporal Machine Learning for Species Distribution Modeling","heading":"5.3 Generating pseudo-absence data","text":"Pseudo-absence points can generated several ways (Iturbide et al., 2015).\ngenerate pseudo-absence data can use maxlike package. First, need\nprepare enough ecological information can help us map habitat species\nusing records Europe. local folder can find:.e. CHELSA Climate Bioclim layers mean\nannual air temperature, annual precipitation amount similar, MODIS Long-term nighttime Land Surface\nTemperatures months 1, 3, 6 9,\nsnow probability images winter months DTM elevation model Copernicus.\ncan load stack rasters R use principal components reduce\noverlap different layers:Next, can fit maxlike model occurrence probability using presence data,\npredict values locations:resulting maps produced using maxlike shown .\nFigure 5.2: Predicted probability occurence Tiger mosquito based maxent analysis. Darker-green areas indicated close 100% probability occurrence.\nmaxlike occurrence probability map indicates Tiger\nmosquito seems prefer coastal areas probably limited winter temperatures.\nminimum temperature survival mosquito adults 3–4 C degrees,\nmosquito eggs minimum temperature lower still -4 degrees (Da Re, Montecino-Latorre, et al., 2021).\nNote habitat suitability analysis one also use maxent algorithm\nanalysis maxlike maxent produce ensemble estimate.Next, can generate reasonable number pseudo-absences (rule thumb,\nnumber generated pseudo-absences exceed 10 20% actual\nnumber occurrence points):single realization 600 simulated pseudo-absences shown :\nFigure 5.3: Simulated pseudo-absences based maxent analysis (predictions background).\nNote use pixels 0 probability occurrence based maxlike results.\nway prevent introducing bias modeling.\ngenerated pseudo-absences, can bind 1 0 records together\nproduce regression /classification matrix, can used \nfit ML models.case GBIF data two possible target variables used:Individual counts .e. number adults observed location;Occurrence / absence states .e. 0/1 values;focus modeling 0/1 states predicting probabilities. \nputs ML exercises category ML classification.","code":"\neco.tifs = list.files(\"./input/mood4km/static\", glob2rx(\"*.tif$\"), full.names=TRUE)\nbasename(eco.tifs)\n#>  [1] \"clm_bioclim.1_chelsa.climate_m_4km_s0..0cm_1980..2010_mood_v2.1.tif\"      \n#>  [2] \"clm_bioclim.12_chelsa.climate_m_4km_s0..0cm_1980..2010_mood_v2.1.tif\"     \n#>  [3] \"clm_bioclim.13_chelsa.climate_m_4km_s0..0cm_1980..2010_mood_v2.1.tif\"     \n#>  [4] \"clm_bioclim.14_chelsa.climate_m_4km_s0..0cm_1980..2010_mood_v2.1.tif\"     \n#>  [5] \"clm_bioclim.5_chelsa.climate_m_4km_s0..0cm_1980..2010_mood_v2.1.tif\"      \n#>  [6] \"clm_bioclim.6_chelsa.climate_m_4km_s0..0cm_1980..2010_mood_v2.1.tif\"      \n#>  [7] \"clm_lst_mod11a2.nighttime.m01_p50_4km_s0..0cm_2000..2021_mood_v1.2.tif\"   \n#>  [8] \"clm_lst_mod11a2.nighttime.m03_p50_4km_s0..0cm_2000..2021_mood_v1.2.tif\"   \n#>  [9] \"clm_lst_mod11a2.nighttime.m06_p50_4km_s0..0cm_2000..2021_mood_v1.2.tif\"   \n#> [10] \"clm_lst_mod11a2.nighttime.m09_p50_4km_s0..0cm_2000..2021_mood_v1.2.tif\"   \n#> [11] \"clm_snow.prob_esacci.dec_p.90_4km_s0..0cm_2000..2012_mood_v2.0.tif\"       \n#> [12] \"clm_snow.prob_esacci.feb_p.90_4km_s0..0cm_2000..2012_mood_v2.0.tif\"       \n#> [13] \"clm_snow.prob_esacci.jan_p.90_4km_s0..0cm_2000..2012_mood_v2.0.tif\"       \n#> [14] \"dtm_elevation_glo90.copernicus_m_4km_s0..0cm_2019_epsg.4326_mood_v1.0.tif\"\n#gc()\ng4km = raster::stack(eco.tifs)\ng4km = as(g4km, \"SpatialGridDataFrame\")\ncc.4km = complete.cases(g4km@data)\ng4km = as(g4km, \"SpatialPixelsDataFrame\")\ng4km = g4km[cc.4km,]\n#summary(cc.4km)\n## 2.2M pixels\n#plot(g4km[14])\ng4km.spc = landmap::spc(g4km)\n#gc()\nmax.fm <- stats::as.formula(paste(\"~\", paste(names(g4km.spc@predicted[1:12]), collapse=\"+\")))\nmax.ml <- maxlike::maxlike(formula=max.fm, rasters=raster::stack(g4km.spc@predicted[1:12]), points=occ.sp@coords, method=\"BFGS\", savedata=TRUE)\n#ment.ml <- dismo::maxent(raster::stack(g4km.spc@predicted[1:12]), occ.sp@coords)\n## bug in \"maxlike\" (https://github.com/rbchan/maxlike/issues/1); need to replace this 'by hand':\nmax.ml$call$formula <- max.fm\n## TH: this operation can be time consuming and is not recommended for large grids\nmax.ml.p <- predict(max.ml)\nmax.ml.p <- methods::as(max.ml.p, \"SpatialGridDataFrame\")\nplot(max.ml.p)\nmax.ml.p = rgdal::writeGDAL(\"./output/occ.prob_aedes_albopictus.tif\")\n## insert 0 values for all occurrences before the date:\nmax.ml.p$absence = ifelse(max.ml.p$band1==100, 1, NA)\ndens.var <- spatstat.geom::as.im(sp::as.image.SpatialGridDataFrame(max.ml.p[\"absence\"]))\npnts.new <- rpoint(600, f=dens.var)"},{"path":"spatiotemporal-machine-learning-for-species-distribution-modeling.html","id":"modeling-distribution-of-mosquitos-through-time","chapter":"5 Spatiotemporal Machine Learning for Species Distribution Modeling","heading":"5.4 Modeling distribution of mosquitos through time","text":"Next can overlay training points (occurrences / adult counts pseudo-absences)\nspacetime produce spatiotemporal regression-matrix. total list \nlayers Europe (MOOD Horizon 2020 project) available Cloud-Optimized GeoTIFFs\ndocumented :\nFigure 5.4: Opening Cloud-Optimized GeoTIFF layers QGIS. total size layers exceeds 30GB efficient access data using COG-architecture S3 services.\nspacetime overlay process can computational hence already pre-computed\nregression / classification matrix:now relatively large matrix basically diversity Earth Observation\nclimatic time-series images. can define model classification problem\noccurrence/absence values (0/1) two states target variable:model function number static dynamic covariate layers:Next, fit Ensemble ML model predict spatiotemporal distribution species\n2000–2021 period following basic steps:Fine-tune hyperparameters per base learner (using smaller subset);Fit stacked learner using optimized parameters;Generate predictions time-series interest;steps can run using functions created extend mlr package:shows model significant errors probability space ranging -0.08 0.12.\nimportant variables based variable importance functionality random forest seem :Night lights (NLT) based Li, Zhou, Zhao, & Zhao (2020);Travel time cities ports based Nelson et al. (2019);Population density (PPD) based Gridded Population World (GPW), v4;Night time images based MOD11A2;Human Footprint dataset based Mu et al. (2022);","code":"\ncov.lst = read.csv(\"./input/mood_layers1km.csv\")\nstr(cov.lst)\n#> 'data.frame':    454 obs. of  5 variables:\n#>  $ n          : int  1 2 3 4 5 6 7 8 9 10 ...\n#>  $ filename   : chr  \"https://s3.eu-central-1.wasabisys.com/mood/CRP/lcv_globalcropland_bowen.et.al_p_1km_s0..0cm_2000_mood_v0.1.tif\" \"https://s3.eu-central-1.wasabisys.com/mood/CRP/lcv_globalcropland_bowen.et.al_p_1km_s0..0cm_2001_mood_v0.1.tif\" \"https://s3.eu-central-1.wasabisys.com/mood/CRP/lcv_globalcropland_bowen.et.al_p_1km_s0..0cm_2002_mood_v0.1.tif\" \"https://s3.eu-central-1.wasabisys.com/mood/CRP/lcv_globalcropland_bowen.et.al_p_1km_s0..0cm_2003_mood_v0.1.tif\" ...\n#>  $ size       : chr  \"9.7 Mb\" \"9.7 Mb\" \"9.7 Mb\" \"9.8 Mb\" ...\n#>  $ source_doi : chr  \"http://doi.org/10.5194/essd-13-5403-2021\" \"http://doi.org/10.5194/essd-13-5403-2021\" \"http://doi.org/10.5194/essd-13-5403-2021\" \"http://doi.org/10.5194/essd-13-5403-2021\" ...\n#>  $ description: chr  \"Cropland fraction historic\" \"Cropland fraction historic\" \"Cropland fraction historic\" \"Cropland fraction historic\" ...\nsource(\"mood_functions.R\")\nrm.all = readRDS(\"./input/regmatrix_aedes_1km.rds\")\ndim(rm.all)\n#> [1] 18234   183\nrm.all$occurrence = as.factor(ifelse(rm.all$individualCount>0, 1, 0))\nsummary(rm.all$occurrence)\n#>     0     1 \n#>  4400 13834\nsel.stat = c(gsub(\"4km\", \"1km\", tools::file_path_sans_ext(basename(eco.tifs))), \n             \"dtm_twi_merit.dem_m_1km_s0..0cm_2017_mood_v1\",\n             \"dtm_floodmap.500y_jrc.hazardmapping_m_1km_s0..0cm_1500..2016_mood_v1.0\",\n             \"dtm_slope_merit.dem_m_1km_s0..0cm_2017_mood_v1\",\n             paste0(\"adm_travel.time.to.cities.cl\", 1:10, \"_cgiar_m_1km_s0..0cm_2000..2020_mood_v1\"),\n             paste0(\"adm_travel.time.to.ports.cl\", 1:5, \"_cgiar_m_1km_s0..0cm_2000..2020_mood_v1\"))\npr.vars = c(sel.stat,  c(\"CRP\", \"FOR\", \"HFP\", \"LST\", \"N02\", \"N08\", \"NLT\", \"PPD\", \"PRE\", \"SNW\", \"T02\", \"T08\"))\nfm.fs = stats::as.formula(paste(\"occurrence ~ \", paste(pr.vars, collapse=\"+\")))\nstr(all.vars(fm.fs))\n#>  chr [1:45] \"occurrence\" ...\nlibrary(dplyr)\n#> \n#> Attaching package: 'dplyr'\n#> The following object is masked from 'package:matrixStats':\n#> \n#>     count\n#> The following objects are masked from 'package:terra':\n#> \n#>     collapse, select\n#> The following objects are masked from 'package:raster':\n#> \n#>     intersect, select, union\n#> The following objects are masked from 'package:stats':\n#> \n#>     filter, lag\n#> The following objects are masked from 'package:base':\n#> \n#>     intersect, setdiff, setequal, union\nfs.rm0 = rm.all %>% dplyr::sample_n(5000)\nf = \"./output/aedes/\"\ntnd.ml = tune_learners(data = fs.rm0, formula = fm.fs, \n                      blocking = factor(fs.rm0$ID), out.dir=f)\n#> Using learners: classif.ranger, classif.xgboost, classif.glmnet...TRUE\nt.m = train_sp_eml(data = rm.all, tune_result = tnd.ml, \n                   blocking = as.factor(rm.all$ID), out.dir = f)\nsummary(t.m$learner.model$super.model$learner.model)\n#> \n#> Call:\n#> stats::glm(formula = f, family = \"binomial\", data = getTaskData(.task, \n#>     .subset), weights = .weights, model = FALSE)\n#> \n#> Deviance Residuals: \n#>     Min       1Q   Median       3Q      Max  \n#> -3.2362   0.0986   0.0986   0.0986   4.2849  \n#> \n#> Coefficients:\n#>                 Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)     -12.0872     0.5069 -23.844  < 2e-16 ***\n#> classif.ranger   14.4862     0.7916  18.301  < 2e-16 ***\n#> classif.xgboost   1.0373     0.4281   2.423   0.0154 *  \n#> classif.glmnet    1.8872     0.3742   5.043 4.58e-07 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for binomial family taken to be 1)\n#> \n#>     Null deviance: 18985.9  on 17505  degrees of freedom\n#> Residual deviance:  1192.9  on 17502  degrees of freedom\n#> AIC: 1200.9\n#> \n#> Number of Fisher Scoring iterations: 9\nxl <- as.data.frame(mlr::getFeatureImportance(t.m[[\"learner.model\"]][[\"base.models\"]][[1]])$res)\nxl$relative_importance = round(100*xl$importance/sum(xl$importance), 1)\nxl = xl[order(xl$relative_importance, decreasing = T),]\nxl$variable = paste0(c(1:length(t.m$features)), \". \", xl$variable)\nxl[1:8,]\n#>                                                                    variable\n#> 40                                                                   1. NLT\n#> 27 2. adm_travel.time.to.cities.cl10_cgiar_m_1km_s0..0cm_2000..2020_mood_v1\n#> 26  3. adm_travel.time.to.cities.cl9_cgiar_m_1km_s0..0cm_2000..2020_mood_v1\n#> 31   4. adm_travel.time.to.ports.cl4_cgiar_m_1km_s0..0cm_2000..2020_mood_v1\n#> 24  5. adm_travel.time.to.cities.cl7_cgiar_m_1km_s0..0cm_2000..2020_mood_v1\n#> 41                                                                   6. PPD\n#> 32   7. adm_travel.time.to.ports.cl5_cgiar_m_1km_s0..0cm_2000..2020_mood_v1\n#> 9        8. clm_bioclim.1_chelsa.climate_m_1km_s0..0cm_1980..2010_mood_v2.1\n#>    importance relative_importance\n#> 40  1342.8258                21.5\n#> 27  1030.3510                16.5\n#> 26   692.6710                11.1\n#> 31   639.9700                10.3\n#> 24   445.3388                 7.1\n#> 41   419.0600                 6.7\n#> 32   181.0785                 2.9\n#> 9    177.5123                 2.8"},{"path":"spatiotemporal-machine-learning-for-species-distribution-modeling.html","id":"generating-the-trend-maps","chapter":"5 Spatiotemporal Machine Learning for Species Distribution Modeling","heading":"5.5 Generating the trend maps","text":"produced predictions Europe mosquito occurrence can also\nstack visualize animation:\nFigure 5.5: Spatiotemporal visualization predicted occurrence probability Tiger mosquito Zoom Spain.\ncan also derive beta coefficient per pixel using greenbrown package see\nparts Europe showing higher increase mosquito density. Example predictions Spain:Alternatively, can also derive beta coefficient using parallelization lm function.\nbasically fits model every time-series pixels returns vector.\nFigure 5.6: Trend map mosquito distribution time. Red color indicates increase occurrence probability, blue color decrease.\n","code":"\nlibrary(plotKML); library(raster)\nes.tifs = list.files(\"./output/spain1km\", glob2rx(\"aedes.albopictus_M_*.tif\"), full.names = TRUE)\nspain1km = raster::brick(raster::stack(es.tifs))\n#spplot(spain1km[[c(1,22)]], col.regions=SAGA_pal[[10]])\n#install.packages(\"greenbrown\", repos=\"http://R-Forge.R-project.org\")\nlibrary(greenbrown)\ntrendmap <- TrendRaster(spain1km, start=c(2000, 1), freq=1, breaks=1) \n## can be computationally intensive\nplot(trendmap[[\"SlopeSEG1\"]], \n     col=rev(SAGA_pal[[\"SG_COLORS_GREEN_GREY_RED\"]]), \n     zlim=c(-1.5,1.5), main=\"Slope SEG1\")\nxs = as(spain1km, \"SpatialGridDataFrame\")\nin.years = as.numeric(substr(sapply(basename(es.tifs), function(i){strsplit(i, \"_\")[[1]][3]}), 1, 4))\ncl <- parallel::makeCluster(parallel::detectCores())\nparallel::clusterExport(cl, c(\"in.years\"))\n## select ONLY pixels that change in time\nsd.pix = which(!parallel::parApply(cl, xs@data, 1, sd, na.rm=TRUE)==0)\n## derive beta per pixel:\n## Because probs are binary variable we use logit transformation\nbetas = unlist(parallel::parApply(cl, xs@data[sd.pix,], 1, function(i) { \n  try( round( coef(lm(y~x, data.frame(x=in.years, \n    y=boot::logit(ifelse(as.vector(i)<=0, 0.1, ifelse(as.vector(i)>=100, 99.9, as.vector(i)))/100))))[2] *1000 ) \n    ) }))\n## write to GeoTIF\nspain.trend = xs[1]\nspain.trend@data[,1] = 0\nspain.trend@data[sd.pix,1] = as.numeric(betas)\nrgdal::writeGDAL(spain.trend[1], \"./output/spain_trend_aedes_1km.tif\", type=\"Int16\", \n          mvFlag=-32768, options=c(\"COMPRESS=DEFLATE\"))\nparallel::stopCluster(cl)"},{"path":"spatiotemporal-machine-learning-for-species-distribution-modeling.html","id":"summary","chapter":"5 Spatiotemporal Machine Learning for Species Distribution Modeling","heading":"5.6 Summary","text":"chapter demonstrate use occurrence-records map distribution target species spacetime.\nuse Ensemble Machine Learning training data overlaid spacetime vs time-series\nNight Light, vegetation, climatic images (years 2000–2021). training data \nlimited occurrence-records neither based probability sampling\nconsistent time. Probably suited training data set example\nmeteorological stations across EU recorded daily records mosquito occurrence (0/1).\nsystematic records dense well distributed locations across Europe \nperfect data set exercise, even using limited GBIF records \nable produce relatively accurate maps probability occurrence.Note case study , also used time .e. Year observation\ncovariate. add Year covariate training point data, predictions\nprobably shown probability occurrence increasing significantly recent years.\nnecessarily match reality ground. Just \nrecords mosquito species recent years, mean\nmosquito appeared recent years. fact, use Year covariate\ndata, likely introduce bias predictions (Syfert, Smith, & Coomes, 2013).GBIF data course limited use SDM comes often misssing meta-information\n, course, huge gaps (Marcer et al., n.d.; Syfert et al., 2013). means also \nanalysis tutorial flawed considered trust-worthy;\nmeans running spatial analysis GBIF data done \ncare aiming correct interpretation.simple alternative derive (kernel) density maps mosquitos Europe\nuse sparr::spattemp.density function:problem usiong sparr::spattemp.density () can used\noccurrence records, (b) assumes systematically\nsampled occurrences phenomena available.\ncase, function sparr::spattemp.density also probably produce biased\nestimate distribution mosquitos time.Ensemble Machine Learning used generate predictions helps produce unbiased estimate \nmosquito occurrences Europe time-series years. disadvantages using\nspatiotemporal ML :correlating Earth Observation data mosquito density, mosquitos\ndynamic can sensed / seen space, least 1km resolution;\nrely correlation environmental factors can explain smaller part variation;assume GBIF training points representative various environmental\nconditions, practice less training data available physically distant\nareas e.g. mountains (hence training data can considered censored);Predictions produced example come relatively high errors extrapolation\nareas, predictions used together uncertainty maps;","code":"\nst.f = occ[,c(\"decimalLongitude\",\"decimalLatitude\",\"Date\",\"individualCount\")]\nst.f$individualCount = as.numeric(ifelse(is.na(st.f$individualCount), 1, st.f$individualCount))\ncoordinates(st.f) <- c(\"decimalLongitude\", \"decimalLatitude\")\nproj4string(st.f) <- CRS(\"+init=epsg:4326\")\ngrid1d = readGDAL(\"./input/mask_5km.tif\")\ngrid1d = as(grid1d, \"SpatialPixelsDataFrame\")\nte = as.vector(grid1d@bbox)\n#plot(grid1d)\nmg_owin <- spatstat.geom::as.owin(data.frame(x = grid1d@coords[,1], y = grid1d@coords[,2], window = TRUE))\nst.f_sp <- spTransform(st.f, grid1d@proj4string)\npp = ppp(x=st.f_sp@coords[,1], y=st.f_sp@coords[,2], marks=as.numeric(substr(st.f_sp$Date, 1, 4)), window = mg_owin)\n## Warning messages:\n#1: 41072 points were rejected as lying outside the specified window \n#2: data contain duplicated points\npp$n\n## 13893\nstr(pp$marks)\n## spacetime density ----\neu.stgrid = sparr::spattemp.density(pp, tt = pp$marks, tlim = c(2000,2022), sres=1569, verbose = TRUE)\n## Calculating trivariate smooth...Done.\n## Edge-correcting...Done.\n## Conditioning on time...Done.\n#plot(eu.stgrid, 2018)\ndmap <- maptools::as.SpatialGridDataFrame.im(eu.stgrid$z[[\"2012\"]])\nsummary(dmap$v*1e12)\nplot(dmap)"},{"path":"spatiotemporal-machine-learning-for-species-distribution-modeling.html","id":"acknowledgments","chapter":"5 Spatiotemporal Machine Learning for Species Distribution Modeling","heading":"5.7 Acknowledgments","text":"project received funding European Union’s Horizon 2020 Research innovation programme grant agreement 874850.","code":""},{"path":"multi-scale-spatial-prediction-models.html","id":"multi-scale-spatial-prediction-models","chapter":"6 Multi-scale spatial prediction models","heading":"6 Multi-scale spatial prediction models","text":"reading work--progress Spatial spatiotemporal interpolation using Ensemble Machine Learning. chapter currently draft version, peer-review publication pending. can find polished first edition https://opengeohub.github.io/spatial-prediction-eml/.","code":""},{"path":"multi-scale-spatial-prediction-models.html","id":"rationale-for-multiscale-models","chapter":"6 Multi-scale spatial prediction models","heading":"6.1 Rationale for multiscale models","text":"previous examples shown fit spatial spatiotemporal models\ngenerate predictions using multiple covariate layers. practice spatial layers\nused predictive mapping come different spatial scales .e. \nrepresent different part spatial variation. least two scales \nspatial variation (Tomislav Hengl et al., 2021):Coarse scale e.g. representing effects planetary climate;Fine scale e.g. representing meso-relief local conditions;fact, can imagine spatial variation can probably decomposed different\nscale components, illustrated plot .\nFigure 6.1: Decomposition signal spatial variation four components plus noise. Based McBratney (1998).\nidea modeling soil spatial variation different scales can traced back work McBratney (1998).\nalso suggests produce predictions models different components\nvariation, sum components produce ensemble prediction. rationale\n, case large datasets, can () significantly reduce size\ndata, (b) separate better focus modeling based component variation.","code":""},{"path":"multi-scale-spatial-prediction-models.html","id":"fitting-and-predicting-with-multiscale-models","chapter":"6 Multi-scale spatial prediction models","heading":"6.2 Fitting and predicting with multiscale models","text":"next example use EML make spatial predictions using data-set \ntwo sets covariates basically different resolutions 250-m 100-m. \nuse Edgeroi data-set (Malone, McBratney, Minasny, & Laslett, 2009) used commonly soil\nscience demonstrate 3D soil mapping soil organic carbon (g/kg) based \nsamples taken diagnostic soil horizons (multiple depth intervals):can fit two independent EML’s using two sets covariates \nproduce final predictions combining . refer two models \ncoarse fine-scale models. fine-scale models often much larger\ndatasets require serious computing capacity.","code":"\ndata(edgeroi)\nedgeroi.sp <- edgeroi$sites\ncoordinates(edgeroi.sp) <- ~ LONGDA94 + LATGDA94\nproj4string(edgeroi.sp) <- CRS(\"+proj=longlat +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +no_defs\")\nedgeroi.sp <- spTransform(edgeroi.sp, CRS(\"+init=epsg:28355\"))\nout.file = paste(getwd(), \"output/edgeroi/edgeroi_training_points.gpkg\", sep=\"/\")\n#if(!file.exists(\"out.file\")){\n#  writeOGR(edgeroi.sp, out.file, layer=\"edgeroi_training_points\", driver = \"GPKG\")\n#}"},{"path":"multi-scale-spatial-prediction-models.html","id":"coarse-scale-model","chapter":"6 Multi-scale spatial prediction models","heading":"6.3 Coarse-scale model","text":"First use 250-m resolution covariates:3D soil data set, also use horizon DEPTH explain distribution SOC soil:can now fit EML directly using derived regression matrix:geoR package reports problems data set 3D hence spatial\nduplicates. can ignore problem use pre-defined cell size 1-km\nspatial blocking, although theory one can also fit 3D variograms \ndetermine blocking parameter using training data.results show EML model significant:can now predict values e.g. 5-cm depth adding dummy spatial layer fixed values:shows following:\nFigure 6.2: Predicted SOC content using 250-m covariates.\naverage prediction error map somewhat higher average error model fitting:predicting top-soil SOC, exponentially higher soil surface hence average model errors top soil slightly larger mean error whole soil.","code":"\nload(\"input/edgeroi.grids.rda\")\ngridded(edgeroi.grids) <- ~x+y\nproj4string(edgeroi.grids) <- CRS(\"+init=epsg:28355\")\nov2 <- over(edgeroi.sp, edgeroi.grids)\nov2$SOURCEID <- edgeroi.sp$SOURCEID\nov2$x = edgeroi.sp@coords[,1]\nov2$y = edgeroi.sp@coords[,2]\nsource(\"PSM_functions.R\")\nh2 <- hor2xyd(edgeroi$horizons)\n## regression matrix:\nrm2 <- plyr::join_all(dfs = list(edgeroi$sites, h2, ov2))\n#> Joining by: SOURCEID\n#> Joining by: SOURCEID\nformulaStringP2 <- ORCDRC ~ DEMSRT5+TWISRT5+EV1MOD5+EV2MOD5+EV3MOD5+DEPTH\nrmP2 <- rm2[complete.cases(rm2[,all.vars(formulaStringP2)]),]\nstr(rmP2[,all.vars(formulaStringP2)])\n#> 'data.frame':    4972 obs. of  7 variables:\n#>  $ ORCDRC : num  8.5 7.3 5 4.7 4.7 ...\n#>  $ DEMSRT5: num  198 198 198 198 198 198 185 185 185 185 ...\n#>  $ TWISRT5: num  19.5 19.5 19.5 19.5 19.5 19.5 19.2 19.2 19.2 19.2 ...\n#>  $ EV1MOD5: num  1.14 1.14 1.14 1.14 1.14 1.14 -4.7 -4.7 -4.7 -4.7 ...\n#>  $ EV2MOD5: num  1.62 1.62 1.62 1.62 1.62 1.62 3.46 3.46 3.46 3.46 ...\n#>  $ EV3MOD5: num  -5.74 -5.74 -5.74 -5.74 -5.74 -5.74 0.01 0.01 0.01 0.01 ...\n#>  $ DEPTH  : num  11.5 17.5 26 55 80 ...\nif(!exists(\"m.oc\")){\n  m.oc = train.spLearner.matrix(rmP2, formulaStringP2, edgeroi.grids, \n                        parallel=FALSE, cov.model=\"nugget\", cell.size=1000)\n}\n#> as.geodata: 4655 replicated data locations found. \n#>  Consider using jitterDupCoords() for jittering replicated locations. \n#> WARNING: there are data at coincident or very closed locations, some of the geoR's functions may not work.\n#>  Use function dup.coords() to locate duplicated coordinates.\n#>  Consider using jitterDupCoords() for jittering replicated locations \n#> # weights:  25\n#> initial  value 253895.808438 \n#> iter  10 value 131424.395513\n#> iter  20 value 92375.545449\n#> iter  30 value 88023.497878\n#> iter  40 value 78161.622563\n#> iter  50 value 71869.588437\n#> iter  60 value 69482.655270\n#> iter  70 value 68642.175713\n#> iter  80 value 68405.024865\n#> iter  90 value 68402.034341\n#> final  value 68402.000242 \n#> converged\n#> # weights:  25\n#> initial  value 254790.970425 \n#> final  value 136782.219163 \n#> converged\n#> # weights:  25\n#> initial  value 285010.126650 \n#> final  value 135478.529982 \n#> converged\n#> # weights:  25\n#> initial  value 253832.562811 \n#> final  value 137484.280876 \n#> converged\n#> # weights:  25\n#> initial  value 254385.881547 \n#> iter  10 value 98551.221418\n#> iter  20 value 94579.214923\n#> iter  30 value 93473.664614\n#> iter  40 value 93169.177514\n#> iter  50 value 93139.660457\n#> iter  60 value 93111.617240\n#> iter  70 value 93111.256287\n#> iter  80 value 93109.156591\n#> iter  90 value 93099.925818\n#> iter 100 value 93021.880998\n#> final  value 93021.880998 \n#> stopped after 100 iterations\n#> # weights:  25\n#> initial  value 233465.782262 \n#> final  value 134647.206038 \n#> converged\n#> # weights:  25\n#> initial  value 246624.689888 \n#> final  value 138702.343415 \n#> converged\n#> # weights:  25\n#> initial  value 241227.341802 \n#> final  value 138599.168021 \n#> converged\n#> # weights:  25\n#> initial  value 245735.599010 \n#> final  value 131152.446689 \n#> converged\n#> # weights:  25\n#> initial  value 258267.657849 \n#> iter  10 value 97368.003255\n#> iter  20 value 91058.331259\n#> iter  30 value 88735.472097\n#> iter  40 value 78495.790097\n#> iter  50 value 72384.348608\n#> iter  60 value 69221.579541\n#> iter  70 value 68248.107158\n#> iter  80 value 68073.306877\n#> iter  80 value 68073.306456\n#> iter  90 value 68072.812887\n#> iter  90 value 68072.812254\n#> final  value 68072.784171 \n#> converged\n#> # weights:  25\n#> initial  value 268786.613045 \n#> iter  10 value 128937.282143\n#> iter  20 value 105536.706972\n#> iter  30 value 100970.717402\n#> iter  40 value 89676.958144\n#> iter  50 value 80499.715395\n#> iter  60 value 76269.748745\n#> iter  70 value 74645.990665\n#> iter  80 value 74429.114194\n#> iter  90 value 74041.488710\n#> iter 100 value 73877.378810\n#> final  value 73877.378810 \n#> stopped after 100 iterations\nsummary(m.oc@spModel$learner.model$super.model$learner.model)\n#> \n#> Call:\n#> stats::lm(formula = f, data = d)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -17.668  -0.984  -0.066   0.711  64.291 \n#> \n#> Coefficients:\n#>               Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)    0.11730    0.18365   0.639  0.52302    \n#> regr.ranger    1.12706    0.02474  45.553  < 2e-16 ***\n#> regr.xgboost  -0.37833    0.07448  -5.080 3.92e-07 ***\n#> regr.nnet     -0.04227    0.02399  -1.762  0.07808 .  \n#> regr.ksvm      0.08299    0.02900   2.861  0.00423 ** \n#> regr.cvglmnet -0.05140    0.03879  -1.325  0.18525    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 3.06 on 4966 degrees of freedom\n#> Multiple R-squared:  0.6938, Adjusted R-squared:  0.6935 \n#> F-statistic:  2250 on 5 and 4966 DF,  p-value: < 2.2e-16\nout.tif = \"output/edgeroi/pred_oc_250m.tif\"\nedgeroi.grids$DEPTH <- 5\nif(!exists(\"edgeroi.oc\")){\n  edgeroi.oc = predict(m.oc, edgeroi.grids[,m.oc@spModel$features])\n}\n#> Predicting values using 'getStackedBaseLearnerPredictions'...TRUE\n#> Deriving model errors using forestError package...TRUE\nif(!file.exists(out.tif)){\n  writeGDAL(edgeroi.oc$pred[\"response\"], out.tif, \n            options = c(\"COMPRESS=DEFLATE\"))\n  writeGDAL(edgeroi.oc$pred[\"model.error\"], \"output/edgeroi/pred_oc_250m_pe.tif\", \n            options = c(\"COMPRESS=DEFLATE\"))\n}\nsummary(edgeroi.oc$pred$model.error)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>   2.323   4.710   6.340   6.929   9.257  15.338"},{"path":"multi-scale-spatial-prediction-models.html","id":"fine-scale-model","chapter":"6 Multi-scale spatial prediction models","heading":"6.4 Fine-scale model","text":"can now fit fine-scale model independently coarse-scale model\nusing 100-m resolution covariates. case 100-m covariates \nbased Landsat 8 gamma radiometrics images (see ?edgeroi details):fit 2nd fine-scale model:shows 100-m resolution covariates help make even accurate\npredictions R-square 0.7. can also make predictions 5-cm depth\nusing (note: takes almost 6x time compute predictions \n250-m resolution data):shows following:\nFigure 6.3: Predicted SOC content using 100-m covariates.\n","code":"\nedgeroi.grids100 = readRDS(\"input/edgeroi.grids.100m.rds\")\n#gridded(edgeroi.grids100) <- ~x+y\n#proj4string(edgeroi.grids100) <- CRS(\"+init=epsg:28355\")\novF <- over(edgeroi.sp, edgeroi.grids100)\novF$SOURCEID <- edgeroi.sp$SOURCEID\novF$x = edgeroi.sp@coords[,1]\novF$y = edgeroi.sp@coords[,2]\nrmF <- plyr::join_all(dfs = list(edgeroi$sites, h2, ovF))\n#> Joining by: SOURCEID\n#> Joining by: SOURCEID\nformulaStringPF <- ORCDRC ~ MVBSRT6+TI1LAN6+TI2LAN6+PCKGAD6+RUTGAD6+PCTGAD6+DEPTH\nrmPF <- rmF[complete.cases(rmF[,all.vars(formulaStringPF)]),]\nstr(rmPF[,all.vars(formulaStringPF)])\n#> 'data.frame':    5001 obs. of  8 variables:\n#>  $ ORCDRC : num  8.5 7.3 5 4.7 4.7 ...\n#>  $ MVBSRT6: num  5.97 5.97 5.97 5.97 5.97 5.97 6.7 6.7 6.7 6.7 ...\n#>  $ TI1LAN6: num  31.8 31.8 31.8 31.8 31.8 31.8 14.3 14.3 14.3 14.3 ...\n#>  $ TI2LAN6: num  32.9 32.9 32.9 32.9 32.9 32.9 22.1 22.1 22.1 22.1 ...\n#>  $ PCKGAD6: num  1.39 1.39 1.39 1.39 1.39 1.39 1.06 1.06 1.06 1.06 ...\n#>  $ RUTGAD6: num  0.14 0.14 0.14 0.14 0.14 0.14 0.16 0.16 0.16 0.16 ...\n#>  $ PCTGAD6: num  7.82 7.82 7.82 7.82 7.82 7.82 6.48 6.48 6.48 6.48 ...\n#>  $ DEPTH  : num  11.5 17.5 26 55 80 ...\nif(!exists(\"m.ocF\")){\n  m.ocF = train.spLearner.matrix(rmPF, formulaStringPF, edgeroi.grids100, \n                        parallel=FALSE, cov.model=\"nugget\", cell.size=1000)\n}\n#> as.geodata: 4655 replicated data locations found. \n#>  Consider using jitterDupCoords() for jittering replicated locations. \n#> WARNING: there are data at coincident or very closed locations, some of the geoR's functions may not work.\n#>  Use function dup.coords() to locate duplicated coordinates.\n#>  Consider using jitterDupCoords() for jittering replicated locations \n#> # weights:  28\n#> initial  value 259952.010839 \n#> iter  10 value 101252.397395\n#> iter  20 value 92849.746212\n#> iter  30 value 84138.890538\n#> iter  40 value 81171.674547\n#> iter  50 value 80244.316068\n#> iter  60 value 79942.793909\n#> iter  70 value 79368.296567\n#> iter  80 value 78323.201957\n#> iter  90 value 77181.404075\n#> iter 100 value 76708.190347\n#> final  value 76708.190347 \n#> stopped after 100 iterations\n#> # weights:  28\n#> initial  value 226902.901029 \n#> iter  10 value 134520.116202\n#> iter  20 value 106665.533239\n#> iter  30 value 100456.523529\n#> iter  40 value 94907.032527\n#> iter  50 value 94598.860459\n#> iter  60 value 94311.934401\n#> iter  70 value 93110.357174\n#> iter  80 value 92843.643684\n#> iter  90 value 92584.240506\n#> iter 100 value 92181.626754\n#> final  value 92181.626754 \n#> stopped after 100 iterations\n#> # weights:  28\n#> initial  value 234725.323531 \n#> iter  10 value 99738.688037\n#> iter  20 value 95202.777671\n#> iter  30 value 93332.714310\n#> iter  40 value 84502.258499\n#> iter  50 value 81245.631274\n#> iter  60 value 80530.199169\n#> iter  70 value 79322.812977\n#> iter  80 value 78753.418715\n#> iter  90 value 78202.739233\n#> iter 100 value 76867.515069\n#> final  value 76867.515069 \n#> stopped after 100 iterations\n#> # weights:  28\n#> initial  value 264624.170952 \n#> iter  10 value 101566.765280\n#> iter  20 value 93105.271953\n#> iter  30 value 79221.563953\n#> iter  40 value 75437.096559\n#> iter  50 value 74819.981899\n#> iter  60 value 74258.787761\n#> iter  70 value 72481.383976\n#> iter  80 value 71415.613349\n#> iter  90 value 69310.427661\n#> iter 100 value 66134.495814\n#> final  value 66134.495814 \n#> stopped after 100 iterations\n#> # weights:  28\n#> initial  value 269334.603789 \n#> iter  10 value 114589.803267\n#> iter  20 value 98585.812766\n#> iter  30 value 95611.345448\n#> iter  40 value 94119.323815\n#> iter  50 value 92069.210575\n#> iter  60 value 90209.532038\n#> iter  70 value 86238.674926\n#> iter  80 value 82014.171622\n#> iter  90 value 78106.825699\n#> iter 100 value 76099.552544\n#> final  value 76099.552544 \n#> stopped after 100 iterations\n#> # weights:  28\n#> initial  value 259884.878897 \n#> iter  10 value 103726.379375\n#> iter  20 value 94570.752266\n#> iter  30 value 76708.677450\n#> iter  40 value 73787.991288\n#> iter  50 value 73126.652279\n#> iter  60 value 72979.797218\n#> iter  70 value 72965.952547\n#> iter  80 value 72468.280591\n#> iter  90 value 72359.464672\n#> iter 100 value 71873.895760\n#> final  value 71873.895760 \n#> stopped after 100 iterations\n#> # weights:  28\n#> initial  value 285193.065615 \n#> iter  10 value 102340.204856\n#> iter  20 value 89532.696472\n#> iter  30 value 83391.890007\n#> iter  40 value 79203.819730\n#> iter  50 value 75520.520094\n#> iter  60 value 71641.894828\n#> iter  70 value 66583.248718\n#> iter  80 value 64316.930097\n#> iter  90 value 64067.020769\n#> iter 100 value 63723.399424\n#> final  value 63723.399424 \n#> stopped after 100 iterations\n#> # weights:  28\n#> initial  value 209074.598360 \n#> iter  10 value 92158.672417\n#> iter  20 value 88477.518932\n#> iter  30 value 83611.323463\n#> iter  40 value 80398.938707\n#> iter  50 value 75639.793693\n#> iter  60 value 72497.710902\n#> iter  70 value 68574.884188\n#> iter  80 value 66515.331380\n#> iter  90 value 64170.821850\n#> iter 100 value 63336.087094\n#> final  value 63336.087094 \n#> stopped after 100 iterations\n#> # weights:  28\n#> initial  value 264913.914581 \n#> iter  10 value 90799.203779\n#> iter  20 value 83326.307020\n#> iter  30 value 71495.281328\n#> iter  40 value 66078.704046\n#> iter  50 value 64850.594409\n#> iter  60 value 64005.390526\n#> iter  70 value 63563.194716\n#> iter  80 value 62770.259605\n#> iter  90 value 62084.800044\n#> iter 100 value 60518.371937\n#> final  value 60518.371937 \n#> stopped after 100 iterations\n#> # weights:  28\n#> initial  value 254133.236100 \n#> iter  10 value 117010.387846\n#> iter  20 value 96496.685965\n#> iter  30 value 81754.875719\n#> iter  40 value 80702.591180\n#> iter  50 value 79637.660106\n#> iter  60 value 77195.047203\n#> iter  70 value 74660.187056\n#> iter  80 value 70931.471910\n#> iter  90 value 69014.578845\n#> iter 100 value 68409.495245\n#> final  value 68409.495245 \n#> stopped after 100 iterations\n#> # weights:  28\n#> initial  value 325116.468080 \n#> iter  10 value 120203.210463\n#> iter  20 value 115964.153279\n#> iter  30 value 90105.298970\n#> iter  40 value 79437.548718\n#> iter  50 value 70168.831437\n#> iter  60 value 69050.767980\n#> iter  70 value 68627.297281\n#> iter  80 value 68156.983626\n#> iter  90 value 67374.568621\n#> iter 100 value 67164.637038\n#> final  value 67164.637038 \n#> stopped after 100 iterations\nsummary(m.ocF@spModel$learner.model$super.model$learner.model)\n#> \n#> Call:\n#> stats::lm(formula = f, data = d)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -19.222  -0.952  -0.047   0.726  61.162 \n#> \n#> Coefficients:\n#>               Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)   -0.47135    0.11510  -4.095 4.29e-05 ***\n#> regr.ranger    1.10204    0.02467  44.667  < 2e-16 ***\n#> regr.xgboost   0.03329    0.07566   0.440    0.660    \n#> regr.nnet      0.03135    0.02196   1.428    0.153    \n#> regr.ksvm     -0.02556    0.02965  -0.862    0.389    \n#> regr.cvglmnet -0.03575    0.02887  -1.238    0.216    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 2.967 on 4995 degrees of freedom\n#> Multiple R-squared:  0.7146, Adjusted R-squared:  0.7143 \n#> F-statistic:  2501 on 5 and 4995 DF,  p-value: < 2.2e-16\nedgeroi.grids100$DEPTH <- 5\nsel.grid = complete.cases(edgeroi.grids100@data[,m.ocF@spModel$features])\nif(!exists(\"edgeroi.ocF\")){\n  edgeroi.ocF = predict(m.ocF, edgeroi.grids100[sel.grid, m.ocF@spModel$features])\n}\n#> Predicting values using 'getStackedBaseLearnerPredictions'...TRUE\n#> Deriving model errors using forestError package...TRUE\nout.tif = \"output/edgeroi/pred_oc_100m.tif\"\nif(!file.exists(out.tif)){\n  writeGDAL(edgeroi.ocF$pred[\"response\"], out.tif, options = c(\"COMPRESS=DEFLATE\"))\n  writeGDAL(edgeroi.ocF$pred[\"model.error\"], \"output/edgeroi/pred_oc_100m_pe.tif\", options = c(\"COMPRESS=DEFLATE\"))\n}"},{"path":"multi-scale-spatial-prediction-models.html","id":"merging-multi-scale-predictions","chapter":"6 Multi-scale spatial prediction models","heading":"6.5 Merging multi-scale predictions","text":"compare coarse scale fine scale predictions see:\nFigure 6.4: Coarse-scale fine-scale predictions soil organic carbon 5-cm depth Edgeroi study area.\nOverall match general patterns also differences locally. expect two models fitted independently using completely different covariates. can merge two predictions produce final ensemble prediction using following principles:User prediction errors per pixel weights accurate predictions get higher weights,Derive propagated error using pooled variance based individual predictions errors,run operation, need downscale maps grid, best using Cubic-splines GDAL:can now read downscaled predictions, merge using prediction errors weights (weighted average per pixel):final map predictions combination two independently produced predictions (Tomislav Hengl et al., 2021):\nFigure 6.5: Merged predictions (coarse+fine scale) SOC content 100-m.\nmerge prediction errors, use pooled variance formula (Rudmin, 2010):summary, merging multi-scale predictions straight forward process,\nassumes reliable prediction errors available coarse fine scale predictions.\npooled variance might show higher errors predictions independent\nmodels differ significantly correct. 2-scale Ensemble Machine\nLearning method Predictive Soil Mapping used, example, produce\npredictions soil properties nutrients Africa 30-m spatial resolution (Tomislav Hengl et al., 2021).","code":"\nedgeroi.grids100@bbox\n#>       min     max\n#> x  741400  789000\n#> y 6646000 6678100\noutD.file = \"output/edgeroi/pred_oc_250m_100m.tif\"\nif(!file.exists(outD.file)){\n  system(paste0('gdalwarp output/edgeroi/pred_oc_250m.tif ', outD.file,  \n         ' -r \\\"cubicspline\\\" -te 741400 6646000 789000 6678100 -tr 100 100 -overwrite'))\n  system(paste0('gdalwarp output/edgeroi/pred_oc_250m_pe.tif output/edgeroi/pred_oc_250m_100m_pe.tif',\n         ' -r \\\"cubicspline\\\" -te 741400 6646000 789000 6678100 -tr 100 100 -overwrite'))\n}\nsel.pix = edgeroi.ocF$pred@grid.index\nedgeroi.ocF$pred$responseC = readGDAL(\"output/edgeroi/pred_oc_250m_100m.tif\")$band1[sel.pix]\n#> output/edgeroi/pred_oc_250m_100m.tif has GDAL driver GTiff \n#> and has 321 rows and 476 columns\nedgeroi.ocF$pred$model.errorC = readGDAL(\"output/edgeroi/pred_oc_250m_100m_pe.tif\")$band1[sel.pix]\n#> output/edgeroi/pred_oc_250m_100m_pe.tif has GDAL driver GTiff \n#> and has 321 rows and 476 columns\nX = comp.var(edgeroi.ocF$pred@data, r1=\"response\", r2=\"responseC\", v1=\"model.error\", v2=\"model.errorC\")\nedgeroi.ocF$pred$responseF = X$response\nout.tif = \"output/edgeroi/pred_oc_100m_merged.tif\"\nif(!file.exists(out.tif)){\n  writeGDAL(edgeroi.ocF$pred[\"responseF\"], out.tif, options = c(\"COMPRESS=DEFLATE\"))\n}\nplot(raster(edgeroi.ocF$pred[\"responseF\"]), col=R_pal[[\"rainbow_75\"]][4:20],\n  main=\"Merged predictions spLearner\", axes=FALSE, box=FALSE)\npoints(edgeroi.sp, pch=\"+\", cex=.8)\ncomp.var\n#> function (x, r1, r2, v1, v2) \n#> {\n#>     r = rowSums(x[, c(r1, r2)] * 1/(x[, c(v1, v2)]^2))/rowSums(1/(x[, \n#>         c(v1, v2)]^2))\n#>     v = sqrt(rowMeans(x[, c(r1, r2)]^2 + x[, c(v1, v2)]^2) - \n#>         rowMeans(x[, c(r1, r2)])^2)\n#>     return(data.frame(response = r, stdev = v))\n#> }\n#> <bytecode: 0x186373b0>\nedgeroi.ocF$pred$model.errorF = X$stdev\nout.tif = \"output/edgeroi/pred_oc_100m_merged_pe.tif\"\nif(!file.exists(out.tif)){\n  writeGDAL(edgeroi.ocF$pred[\"model.errorF\"], out.tif, options = c(\"COMPRESS=DEFLATE\"))\n}"},{"path":"summary-1.html","id":"summary-1","chapter":"7 Summary","heading":"7 Summary","text":"reading work--progress Spatial spatiotemporal interpolation using Ensemble Machine Learning. chapter currently draft version, peer-review publication pending. can find polished first edition https://opengeohub.github.io/spatial-prediction-eml/.tutorial demonstrates use Ensemble Machine Learning predictive\nmapping going numeric 2D, factor 3D variables. mind \nexamples shown based relatively small datasets, can still become\ncomputational add even learners. principle recommend:adding learners significantly less accurate best learners\n(.e. focusing top 4–5 best performing learners),fitting EML <50–100 training points,fitting EML spatial interpolation points heavily spatially clustered,using landmap package large datasets,derivation prediction error prediction interval recommend using \nmethod Lu & Hardin (2021). method default produces three measures uncertainty:Root Mean Square Prediction Error (RMSPE) = estimated conditional mean squared prediction errors random forest predictions,bias = estimated conditional biases random forest predictions,lower upper bounds / prediction intervals given probability e.g. 0.05 95% probability interval,can also follow introduction Ensemble Machine Learning Open Data Science Europe workshop video recordings.Please note mlr package discontinued, example might become unstable time. working migrating code landmap package make train.spLearner function work new mlr3 package.dataset used test Ensemble Machine Learning, please come back us share experiences posting issue /providing screenshot results.","code":""},{"path":"references.html","id":"references","chapter":"8 References","heading":"8 References","text":"","code":""}]
