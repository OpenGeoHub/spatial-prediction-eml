% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  graybox,natbib,nospthms]{svmono}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
  \setmonofont[Scale=0.7]{Source Code Pro}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Spatial and spatiotemporal interpolation using Ensemble Machine Learning},
  pdfauthor={Tom Hengl, Leandro Parente and Carmelo Bonannella},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage[paperwidth=18.90cm, paperheight=24.58cm, top=2.1cm, bottom=2.1cm, inner=2cm, outer=2cm]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.33,0.33,0.33}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.61,0.61,0.61}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.06,0.06,0.06}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.5,0.5,0.5}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0,0,0}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.27,0.27,0.27}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.27,0.27,0.27}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.06,0.06,0.06}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.14,0.14,0.14}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.06,0.06,0.06}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0,0,0}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.27,0.27,0.27}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.43,0.43,0.43}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0,0,0}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.5,0.5,0.5}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.5,0.5,0.5}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0,0,0}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.5,0.5,0.5}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{amsmath} % if desired
\usepackage{unicode-math}
%\setmathfont{latinmodern-math.otf}
\usepackage{float}
\usepackage{color}
\usepackage{fancyvrb}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{graphicx}
\providecommand{\tightlist}{\setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
%\usepackage[includemp,
%            paperwidth=18.90cm,
%            paperheight=24.58cm,
%            top=2.170cm,
%            bottom=3.510cm,
%            inner=2.1835cm,
%            outer=2.1835cm,
%            marginparwidth=4cm,
%            marginparsep=0.4cm]{geometry}

% Margin paragraphs:
%\setlength{\marginparwidth}{1.8cm}
%\setlength{\marginparsep}{4pt}

\usepackage[hyphens]{url}
\usepackage{hyperref}

% Place links in parens
\renewcommand{\href}[2]{#2 (\url{#1})}
% Use auto ref for internal links
\let\oldhyperlink=\hyperlink
\renewcommand{\hyperlink}[2]{\autoref{#1}}
\def\chapterautorefname{Chapter}
\def\sectionautorefname{Section}
\def\subsectionautorefname{Section}
\def\subsubsectionautorefname{Section}

\setlength{\emergencystretch}{3em}  % prevent overfull lines
\vbadness=10000 % suppress underfull \vbox
\hbadness=10000 % suppress underfull \vbox
\hfuzz=10pt

\usepackage{framed,color}
\definecolor{shadecolor}{RGB}{231,231,231}

\renewcommand{\textfraction}{0.05}
\renewcommand{\topfraction}{0.8}
\renewcommand{\bottomfraction}{0.8}
\renewcommand{\floatpagefraction}{0.75}

\renewenvironment{quote}{\begin{VF}}{\end{VF}}
\let\oldhref\href
\renewcommand{\href}[2]{#2\footnote{\url{#1}}}

\let\BeginKnitrBlock\begin \let\EndKnitrBlock\end

\ifxetex
  \usepackage{letltxmacro}
  \setlength{\XeTeXLinkMargin}{1pt}
  \LetLtxMacro\SavedIncludeGraphics\includegraphics
  \def\includegraphics#1#{% #1 catches optional stuff (star/opt. arg.)
    \IncludeGraphicsAux{#1}%
  }%
  \newcommand*{\IncludeGraphicsAux}[2]{%
    \XeTeXLinkBox{%
      \SavedIncludeGraphics#1{#2}%
    }%
  }%
\fi

\makeatletter
\newenvironment{kframe}{%
\medskip{}
\setlength{\fboxsep}{.8em}
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\makeatletter
\@ifundefined{Shaded}{
}{\renewenvironment{Shaded}{\begin{kframe}}{\end{kframe}}}
\makeatother

\newenvironment{rmdblock}[1]
  {
  % \begin{itemize}
  % \renewcommand{\labelitemi}{
  %   \raisebox{-.7\height}[0pt][0pt]{
  %     {\setkeys{Gin}{width=3em,keepaspectratio}\includegraphics{images/#1}}
  %   }
  % }
  % \setlength{\fboxsep}{10em}
  \begin{kframe}
  % \item
  }
  {
  \end{kframe}
  % \end{itemize}
  }
\newenvironment{rmdnote}
  {\begin{rmdblock}{note}}
  {\end{rmdblock}}
\newenvironment{rmdcaution}
  {\begin{rmdblock}{caution}}
  {\end{rmdblock}}
\newenvironment{rmdimportant}
  {\begin{rmdblock}{important}}
  {\end{rmdblock}}
\newenvironment{rmdtip}
  {\begin{rmdblock}{tip}}
  {\end{rmdblock}}
\newenvironment{rmdwarning}
  {\begin{rmdblock}{warning}}
  {\end{rmdblock}}

\usepackage{makeidx}
\makeindex
\ifluatex
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{natbib}
\bibliographystyle{spbasic}

\title{Spatial and spatiotemporal interpolation using Ensemble Machine Learning}
\author{Tom Hengl, Leandro Parente and Carmelo Bonannella}
\date{}

\begin{document}
\maketitle

%\cleardoublepage\newpage\thispagestyle{empty}\null
%\cleardoublepage\newpage\thispagestyle{empty}\null
%\cleardoublepage\newpage
\thispagestyle{empty}
\begin{center}
% \includegraphics{images/dedication.pdf}
\end{center}

\setlength{\abovedisplayskip}{-5pt}
\setlength{\abovedisplayshortskip}{-5pt}

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\hypertarget{introduction}{%
\chapter*{Introduction}\label{introduction}}
\addcontentsline{toc}{chapter}{Introduction}

\hypertarget{ensemble-machine-learning}{%
\section*{Ensemble Machine Learning}\label{ensemble-machine-learning}}
\addcontentsline{toc}{section}{Ensemble Machine Learning}

\href{https://opengeohub.github.io/spatial-prediction-eml/}{\includegraphics[width=2.60417in,height=\textheight]{cover.jpg}} This \href{https://opengeohub.github.io/spatial-prediction-eml/}{Rmarkdown tutorial} provides practical instructions, illustrated with sample
dataset, on how to use Ensemble Machine Learning to generate predictions (maps) from
2D, 3D, 2D+T (spatiotemporal) training (point) datasets. We show functionality to do
automated benchmarking for spatial/spatiotemporal prediction problems, and for which
we use primarily the mlr framework and spatial packages terra, rgdal and similar..

Ensembles are predictive models that combine predictions from two or more learners
\citep{seni2010ensemble, zhang2012ensemble}. The specific benefits of using Ensemble learners are:

\begin{itemize}
\tightlist
\item
  \textbf{Performance}: they can help improve the average prediction performance over any individual contributing learner in the ensemble.
\item
  \textbf{Robustness}: they can help reduce extrapolation / overshooting effects of individual learners.
\item
  \textbf{Unbiasness}: they can help determine a model-free estimate of prediction errors.
\end{itemize}

Even the most flexible and best performing learners such as Random Forest or neural
networks always carry a bias in the sense that the fitting produces recognizable
patterns and these are limited by the properties of the algorithm. In the case of
ensembles, the modeling algorithm becomes secondary, and even though the improvements
in accuracy are often minor as compared to the best individual learner, there is
a good chance that the final EML model will be less prone to overshooting and
extrapolation problems.

There are in principle three ways to apply ensembles \citep{zhang2012ensemble}:

\begin{itemize}
\tightlist
\item
  \emph{bagging}: learn in parallel, then combine using some deterministic principle (e.g.~weighted averaging),
\item
  \emph{boosting}: learn sequentially in an adaptive way, then combine using some deterministic principle,
\item
  \emph{stacking}: learn in parallel, then fit a meta-model to predict ensemble estimates,
\end{itemize}

The \emph{``meta-model''} is an additional model that basically combines all individual
or \emph{``base learners''}. In this tutorial we focus only on the stacking approach to Ensemble ML.

There are several packages in R that implement Ensemble ML, for example:

\begin{itemize}
\tightlist
\item
  \href{https://cran.r-project.org/web/packages/SuperLearner/vignettes/Guide-to-SuperLearner.html}{SuperLearner} package,
\item
  \href{https://cran.r-project.org/web/packages/caretEnsemble/vignettes/caretEnsemble-intro.html}{caretEnsemble} package,
\item
  \href{http://docs.h2o.ai/h2o-tutorials/latest-stable/tutorials/ensembles-stacking/index.html}{h2o.stackedEnsemble} package,
\item
  \href{https://mlr.mlr-org.com/reference/makeStackedLearner.html}{mlr} and \href{https://mlr3gallery.mlr-org.com/posts/2020-04-27-tuning-stacking/}{mlr3} packages,
\end{itemize}

Ensemble ML is also available in Python through the \href{https://scikit-learn.org/stable/modules/ensemble.html}{scikit-learn} library.

In this tutorial we focus primarily on using the \href{https://mlr.mlr-org.com/}{mlr package},
i.e.~a wrapper functions to mlr implemented in the landmap package.

\hypertarget{using-geographical-distances-to-improve-spatial-interpolation}{%
\section*{Using geographical distances to improve spatial interpolation}\label{using-geographical-distances-to-improve-spatial-interpolation}}
\addcontentsline{toc}{section}{Using geographical distances to improve spatial interpolation}

Machine Learning was for long time been considered suboptimal for spatial
interpolation problems, in comparison to classifical geostatistical techniques
such as kriging, because it basically ignores spatial dependence structure in
the data. To incorporate spatial dependence structures in machine learning, one
can now add the so-called ``geographical features'': buffer distance, oblique
distances, and/or distances in the watershed, as features. This has shown to
improve prediction performance and produce maps that visually appear as they
have been produced by kriging \citep{hengl2018random}.

Use of geographical as features in machine learning for spatial predictions is explained in detail in:

\begin{itemize}
\tightlist
\item
  Behrens, T., Schmidt, K., Viscarra Rossel, R. A., Gries, P., Scholten, T., \& MacMillan, R. A. (2018). \href{https://doi.org/10.1111/ejss.12687}{Spatial modelling with Euclidean distance fields and machine learning}. European journal of soil science, 69(5), 757-770.
\item
  Hengl, T., Nussbaum, M., Wright, M. N., Heuvelink, G. B., \& Gräler, B. (2018). \href{https://doi.org/10.7717/peerj.5518}{Random forest as a generic framework for predictive modeling of spatial and spatio-temporal variables}. PeerJ, 6, e5518. \url{https://doi.org/10.7717/peerj.5518}\\
\item
  Møller, A. B., Beucher, A. M., Pouladi, N., and Greve, M. H. (2020). \href{https://doi.org/10.5194/soil-6-269-2020}{Oblique geographic coordinates as covariates for digital soil mapping}. SOIL, 6, 269--289, \url{https://doi.org/10.5194/soil-6-269-2020}
\item
  Sekulić, A., Kilibarda, M., Heuvelink, G.B., Nikolić, M., Bajat, B. (2020). \href{https://doi.org/10.3390/rs12101687}{Random Forest Spatial Interpolation}. Remote Sens. 12, 1687. \url{https://doi.org/10.3390/rs12101687}
\end{itemize}

In the case the number of covariates / features becomes large, and assuming the
covariates are diverse, and that the points are equally spread in an area of
interest, there is probably no need for using geographical distances in model
training because unique combinations of features become so large that they can
be used to represent \emph{geographical position} \citep{hengl2018random}.

\hypertarget{installing-the-landmap-package}{%
\section*{Installing the landmap package}\label{installing-the-landmap-package}}
\addcontentsline{toc}{section}{Installing the landmap package}

To install the most recent landmap package from Github use:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(devtools)}
\FunctionTok{install\_github}\NormalTok{(}\StringTok{"envirometrix/landmap"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{important-literature}{%
\section*{Important literature}\label{important-literature}}
\addcontentsline{toc}{section}{Important literature}

For an introduction to Spatial Data Science and Machine Learning with R we
recommend studying first:

\begin{itemize}
\tightlist
\item
  Becker, M. et al.: \textbf{\href{https://mlr3book.mlr-org.com/}{``mlr3 book''}};\\
\item
  Bivand, R., Pebesma, E. and Gómez-Rubio, V.: \textbf{\href{https://asdar-book.org/}{``Applied Spatial Data Analysis with R''}};\\
\item
  Irizarry, R.A.: \textbf{\href{https://rafalab.github.io/dsbook/}{``Introduction to Data Science: Data Analysis and Prediction Algorithms with R''}};\\
\item
  Kuhn, M.: \textbf{\href{https://topepo.github.io/caret/}{``The caret package''}};\\
\item
  Molnar, C.: \textbf{\href{https://christophm.github.io/interpretable-ml-book/}{``Interpretable Machine Learning: A Guide for Making Black Box Models Explainable''}};\\
\item
  Lovelace, R., Nowosad, J. and Muenchow, J.: \textbf{\href{https://geocompr.robinlovelace.net/}{``Geocomputation with R''}};
\end{itemize}

For an introduction to \textbf{Predictive Soil Mapping} using R refer to \url{https://soilmapper.org}.

Machine Learning in \textbf{python} with resampling can be best implemented via the
\href{https://scikit-learn.org/stable/}{scikit-learn library}, which matches in
functionality what is available via the mlr package in R.

\hypertarget{license}{%
\section*{License}\label{license}}
\addcontentsline{toc}{section}{License}

\href{http://creativecommons.org/licenses/by-sa/4.0/}{}

This work is licensed under a \href{http://creativecommons.org/licenses/by-sa/4.0/}{Creative Commons Attribution-ShareAlike 4.0 International License}.

\hypertarget{acknowledgements}{%
\section*{Acknowledgements}\label{acknowledgements}}
\addcontentsline{toc}{section}{Acknowledgements}

\includegraphics{tex/R_logo.svg.png} This tutorial is based on the \textbf{\href{https://r4ds.had.co.nz/}{``R for Data Science''}}
book by Hadley Wickham and contributors.

\textbf{\href{https://openlandmap.org}{OpenLandMap}} is a collaborative effort and many people
have contributed data, software, fixes and improvements via pull request.

\href{https://opengeohub.org}{OpenGeoHub} is an independent not-for-profit research
foundation promoting Open Source and Open Data solutions. These tools were developed
primarily for the need of the Geo-harmonizer project and to enable generation of
next-generation environmental layers for continental Europe \citep{witjes2021spatiotemporal, Bonannella2022}.
\textbf{\href{https://envirometrix.nl}{EnvirometriX Ltd.}} is the commercial branch of the group
responsible for designing soil sampling designs for the \textbf{\href{https://agricaptureco2.eu/}{AgriCapture}}
and similar soil monitoring projects.

\href{https://opengeohub.org}{}

\textbf{\href{https://opendatascience.eu/}{OpenDataScience.eu}} project is co-financed by the European Union (\textbf{\href{https://ec.europa.eu/inea/en/connecting-europe-facility/cef-telecom/2018-eu-ia-0095}{CEF Telecom project 2018-EU-IA-0095}}).

\hypertarget{introduction-to-spatial-and-spatiotemporal-data}{%
\chapter{Introduction to spatial and spatiotemporal data}\label{introduction-to-spatial-and-spatiotemporal-data}}

You are reading the work-in-progress Spatial and spatiotemporal interpolation using Ensemble Machine Learning. This chapter is currently draft version, a peer-review publication is pending. You can find the polished first edition at \url{https://opengeohub.github.io/spatial-prediction-eml/}.

\hypertarget{spatial-data-and-spatial-interpolation}{%
\section{Spatial data and spatial interpolation}\label{spatial-data-and-spatial-interpolation}}

Spatial and/or geospatial data is any data that is spatially referenced (in the
case of geographical data referenced to Earth surface) i.e.~\(X\) and \(Y\) coordinates
are known. With the implementation of the GPS and Earth Observation technology,
almost everything is becoming spatial data and hence tools such as Geographical
Information Systems (GIS) and spatial analysis tools to process, analyze and
visualize geospatial data are \href{https://towardsdatascience.com/the-impact-of-geospatial-features-on-machine-learning-3a71c99f080a}{becoming essential}.

\textbf{Spatial interpolation} and/or \textbf{Spatial Prediction} is a process of estimating values
of the target variable over the whole area of interest by using some input training
point data, algorithm and values of the covariates at new locations \citep{Mitas1999Wiley}.
Interpolation results in images or maps, which can then be used for decision making or similar.
There is a difference between \emph{interpolation} and \emph{prediction}: \emph{prediction}
can imply both interpolation and extrapolation. We will more commonly use the
term \emph{spatial prediction} in this tutorial, even though the term \emph{spatial interpolation}
has been more widely accepted \citep{Mitas1999Wiley}. In geostatistics, e.g.~in the
case of \textbf{ordinary kriging}, interpolation corresponds to cases where the
location being estimated is surrounded by the sampling locations and is within
the spatial auto-correlation range \citep{Diggle2007Springer}. Prediction outside of the practical range
(i.e.~where prediction error exceeds the global variance) is referred to as
\textbf{spatial extrapolation}. In other words, extrapolation is prediction at locations
where we do not have enough statistical evidence (based on the statistical model)
to make significant predictions.

\hypertarget{spatial-interpolation-using-ensemble-machine-learning}{%
\section{Spatial interpolation using Ensemble Machine Learning}\label{spatial-interpolation-using-ensemble-machine-learning}}

\textbf{Ensemble Machine Learning} (Ensemble ML) is an approach to modeling where, instead of using a
single best learner, we use multiple \textbf{strong learners} and then combine their
predictive capabilities into a single union. This can both lead to higher
accuracy and robustness \citep{seni2010ensemble}, but also helps with deriving
model-free estimate of prediction errors through nonparametric techniques such as
bootstrapping \citep{zhang2012ensemble}. This way we can help decrease some methodological disadvantages of
individual learners as shown in the previous example with synthetic data.

Ensemble ML can be used to fit models and generate predictions using points data
the same way ordinary kriging is used to generate interpolations. Ensemble ML
for predictive mapping in 2D and 3D is discussed in detail in the first chapter
of the tutorial. Spatiotemporal interpolation using EML (2D+T, 3D+T) is at the
order of magnitude more computational \citep{gasch2015spatio} but it follows the same logic.

Ensemble ML based on stacking is implemented in the mlr package \citep{bischl2016mlr}
and can be initiated via the \texttt{makeStackedLearner} function e.g.:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m }\OtherTok{=}\NormalTok{ mlr}\SpecialCharTok{::}\FunctionTok{makeStackedLearner}\NormalTok{(}\AttributeTok{base.learners =}\NormalTok{ lrns, }
              \AttributeTok{super.learner =} \StringTok{"regr.ml"}\NormalTok{, }\AttributeTok{method =} \StringTok{"stack.cv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

here the base learner predictions will be computed by 5-fold cross-validation
(repeated re-fitting) and then used to determine the meta-learner. This algorithm
is known as the \emph{\href{https://machinelearningmastery.com/super-learner-ensemble-in-python/}{``Super Learner''}} algorithm \citep{Polley2010}.

In the case of spatial prediction, we also want to \emph{block} training points based on
spatial proximity to prevent from producing bias predictions. For this we should
know the range of spatial dependence for the model residuals or similar i.e.~
something that can be derived by fitting a variogram, then limit the minimum
spatial distance between training and validation points to avoid overfitting or
similar (for more info refer to the \href{https://opengeohub.github.io/spatial-sampling-ml/}{Spatial Sampling tutorial}).

To automate fitting of an Ensemble Machine Learning models for the purpose of
spatial interpolation / prediction, one can now use the \href{https://github.com/Envirometrix/landmap}{landmap} package that combines:

\begin{itemize}
\tightlist
\item
  derivation of geographical distances,\\
\item
  conversion of grids to principal components,\\
\item
  automated filling of gaps in gridded data,\\
\item
  automated fitting of variogram and determination of spatial auto-correlation structure,\\
\item
  spatial overlay,\\
\item
  model training using spatial Cross-Validation \citep{lovelace2019geocomputation},\\
\item
  model stacking i.e.~fitting of the final EML,
\end{itemize}

The concept of automating spatial interpolation until the level that almost no
human interaction is required is referred to as \textbf{``automated mapping''} or automated
spatial interpolation \citep{pebesma2011intamap}.

\hypertarget{spatiotemporal-data}{%
\section{Spatiotemporal data}\label{spatiotemporal-data}}

Spatiotemporal data is practically any data that is referenced in both space and
time. This implies that the following \emph{coordinates} are known:

\begin{itemize}
\tightlist
\item
  geographic location (longitude and latitude or projected \(X,Y\) coordinates);\\
\item
  spatial location accuracy or size of the block / volume in the case of bulking of samples;\\
\item
  height above the ground surface (elevation);\\
\item
  start and end time of measurement (year, month, day, hour, minute etc.);
\end{itemize}

Consider for example daily temperature measured at some meteorological station.
This would have the following coordinates:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{temp }\OtherTok{=} \DecValTok{22}
\NormalTok{lat }\OtherTok{=} \FloatTok{44.56123}
\NormalTok{lon }\OtherTok{=} \FloatTok{19.27734}
\NormalTok{delta.xy }\OtherTok{=} \DecValTok{30}
\NormalTok{begin.time }\OtherTok{=} \StringTok{"2013{-}09{-}02 00:00:00 CEST"}
\NormalTok{end.time }\OtherTok{=} \StringTok{"2013{-}09{-}03 00:00:00 CEST"}
\end{Highlighting}
\end{Shaded}

which means that the measurement is fully spatiotemporally referenced with
both \(X,Y\) location defined, \texttt{delta.xy} location accuracy known, and begin and
end time of measurement specified (in this case temporal support is 1 day).

Analysis of spatiotemporal data is somewhat different from pure spatial
analysis. Time is of course NOT \emph{just another} spatial dimension i.e.~it has
specific properties and different statistical assumptions and methods apply to
spatiotemporal data. For an introduction to spatiotemporal data in R please
refer to the \textbf{spacetime} package tutorial \citep{pebesma2012spacetime}.

Conceptually speaking, spatiotemporal datasets and corresponding
databases can be matched with the two major groups of features \citep{erwig1999spatio}: (1)
\textbf{moving or dynamic objects} (discrete or vector geometries), and (2)
dynamic \textbf{regions} (fields or continuous features). Distinct objects (entities)
such as people, animals, vehicles and similar are best represented using
vectors and \textbf{trajectories} (movement through time), and fields are commonly\\
represented using \textbf{gridded structures}. In the case of working with
fields, we basically map either:

\begin{itemize}
\tightlist
\item
  dynamic changes in quantity or density of some material or chemical element,\\
\item
  energy flux or any similar physical measurements,\\
\item
  dynamic changes in probability of occurrence of some feature or object,
\end{itemize}

Spatiotemporal data can be best visualized 2D+T plots \textbf{space-time cubes}. One
example of a spacetime cube is the following plot in Fig. \ref{fig:space-time-cube} \citep{hengl2012spatio, hengl2015plotkml}.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{./img/Fig_space_time_cube} 

}

\caption{Space-time cube visualized in R: (a) cloud plot showing location of meteorological stations in Croatia, (b) illustration of spatial and temporal support in the space-time cube.}\label{fig:space-time-cube}
\end{figure}

The plot above shows distribution of meteorological stations over Croatia, and
then repeated measurements through time. This dataset is further used in the
use-case examples to produce spatiotemporal predictions of daily temperatures.

\hypertarget{time-series-analysis}{%
\section{Time-series analysis}\label{time-series-analysis}}

Field of statistics dealing with modeling changes of variables through time,
including predicting values beyond the training data (forecasting) is
\textbf{time-series analysis}. Some systematic guides on how to run time-series
analysis in R can be found \href{http://r-statistics.co/Time-Series-Analysis-With-R.html}{here}.

How a variable varies through time (time-series curves) can often be drastically different
from how it changes in space (spatial patterns). In general, one can say that, for many
environmental variables, variation of values through time can be separated into
\textbf{components} such as:

\begin{itemize}
\tightlist
\item
  Long-term component (\textbf{trend}) determined by long-term geological and
  extraterrestrial processes,
\item
  Seasonal monthly and/or daily component (\textbf{seasonality}) determined by Earth rotation and
  incoming sun radiation,
\item
  \textbf{Variation} component which can be due to chaotic behavior and/or
  local factors (hence \emph{autocorrelated}), and
\item
  \textbf{Pure noise} i.e.~measurement errors and similar,
\end{itemize}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{./img/Fig_decomposition_timeseries} 

}

\caption{Illustration of decomposition of time-series into: (1) trend, (2) seasonality, and (3) random.}\label{fig:decomp-time}
\end{figure}

Consider for example the case of the land surface temperature. The long-term
component is determined by variations in Earth's orbit and/or Sun's
energy output resulting in gradual drops and rises of global mean
temperature (\href{https://en.wikipedia.org/wiki/Ice_age}{glacials and interglacials}).
Fig. \ref{fig:global-temp} shows example of a global temperature reconstruction from proxy data of \citet{Marcott1198}.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{./img/Fig_global_mean_temp_longterm_trend} 

}

\caption{Global temperature reconstruction. This shows how global temperature varies on a long-term term scale. Graph by: Klaus Bitterman.}\label{fig:global-temp}
\end{figure}

Seasonal i.e.~monthly and daily components of variation of land surface temperature
are also quite systematic. They are basically determined by Earth's rotation
and angles of Sun in relation to Earth's surface. This is a relatively stable
pattern that looks like sinusoidal curves or similar. The plot below shows variation
of values of soil moisture and soil temperature at one meteo station in USA
across multiple years \citep{gasch2015spatio}.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{./img/Fig_cookfarm_plots_seasonality} 

}

\caption{Sensor values from five depths (0.3, 0.6, 0.9, 1.2, and 1.5 m) at one station at Cook Agronomy Farm from January 2011–January 2014. The black line indicates locally fitted splines.}\label{fig:cookfarm-plot}
\end{figure}

The data set in Fig. \ref{fig:cookfarm-plot} is further discussed in the case
studies to demonstrate 3D+T spatiotemporal modeling \citep{gasch2015spatio}.
As we will see later, the seasonal daily and monthly part of variation
is systematic and can be modeling using latitude, altitude and time/day of the year.

\hypertarget{visualizing-spatiotemporal-data}{%
\section{Visualizing spatiotemporal data}\label{visualizing-spatiotemporal-data}}

Spatial data is usually visualized using static or interactive maps (see e.g.~\href{https://r-spatial.github.io/mapview/}{mapview} and/or \href{https://cran.r-project.org/web/packages/tmap/vignettes/tmap-getstarted.html}{tmap package}).
Spatiotemporal data (2D+T) is more complex to visualize than 2D data, while 3D+T data
can even require special software \citep{hengl2015plotkml} before
users can make any seamless interpretation.

There are three possible groups of ways to visualize spatiotemporal
data:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Using \textbf{static images} showing trend parameters together with
  time-series plots at selected representative point locations.\\
\item
  Using \textbf{time-slices} or series of visualizations of the same
  spatial domain but changing in time (time-lapses).\\
\item
  Using \textbf{animations} or \textbf{interactive plots with time-sliders}
  allowing users to choose \emph{speed} and \emph{direction} of animation.
\end{enumerate}

For an introduction to visualizing spatiotemporal and time-series data
refer to \citet{lamigueiro2014displaying}. More complex visualization of
spatiotemporal / dynamic geographic features is possible by using the
\url{https://geemap.org/} package (a Python package for interactive mapping with Google Earth Engine, ipyleaflet, and ipywidgets).

OpenLandMap.org also has multiple temporal datasets and users can interactive with
the time-dimension by using time-slider implemented in \href{http://osgl.grf.bg.ac.rs/books/gvvk-en/}{OpenLayers and Geoserver} \citep{KilibardaProtic2019}.

\url{https://miro.medium.com/max/700/0*fqWNU4XNbjHE3Uv_}

\hypertarget{spatiotemporal-interpolation}{%
\section{Spatiotemporal interpolation}\label{spatiotemporal-interpolation}}

Spatiotemporal interpolation and/or prediction implies that point
samples are used to interpolate within the spacetime cube. This
obviously assumes that enough point measurements are available, and which are spread in
both space and time. We will show in this tutorial how Machine Learning can
be used to interpolate values within the spacetime cube using real case-studies.
Spatiotemporal interpolation using various kriging methods is implemented in
the \href{https://cran.r-project.org/web/packages/gstat/vignettes/spatio-temporal-kriging.pdf}{gstat package} \citep{Bivand2013Springer}, but is not addressed in this tutorial.

For success of spatiotemporal interpolation (in terms of prediction accuracy),
the key is to recognize systematic component of variation in spacetime, which
is usually possible if we can find relationship between the target variable and
some EO data that is available as a time-series and covers the same spacetime cube
of interest. Once we establish a significant relation between \textbf{dynamic target} and
\textbf{dynamic covariates}, we can use the fitted model to predict anywhere in spacetime cube.

For more in-depth discussion on spatiotemporal data in R please refer to
\citet{wikle2019spatio}. For in-depth discussion on spatial and spatiotemporal
blocking for purpose of modeling building and cross-validation refer to
\citet{roberts2017cross}.

\hypertarget{modeling-seasonal-components}{%
\section{Modeling seasonal components}\label{modeling-seasonal-components}}

Seasonality is the characteristic of the target variable to follow cyclical
patterns such as in trigonometric functions. Such repeating patterns can happen
at different \textbf{time-scales}:

\begin{itemize}
\tightlist
\item
  inter-annually,\\
\item
  monthly or based on a season (spring, summer, autumn, winter),\\
\item
  daily,\\
\item
  hourly i.e.~day-time and night-time patterns,
\end{itemize}

The monthly and daily seasonal component of variation is determined by Earth's rotation
and Sun's angle. \citet{kilibarda2014spatio} have shown that the seasonal
component e.g.~geometric Earth surface minimum and maximum daily temperature
can be modeled, universally anywhere on globe, by using the following formula:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{temp.from.geom }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(fi, day, }\AttributeTok{a=}\FloatTok{30.419375}\NormalTok{, }
                           \AttributeTok{b=}\SpecialCharTok{{-}}\FloatTok{15.539232}\NormalTok{, }\AttributeTok{elev=}\DecValTok{0}\NormalTok{, }\AttributeTok{t.grad=}\FloatTok{0.6}\NormalTok{) \{}
\NormalTok{  f }\OtherTok{=} \FunctionTok{ifelse}\NormalTok{(fi}\SpecialCharTok{==}\DecValTok{0}\NormalTok{, }\FloatTok{1e{-}10}\NormalTok{, fi)}
\NormalTok{  costeta }\OtherTok{=} \FunctionTok{cos}\NormalTok{( (day}\DecValTok{{-}18}\NormalTok{ )}\SpecialCharTok{*}\NormalTok{pi}\SpecialCharTok{/}\FloatTok{182.5} \SpecialCharTok{+} \DecValTok{2}\SpecialCharTok{\^{}}\NormalTok{(}\DecValTok{1}\SpecialCharTok{{-}}\FunctionTok{sign}\NormalTok{(fi) ) }\SpecialCharTok{*}\NormalTok{pi) }
\NormalTok{  cosfi }\OtherTok{=} \FunctionTok{cos}\NormalTok{(fi}\SpecialCharTok{*}\NormalTok{pi}\SpecialCharTok{/}\DecValTok{180}\NormalTok{ )}
\NormalTok{  A }\OtherTok{=}\NormalTok{ cosfi}
\NormalTok{  B }\OtherTok{=}\NormalTok{ (}\DecValTok{1}\SpecialCharTok{{-}}\NormalTok{costeta ) }\SpecialCharTok{*} \FunctionTok{abs}\NormalTok{(}\FunctionTok{sin}\NormalTok{(fi}\SpecialCharTok{*}\NormalTok{pi}\SpecialCharTok{/}\DecValTok{180}\NormalTok{ ) )}
\NormalTok{  x }\OtherTok{=}\NormalTok{ a}\SpecialCharTok{*}\NormalTok{A }\SpecialCharTok{+}\NormalTok{ b}\SpecialCharTok{*}\NormalTok{B }\SpecialCharTok{{-}}\NormalTok{ t.grad }\SpecialCharTok{*}\NormalTok{ elev }\SpecialCharTok{/} \DecValTok{100}
  \FunctionTok{return}\NormalTok{(x)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

where \texttt{day} is the day of year, \texttt{fi} is the latitude, the number 18 represents
the coldest day in the northern and warmest day in the southern hemisphere,
\texttt{elev} is the elevation in meter, 0.6 is the vertical temperature gradient per
100-m, and \texttt{sign} denotes the \emph{signum} function that extracts the sign of a real number.

This formula accounts for different seasons at southern and northern
hemisphere and can be basically applied on gridded surfaces to compute expected
temperature at a given day. A simple example of min daily temperature is:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{temp.from.geom}\NormalTok{(}\AttributeTok{fi=}\DecValTok{52}\NormalTok{, }\AttributeTok{day=}\DecValTok{120}\NormalTok{)}
\CommentTok{\#\textgreater{} [1] 8.73603}
\end{Highlighting}
\end{Shaded}

If we plot this function for five consecutive years, we get something similar to the
spline-fitted functions in the previous plot:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{days }\OtherTok{=} \FunctionTok{seq}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{(}\DecValTok{5}\SpecialCharTok{*}\DecValTok{365}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(}\FunctionTok{temp.from.geom}\NormalTok{(}\AttributeTok{fi=}\DecValTok{52}\NormalTok{, }\AttributeTok{day=}\NormalTok{days))}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{spatial-data_files/figure-latex/seasonal-plot-1} 

}

\caption{Geometric temperature function plot for a given latitude.}\label{fig:seasonal-plot}
\end{figure}

\hypertarget{predictive-mapping-using-spatial-and-spatiotemporal-ml-in-r}{%
\section{Predictive mapping using spatial and spatiotemporal ML in R}\label{predictive-mapping-using-spatial-and-spatiotemporal-ml-in-r}}

Standard spatiotemporal ML for predictive mapping typically includes the
following steps \citep{hengl2018random, hengl2019predictive}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Prepare training (points) data and data cube with all covariates
  ideally as an analysis-ready datacube.
\item
  Overlay points and create a regression-matrix.\\
\item
  Fine-tune initial model, reduce complexity
  and produce production-ready prediction model.\\
\item
  Run mapping accuracy assessment and determine prediction uncertainty
  including the per pixel uncertainty.\\
\item
  Generate predictions and save as maps.
\item
  Visualize predictions using web-GIS solutions.
\end{enumerate}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{./img/Fig_general_scheme_PEM} 

}

\caption{General Machine Learning framework recommended for predictive mapping of vegetation / ecological / soil variables. Assuming full automation of modeling, [2nd-round samples](https://opengeohub.github.io/spatial-sampling-ml/) can be used to gradually improve mapping accuracy.}\label{fig:pem-scheme}
\end{figure}

\hypertarget{extrapolation-and-over-fitting-problems-of-ml-methods}{%
\section{Extrapolation and over-fitting problems of ML methods}\label{extrapolation-and-over-fitting-problems-of-ml-methods}}

Machine Learning has \emph{defacto} become next-generation applied predictive modeling
framework. ML techniques such as \textbf{Random Forest} (RF) have proven to
over-perform vs more simple linear statistical methods, especially where
the datasets are large, complex and target variable follows complex
relationship with covariates \citep{hengl2018random}. Random Forest comes at a cost
however. There are four main practical disadvantages of RF:

\begin{itemize}
\tightlist
\item
  Depending on data and assumptions about data, it can over-fit values
  without an analyst even noticing it.\\
\item
  It predicts well only within the feature space with enough training
  data. \textbf{Extrapolation} i.e.~prediction outside the training space can
  lead to poor performance \citep{meyerPebesma2020}.\\
\item
  It can be computationally expensive with computational load increasing
  exponentially with the number of covariates.\\
\item
  It requires quality training data and is highly sensitive to blunders and
  typos in the data.
\end{itemize}

Read more about extrapolation problems of Random Forest in \href{https://medium.com/nerd-for-tech/extrapolation-is-tough-for-trees-tree-based-learners-combining-learners-of-different-type-makes-659187a6f58d}{this post}.

In the following section we will demonstrate that indeed RF can overfit
data and can have serious problems with predicting in the extrapolation
space. Consider for example the following small synthetic dataset assuming
simple linear relationship (see \href{https://twitter.com/DylanBeaudette/status/1410666900581851138}{original post by Dylan Beaudette}):

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{200}\NormalTok{)}
\NormalTok{n }\OtherTok{=} \DecValTok{100}
\NormalTok{x }\OtherTok{\textless{}{-}} \DecValTok{1}\SpecialCharTok{:}\NormalTok{n}
\NormalTok{y }\OtherTok{\textless{}{-}}\NormalTok{ x }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(}\AttributeTok{n =} \DecValTok{50}\NormalTok{, }\AttributeTok{mean =} \DecValTok{15}\NormalTok{, }\AttributeTok{sd =} \DecValTok{15}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

If we fit a simple \textbf{Ordinary Least Square} model to this data we get:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m0 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x)}
\FunctionTok{summary}\NormalTok{(m0)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} lm(formula = y \textasciitilde{} x)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residuals:}
\CommentTok{\#\textgreater{}      Min       1Q   Median       3Q      Max }
\CommentTok{\#\textgreater{} {-}27.6093  {-}6.4396   0.6437   6.7742  26.7192 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}             Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept) 14.84655    2.37953   6.239 1.12e{-}08 ***}
\CommentTok{\#\textgreater{} x            0.97910    0.04091  23.934  \textless{} 2e{-}16 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residual standard error: 11.81 on 98 degrees of freedom}
\CommentTok{\#\textgreater{} Multiple R{-}squared:  0.8539, Adjusted R{-}squared:  0.8524 }
\CommentTok{\#\textgreater{} F{-}statistic: 572.9 on 1 and 98 DF,  p{-}value: \textless{} 2.2e{-}16}
\end{Highlighting}
\end{Shaded}

we see that the model explains about 85\% of variation in the data and that the
RMSE estimated by the model (residual standard error) matches very well the
noise component we have inserted on purpose using the \texttt{rnorm} function.

If we fit a Random Forest model to this data we get:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(randomForest)}
\CommentTok{\#\textgreater{} randomForest 4.6{-}14}
\CommentTok{\#\textgreater{} Type rfNews() to see new features/changes/bug fixes.}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Attaching package: \textquotesingle{}randomForest\textquotesingle{}}
\CommentTok{\#\textgreater{} The following object is masked from \textquotesingle{}package:ranger\textquotesingle{}:}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}     importance}
\NormalTok{rf }\OtherTok{=}\NormalTok{ randomForest}\SpecialCharTok{::}\FunctionTok{randomForest}\NormalTok{(}\FunctionTok{data.frame}\NormalTok{(}\AttributeTok{x=}\NormalTok{x), y, }\AttributeTok{nodesize =} \DecValTok{5}\NormalTok{, }\AttributeTok{keep.inbag =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{rf}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{}  randomForest(x = data.frame(x = x), y = y, nodesize = 5, keep.inbag = TRUE) }
\CommentTok{\#\textgreater{}                Type of random forest: regression}
\CommentTok{\#\textgreater{}                      Number of trees: 500}
\CommentTok{\#\textgreater{} No. of variables tried at each split: 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}           Mean of squared residuals: 202.6445}
\CommentTok{\#\textgreater{}                     \% Var explained: 78.34}
\end{Highlighting}
\end{Shaded}

Next, we can estimate the prediction errors using the method of \citet{lu2021unified},
which is available via the \texttt{forestError} package:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(forestError)}
\NormalTok{rmse }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(a, b) \{  }\FunctionTok{sqrt}\NormalTok{(}\FunctionTok{mean}\NormalTok{((a }\SpecialCharTok{{-}}\NormalTok{ b)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)) \}}
\NormalTok{dat }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(x,y)}
\NormalTok{newdata }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{x =} \SpecialCharTok{{-}}\DecValTok{100}\SpecialCharTok{:}\DecValTok{200}
\NormalTok{)}
\NormalTok{newdata}\SpecialCharTok{$}\NormalTok{y.lm }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(m0, }\AttributeTok{newdata =}\NormalTok{ newdata)}
\DocumentationTok{\#\# prediction error from forestError:}
\NormalTok{quantiles }\OtherTok{=} \FunctionTok{c}\NormalTok{((}\DecValTok{1}\FloatTok{{-}.682}\NormalTok{)}\SpecialCharTok{/}\DecValTok{2}\NormalTok{, }\DecValTok{1}\SpecialCharTok{{-}}\NormalTok{(}\DecValTok{1}\FloatTok{{-}.682}\NormalTok{)}\SpecialCharTok{/}\DecValTok{2}\NormalTok{)}
\NormalTok{pr.rf }\OtherTok{=}\NormalTok{ forestError}\SpecialCharTok{::}\FunctionTok{quantForestError}\NormalTok{(rf, }\AttributeTok{X.train=}\FunctionTok{data.frame}\NormalTok{(}\AttributeTok{x=}\NormalTok{x), }
                                      \AttributeTok{X.test=}\FunctionTok{data.frame}\NormalTok{(}\AttributeTok{x =} \SpecialCharTok{{-}}\DecValTok{100}\SpecialCharTok{:}\DecValTok{200}\NormalTok{), }
                                      \AttributeTok{Y.train=}\NormalTok{y, }\AttributeTok{alpha =}\NormalTok{ (}\DecValTok{1}\SpecialCharTok{{-}}\NormalTok{(quantiles[}\DecValTok{2}\NormalTok{]}\SpecialCharTok{{-}}\NormalTok{quantiles[}\DecValTok{1}\NormalTok{])))}
\NormalTok{newdata}\SpecialCharTok{$}\NormalTok{y.rf }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(rf, }\AttributeTok{newdata =}\NormalTok{ newdata)}
\NormalTok{rmse.lm }\OtherTok{\textless{}{-}} \FunctionTok{round}\NormalTok{(}\FunctionTok{rmse}\NormalTok{(y, }\FunctionTok{predict}\NormalTok{(m0)), }\DecValTok{1}\NormalTok{)}
\NormalTok{rmse.rf }\OtherTok{\textless{}{-}} \FunctionTok{round}\NormalTok{(}\FunctionTok{rmse}\NormalTok{(y, }\FunctionTok{predict}\NormalTok{(rf)), }\DecValTok{1}\NormalTok{)}
\NormalTok{rmse.lm; rmse.rf}
\CommentTok{\#\textgreater{} [1] 11.7}
\CommentTok{\#\textgreater{} [1] 14.2}
\end{Highlighting}
\end{Shaded}

This shows that RF estimates higher RMSE than linear model. However, if
we visualize the two models against each other we see that indeed RF
algorithm seems to over-fit this specific data:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{leg.txt }\OtherTok{\textless{}{-}} \FunctionTok{sprintf}\NormalTok{(}\StringTok{"\%s (\%s)"}\NormalTok{, }\FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}lm\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}RF\textquotesingle{}}\NormalTok{), }\FunctionTok{c}\NormalTok{(rmse.lm, rmse.rf))}
\FunctionTok{par}\NormalTok{(}\AttributeTok{mar =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{), }\AttributeTok{fg =} \StringTok{\textquotesingle{}black\textquotesingle{}}\NormalTok{, }\AttributeTok{bg =} \StringTok{\textquotesingle{}white\textquotesingle{}}\NormalTok{)}
\FunctionTok{plot}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x, }\AttributeTok{xlim =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{25}\NormalTok{, }\DecValTok{125}\NormalTok{), }\AttributeTok{ylim =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{50}\NormalTok{, }\DecValTok{150}\NormalTok{), }\AttributeTok{type =} \StringTok{\textquotesingle{}n\textquotesingle{}}\NormalTok{, }\AttributeTok{axes =} \ConstantTok{FALSE}\NormalTok{)}
\FunctionTok{grid}\NormalTok{()}
\FunctionTok{points}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x, }\AttributeTok{cex =} \DecValTok{1}\NormalTok{, }\AttributeTok{pch =} \DecValTok{16}\NormalTok{, }\AttributeTok{las =} \DecValTok{1}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(y.lm }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x, }\AttributeTok{data =}\NormalTok{ newdata, }\AttributeTok{col =} \DecValTok{2}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(y.rf }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x, }\AttributeTok{data =}\NormalTok{ newdata, }\AttributeTok{col =} \DecValTok{4}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(newdata}\SpecialCharTok{$}\NormalTok{x, pr.rf}\SpecialCharTok{$}\NormalTok{estimates}\SpecialCharTok{$}\NormalTok{lower\_0}\FloatTok{.318}\NormalTok{, }\AttributeTok{lty=}\DecValTok{2}\NormalTok{,}\AttributeTok{col=}\DecValTok{4}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(newdata}\SpecialCharTok{$}\NormalTok{x, pr.rf}\SpecialCharTok{$}\NormalTok{estimates}\SpecialCharTok{$}\NormalTok{upper\_0}\FloatTok{.318}\NormalTok{, }\AttributeTok{lty=}\DecValTok{2}\NormalTok{,}\AttributeTok{col=}\DecValTok{4}\NormalTok{)}
\FunctionTok{legend}\NormalTok{(}\StringTok{\textquotesingle{}bottom\textquotesingle{}}\NormalTok{, }\AttributeTok{legend =}\NormalTok{ leg.txt, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{, }\AttributeTok{lty =} \DecValTok{1}\NormalTok{, }\AttributeTok{col =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{3}\NormalTok{), }\AttributeTok{horiz =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{title =} \StringTok{\textquotesingle{}RMSE\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{spatial-data_files/figure-latex/synthetic-lm-1} 

}

\caption{Difference in model fits for sythetic data: lm vs RF. In this case we know that RF (blue line) is overfitting and under-estimating the prediction error in the extrapolation space. Dotted line shows respective 1 std. prediction interval for RF.}\label{fig:synthetic-lm}
\end{figure}

RF basically tries to fit relationship even to the \textbf{pure noise} component of variation
(we know it is pure noise because we have generated it using the \texttt{rnorm} function).
This is obvious over-fitting as we do not want to model something which is purely random.

Extrapolation would not maybe be so much of a problem in the example above if
the prediction intervals from the \texttt{forestError} package expressed more
realistically that the predictions deviate from the \emph{linear structure} in the
data. Assuming that, after the prediction, one would eventually collect
ground-truth data for the RF model above, these would probably show that the
prediction error / prediction intervals are completely off. Most traditional
statisticians would consider these too-narrow and over-optimistic and the
fitted line over-fit, and hence any further down the pipeline over-optimistic
prediction uncertainty can result in decision makers being over-confident,
leading to wrong decisions, and consequently making users losing any confidence in RF.
For an in-depth discussion on extrapolation problems and \textbf{Area of Applicability}
of Machine Learning models please refer to \citet{meyerPebesma2020}.

A possible solution to the problem above is to, instead of using only one
learners, we use multiple learners and then apply robust cross-validation that
prevents the target model from over-fitting. This can be implemented efficiently,
for example, by using the \texttt{mlr} package \citep{bischl2016mlr}. We can run an Ensemble
Model by applying the following four steps. First, we define the task of interest
and a combination of learners i.e.~so-called \textbf{base learners}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(mlr)}
\FunctionTok{library}\NormalTok{(kernlab)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Attaching package: \textquotesingle{}kernlab\textquotesingle{}}
\CommentTok{\#\textgreater{} The following objects are masked from \textquotesingle{}package:raster\textquotesingle{}:}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}     buffer, rotated}
\FunctionTok{library}\NormalTok{(mboost)}
\CommentTok{\#\textgreater{} Loading required package: parallel}
\CommentTok{\#\textgreater{} Loading required package: stabs}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Attaching package: \textquotesingle{}stabs\textquotesingle{}}
\CommentTok{\#\textgreater{} The following object is masked from \textquotesingle{}package:mlr\textquotesingle{}:}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}     subsample}
\CommentTok{\#\textgreater{} This is mboost 2.9{-}2. See \textquotesingle{}package?mboost\textquotesingle{} and \textquotesingle{}news(package  = "mboost")\textquotesingle{}}
\CommentTok{\#\textgreater{} for a complete list of changes.}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Attaching package: \textquotesingle{}mboost\textquotesingle{}}
\CommentTok{\#\textgreater{} The following object is masked from \textquotesingle{}package:glmnet\textquotesingle{}:}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}     Cindex}
\CommentTok{\#\textgreater{} The following objects are masked from \textquotesingle{}package:raster\textquotesingle{}:}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}     cv, extract}
\FunctionTok{library}\NormalTok{(landmap)}
\NormalTok{SL.library }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\StringTok{"regr.ranger"}\NormalTok{, }\StringTok{"regr.glm"}\NormalTok{, }\StringTok{"regr.gamboost"}\NormalTok{, }\StringTok{"regr.ksvm"}\NormalTok{)}
\NormalTok{lrns }\OtherTok{\textless{}{-}} \FunctionTok{lapply}\NormalTok{(SL.library, mlr}\SpecialCharTok{::}\NormalTok{makeLearner)}
\NormalTok{tsk }\OtherTok{\textless{}{-}}\NormalTok{ mlr}\SpecialCharTok{::}\FunctionTok{makeRegrTask}\NormalTok{(}\AttributeTok{data =}\NormalTok{ dat, }\AttributeTok{target =} \StringTok{"y"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

In this case we use basically four very different models: RF (\texttt{ranger}),
linear model (\texttt{glm}), Gradient boosting (\texttt{gamboost}) and Support Vector
Machine (\texttt{kvsm}). Second, we train the Ensemble model by using the stacking approach:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{init.m }\OtherTok{\textless{}{-}}\NormalTok{ mlr}\SpecialCharTok{::}\FunctionTok{makeStackedLearner}\NormalTok{(lrns, }\AttributeTok{method =} \StringTok{"stack.cv"}\NormalTok{, }\AttributeTok{super.learner =} \StringTok{"regr.lm"}\NormalTok{, }\AttributeTok{resampling=}\NormalTok{mlr}\SpecialCharTok{::}\FunctionTok{makeResampleDesc}\NormalTok{(}\AttributeTok{method =} \StringTok{"CV"}\NormalTok{))}
\NormalTok{eml }\OtherTok{=} \FunctionTok{train}\NormalTok{(init.m, tsk)}
\FunctionTok{summary}\NormalTok{(eml}\SpecialCharTok{$}\NormalTok{learner.model}\SpecialCharTok{$}\NormalTok{super.model}\SpecialCharTok{$}\NormalTok{learner.model)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} stats::lm(formula = f, data = d)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residuals:}
\CommentTok{\#\textgreater{}      Min       1Q   Median       3Q      Max }
\CommentTok{\#\textgreater{} {-}28.0474  {-}7.0473  {-}0.4075   7.2528  28.6083 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}               Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept)   {-}0.86179    3.01456  {-}0.286  0.77560    }
\CommentTok{\#\textgreater{} regr.ranger   {-}0.27294    0.24292  {-}1.124  0.26402    }
\CommentTok{\#\textgreater{} regr.glm       4.75714    1.09051   4.362 3.27e{-}05 ***}
\CommentTok{\#\textgreater{} regr.gamboost {-}3.55134    1.14764  {-}3.094  0.00259 ** }
\CommentTok{\#\textgreater{} regr.ksvm      0.08578    0.28482   0.301  0.76394    }
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residual standard error: 11.25 on 95 degrees of freedom}
\CommentTok{\#\textgreater{} Multiple R{-}squared:  0.8714, Adjusted R{-}squared:  0.8659 }
\CommentTok{\#\textgreater{} F{-}statistic: 160.9 on 4 and 95 DF,  p{-}value: \textless{} 2.2e{-}16}
\end{Highlighting}
\end{Shaded}

The results show that \texttt{ranger} and \texttt{ksvm} basically under-perform and are in
fact not significant for this specific data i.e.~could be probably omitted from
modeling.

Note that, for \textbf{stacking} of multiple learners we use a separate model
(a meta-learner) which is in this case a simple linear model. We use a
simple model because we assume that the non-linear relationships have
already been modeled via complex models such as \texttt{ranger}, \texttt{gamboost}
and/or \texttt{ksvm}.

Next, we need to estimate mapping accuracy and \textbf{prediction errors} for
Ensemble predictions. This is not trivial as there are no simple derived formulas.
We need to use a non-parametric approach basically and this can be very computational.
A computationally interesting approach is to first estimate the (global) mapping
accuracy, then adjust the prediction variance from multiple base learners:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{newdata}\SpecialCharTok{$}\NormalTok{y.eml }\OtherTok{=} \FunctionTok{predict}\NormalTok{(eml, }\AttributeTok{newdata =}\NormalTok{ newdata)}\SpecialCharTok{$}\NormalTok{data}\SpecialCharTok{$}\NormalTok{response}
\NormalTok{m.train }\OtherTok{=}\NormalTok{ eml}\SpecialCharTok{$}\NormalTok{learner.model}\SpecialCharTok{$}\NormalTok{super.model}\SpecialCharTok{$}\NormalTok{learner.model}\SpecialCharTok{$}\NormalTok{model}
\NormalTok{m.terms }\OtherTok{=}\NormalTok{ eml}\SpecialCharTok{$}\NormalTok{learner.model}\SpecialCharTok{$}\NormalTok{super.model}\SpecialCharTok{$}\NormalTok{learner.model}\SpecialCharTok{$}\NormalTok{terms}
\NormalTok{eml.MSE0 }\OtherTok{=}\NormalTok{ matrixStats}\SpecialCharTok{::}\FunctionTok{rowSds}\NormalTok{(}\FunctionTok{as.matrix}\NormalTok{(m.train[,}\FunctionTok{all.vars}\NormalTok{(m.terms)[}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{]]), }\AttributeTok{na.rm=}\ConstantTok{TRUE}\NormalTok{)}\SpecialCharTok{\^{}}\DecValTok{2}
\NormalTok{eml.MSE }\OtherTok{=} \FunctionTok{deviance}\NormalTok{(eml}\SpecialCharTok{$}\NormalTok{learner.model}\SpecialCharTok{$}\NormalTok{super.model}\SpecialCharTok{$}\NormalTok{learner.model)}\SpecialCharTok{/}\FunctionTok{df.residual}\NormalTok{(eml}\SpecialCharTok{$}\NormalTok{learner.model}\SpecialCharTok{$}\NormalTok{super.model}\SpecialCharTok{$}\NormalTok{learner.model)}
\DocumentationTok{\#\# correction factor / mass{-}preservation of MSE}
\NormalTok{eml.cf }\OtherTok{=}\NormalTok{ eml.MSE}\SpecialCharTok{/}\FunctionTok{mean}\NormalTok{(eml.MSE0, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{eml.cf}
\CommentTok{\#\textgreater{} [1] 9.328646}
\end{Highlighting}
\end{Shaded}

This shows that variance of the learners is about 10 times smaller than
the actual CV variance. Again, this proves that many learners try to fit
data very closely so that variance of different base learners is often
smoothed out.

Next, we can predict values and prediction errors at all new locations:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pred }\OtherTok{=}\NormalTok{ mlr}\SpecialCharTok{::}\FunctionTok{getStackedBaseLearnerPredictions}\NormalTok{(eml, }\AttributeTok{newdata=}\FunctionTok{data.frame}\NormalTok{(}\AttributeTok{x =} \SpecialCharTok{{-}}\DecValTok{100}\SpecialCharTok{:}\DecValTok{200}\NormalTok{))}
\NormalTok{rf.sd }\OtherTok{=} \FunctionTok{sqrt}\NormalTok{(matrixStats}\SpecialCharTok{::}\FunctionTok{rowSds}\NormalTok{(}\FunctionTok{as.matrix}\NormalTok{(}\FunctionTok{as.data.frame}\NormalTok{(pred)), }\AttributeTok{na.rm=}\ConstantTok{TRUE}\NormalTok{)}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{*}\NormalTok{ eml.cf)}
\NormalTok{rmse.eml }\OtherTok{\textless{}{-}} \FunctionTok{round}\NormalTok{(}\FunctionTok{sqrt}\NormalTok{(eml.MSE), }\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

and the plot the results of fitting linear model vs EML:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{leg.txt }\OtherTok{\textless{}{-}} \FunctionTok{sprintf}\NormalTok{(}\StringTok{"\%s (\%s)"}\NormalTok{, }\FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}lm\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}EML\textquotesingle{}}\NormalTok{), }\FunctionTok{c}\NormalTok{(rmse.lm, rmse.eml))}
\FunctionTok{par}\NormalTok{(}\AttributeTok{mar =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{), }\AttributeTok{fg =} \StringTok{\textquotesingle{}black\textquotesingle{}}\NormalTok{, }\AttributeTok{bg =} \StringTok{\textquotesingle{}white\textquotesingle{}}\NormalTok{)}
\FunctionTok{plot}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x, }\AttributeTok{xlim =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{25}\NormalTok{, }\DecValTok{125}\NormalTok{), }\AttributeTok{ylim =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{50}\NormalTok{, }\DecValTok{150}\NormalTok{), }\AttributeTok{type =} \StringTok{\textquotesingle{}n\textquotesingle{}}\NormalTok{, }\AttributeTok{axes =} \ConstantTok{FALSE}\NormalTok{)}
\FunctionTok{grid}\NormalTok{()}
\FunctionTok{points}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x, }\AttributeTok{cex =} \DecValTok{1}\NormalTok{, }\AttributeTok{pch =} \DecValTok{16}\NormalTok{, }\AttributeTok{las =} \DecValTok{1}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(y.lm }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x, }\AttributeTok{data =}\NormalTok{ newdata, }\AttributeTok{col =} \DecValTok{2}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(y.eml }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x, }\AttributeTok{data =}\NormalTok{ newdata, }\AttributeTok{col =} \DecValTok{4}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(newdata}\SpecialCharTok{$}\NormalTok{x, newdata}\SpecialCharTok{$}\NormalTok{y.eml}\SpecialCharTok{+}\NormalTok{rmse.eml}\SpecialCharTok{+}\NormalTok{rf.sd, }\AttributeTok{lty=}\DecValTok{2}\NormalTok{, }\AttributeTok{col=}\DecValTok{4}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(newdata}\SpecialCharTok{$}\NormalTok{x, newdata}\SpecialCharTok{$}\NormalTok{y.eml}\SpecialCharTok{{-}}\NormalTok{(rmse.eml}\SpecialCharTok{+}\NormalTok{rf.sd), }\AttributeTok{lty=}\DecValTok{2}\NormalTok{, }\AttributeTok{col=}\DecValTok{4}\NormalTok{)}
\FunctionTok{legend}\NormalTok{(}\StringTok{\textquotesingle{}bottom\textquotesingle{}}\NormalTok{, }\AttributeTok{legend =}\NormalTok{ leg.txt, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{, }\AttributeTok{lty =} \DecValTok{1}\NormalTok{, }\AttributeTok{col =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{3}\NormalTok{), }\AttributeTok{horiz =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{title =} \StringTok{\textquotesingle{}RMSE\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{spatial-data_files/figure-latex/synthetic-eml-1} 

}

\caption{Difference in model fits for sythetic data: lm vs Ensemble ML.}\label{fig:synthetic-eml}
\end{figure}

From the plot above, we see that the prediction error intervals in the
extrapolation space are now wider (compare with Fig. \ref{fig:synthetic-lm}), and this reflects much better
what we would expect than if we have only used the \texttt{forestError} package.

In summary: it appears that combining linear and non-linear tree-based
models in an Ensemble ML framework helps both: decrease over-fitting and produce
more realistic predictions of uncertainty / prediction intervals. The Ensemble ML
framework correctly identifies linear models as being more important than
random forest or similar. Hopefully, this provides enough evidence to convince you
that Ensemble ML is potentially interesting for use as a generic solution for
spatial and spatiotemporal interpolation and extrapolation.

\hypertarget{spatial-interpolation-using-ensemble-ml}{%
\chapter{Spatial interpolation using Ensemble ML}\label{spatial-interpolation-using-ensemble-ml}}

You are reading the work-in-progress Spatial and spatiotemporal interpolation using Ensemble Machine Learning. This chapter is currently draft version, a peer-review publication is pending. You can find the polished first edition at \url{https://opengeohub.github.io/spatial-prediction-eml/}.

\hypertarget{spatial-interpolation-using-ml-and-buffer-distances-to-points}{%
\section{Spatial interpolation using ML and buffer distances to points}\label{spatial-interpolation-using-ml-and-buffer-distances-to-points}}

A relatively simple approach to interpolate values from point data using e.g.~
Random Forest is to use \textbf{buffer distances} to all points as covariates. We can
here use the meuse dataset for testing \citep{hengl2018random}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(rgdal)}
\FunctionTok{library}\NormalTok{(ranger)}
\FunctionTok{library}\NormalTok{(raster)}
\FunctionTok{library}\NormalTok{(plotKML)}
\FunctionTok{demo}\NormalTok{(meuse, }\AttributeTok{echo=}\ConstantTok{FALSE}\NormalTok{)}
\NormalTok{grid.dist0 }\OtherTok{\textless{}{-}}\NormalTok{ landmap}\SpecialCharTok{::}\FunctionTok{buffer.dist}\NormalTok{(meuse[}\StringTok{"zinc"}\NormalTok{], meuse.grid[}\DecValTok{1}\NormalTok{], }
                                   \AttributeTok{classes=}\FunctionTok{as.factor}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\FunctionTok{nrow}\NormalTok{(meuse)))}
\end{Highlighting}
\end{Shaded}

This creates 155 gridded maps i.e.~one map per training point. These maps of
distances can now be used to predict some target variable by running:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dn0 }\OtherTok{\textless{}{-}} \FunctionTok{paste}\NormalTok{(}\FunctionTok{names}\NormalTok{(grid.dist0), }\AttributeTok{collapse=}\StringTok{"+"}\NormalTok{)}
\NormalTok{fm0 }\OtherTok{\textless{}{-}} \FunctionTok{as.formula}\NormalTok{(}\FunctionTok{paste}\NormalTok{(}\StringTok{"zinc \textasciitilde{} "}\NormalTok{, dn0))}
\NormalTok{ov.zinc }\OtherTok{\textless{}{-}} \FunctionTok{over}\NormalTok{(meuse[}\StringTok{"zinc"}\NormalTok{], grid.dist0)}
\NormalTok{rm.zinc }\OtherTok{\textless{}{-}} \FunctionTok{cbind}\NormalTok{(meuse}\SpecialCharTok{@}\NormalTok{data[}\StringTok{"zinc"}\NormalTok{], ov.zinc)}
\NormalTok{m.zinc }\OtherTok{\textless{}{-}} \FunctionTok{ranger}\NormalTok{(fm0, rm.zinc, }\AttributeTok{num.trees=}\DecValTok{150}\NormalTok{, }\AttributeTok{seed=}\DecValTok{1}\NormalTok{)}
\NormalTok{m.zinc}
\CommentTok{\#\textgreater{} Ranger result}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{}  ranger(fm0, rm.zinc, num.trees = 150, seed = 1) }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Type:                             Regression }
\CommentTok{\#\textgreater{} Number of trees:                  150 }
\CommentTok{\#\textgreater{} Sample size:                      155 }
\CommentTok{\#\textgreater{} Number of independent variables:  155 }
\CommentTok{\#\textgreater{} Mtry:                             12 }
\CommentTok{\#\textgreater{} Target node size:                 5 }
\CommentTok{\#\textgreater{} Variable importance mode:         none }
\CommentTok{\#\textgreater{} Splitrule:                        variance }
\CommentTok{\#\textgreater{} OOB prediction error (MSE):       67501.48 }
\CommentTok{\#\textgreater{} R squared (OOB):                  0.4990359}
\end{Highlighting}
\end{Shaded}

Using this model we can generate and plot predictions using:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{op }\OtherTok{\textless{}{-}} \FunctionTok{par}\NormalTok{(}\AttributeTok{oma=}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{), }\AttributeTok{mar=}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{3}\NormalTok{))}
\NormalTok{zinc.rfd }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(m.zinc, grid.dist0}\SpecialCharTok{@}\NormalTok{data)}\SpecialCharTok{$}\NormalTok{predictions}
\NormalTok{meuse.grid}\SpecialCharTok{$}\NormalTok{zinc.rfd }\OtherTok{=}\NormalTok{ zinc.rfd}
\FunctionTok{plot}\NormalTok{(}\FunctionTok{raster}\NormalTok{(meuse.grid[}\StringTok{"zinc.rfd"}\NormalTok{]), }\AttributeTok{col=}\NormalTok{R\_pal[[}\StringTok{"rainbow\_75"}\NormalTok{]][}\DecValTok{4}\SpecialCharTok{:}\DecValTok{20}\NormalTok{],}
         \AttributeTok{main=}\StringTok{"Predictions RF on buffer distances"}\NormalTok{, }\AttributeTok{axes=}\ConstantTok{FALSE}\NormalTok{, }\AttributeTok{box=}\ConstantTok{FALSE}\NormalTok{)}
\FunctionTok{points}\NormalTok{(meuse, }\AttributeTok{pch=}\StringTok{"+"}\NormalTok{, }\AttributeTok{cex=}\NormalTok{.}\DecValTok{8}\NormalTok{)}
\FunctionTok{par}\NormalTok{(op)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{spatial-interpolation_files/figure-latex/map-buff-1} 

}

\caption{Values of Zinc predicted using only RF on buffer distances.}\label{fig:map-buff}
\end{figure}

The resulting predictions produce patterns very much similar to what we would
produce if we have used ordinary kriging or similar. Note however that for RFsp
model: (1) we did not have to fit any variogram, (2) the model is in essence \emph{over-
parameterized} with basically more covariates than training points.

\hypertarget{spatial-interpolation-using-ml-and-geographical-distances-to-neighbors}{%
\section{Spatial interpolation using ML and geographical distances to neighbors}\label{spatial-interpolation-using-ml-and-geographical-distances-to-neighbors}}

Deriving buffer distances for all points is obviously not suitable for very
large point datasets. \citet{sekulic2020random} describe an alternative, a more scalable
method that uses closest neighbors (and their values) as covariates to predict
target variable. This can be implemented using the \texttt{meteo} package:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(meteo)}
\CommentTok{\#\textgreater{} Warning: replacing previous import \textquotesingle{}caret::MAE\textquotesingle{} by \textquotesingle{}DescTools::MAE\textquotesingle{} when loading}
\CommentTok{\#\textgreater{} \textquotesingle{}meteo\textquotesingle{}}
\CommentTok{\#\textgreater{} Warning: replacing previous import \textquotesingle{}caret::RMSE\textquotesingle{} by \textquotesingle{}DescTools::RMSE\textquotesingle{} when}
\CommentTok{\#\textgreater{} loading \textquotesingle{}meteo\textquotesingle{}}
\NormalTok{nearest\_obs }\OtherTok{\textless{}{-}}\NormalTok{ meteo}\SpecialCharTok{::}\FunctionTok{near.obs}\NormalTok{(}\AttributeTok{locations =}\NormalTok{ meuse.grid, }
                               \AttributeTok{locations.x.y =} \FunctionTok{c}\NormalTok{(}\StringTok{"x"}\NormalTok{,}\StringTok{"y"}\NormalTok{), }
                               \AttributeTok{observations =}\NormalTok{ meuse, }\AttributeTok{observations.x.y=}\FunctionTok{c}\NormalTok{(}\StringTok{"x"}\NormalTok{,}\StringTok{"y"}\NormalTok{), }
                               \AttributeTok{zcol =} \StringTok{"zinc"}\NormalTok{, }\AttributeTok{n.obs =} \DecValTok{10}\NormalTok{, }\AttributeTok{rm.dupl =} \ConstantTok{TRUE}\NormalTok{)}
\CommentTok{\#\textgreater{} Warning in if (class(knn1$nn.idx) != "integer") \{: the condition has length \textgreater{} 1}
\CommentTok{\#\textgreater{} and only the first element will be used}
\FunctionTok{str}\NormalTok{(nearest\_obs)}
\CommentTok{\#\textgreater{} \textquotesingle{}data.frame\textquotesingle{}:    3103 obs. of  20 variables:}
\CommentTok{\#\textgreater{}  $ dist1 : num  168.2 112 139.9 172 56.4 ...}
\CommentTok{\#\textgreater{}  $ dist2 : num  204 165 164 173 127 ...}
\CommentTok{\#\textgreater{}  $ dist3 : num  239 183 210 230 139 ...}
\CommentTok{\#\textgreater{}  $ dist4 : num  282 268 246 241 265 ...}
\CommentTok{\#\textgreater{}  $ dist5 : num  370 331 330 335 297 ...}
\CommentTok{\#\textgreater{}  $ dist6 : num  407 355 370 380 306 ...}
\CommentTok{\#\textgreater{}  $ dist7 : num  429 406 391 388 390 ...}
\CommentTok{\#\textgreater{}  $ dist8 : num  504 448 473 472 393 ...}
\CommentTok{\#\textgreater{}  $ dist9 : num  523 476 484 496 429 ...}
\CommentTok{\#\textgreater{}  $ dist10: num  524 480 488 497 431 ...}
\CommentTok{\#\textgreater{}  $ obs1  : num  1022 1022 1022 640 1022 ...}
\CommentTok{\#\textgreater{}  $ obs2  : num  640 640 640 1022 1141 ...}
\CommentTok{\#\textgreater{}  $ obs3  : num  1141 1141 1141 257 640 ...}
\CommentTok{\#\textgreater{}  $ obs4  : num  257 257 257 1141 257 ...}
\CommentTok{\#\textgreater{}  $ obs5  : num  346 346 346 346 346 346 346 346 346 346 ...}
\CommentTok{\#\textgreater{}  $ obs6  : num  406 406 406 269 406 406 406 269 257 406 ...}
\CommentTok{\#\textgreater{}  $ obs7  : num  269 269 269 406 269 ...}
\CommentTok{\#\textgreater{}  $ obs8  : num  1096 1096 1096 281 1096 ...}
\CommentTok{\#\textgreater{}  $ obs9  : num  347 347 347 347 504 347 347 279 269 504 ...}
\CommentTok{\#\textgreater{}  $ obs10 : num  281 504 281 279 347 504 279 347 347 347 ...}
\end{Highlighting}
\end{Shaded}

which produces 20 grids showing assigned values from 1st to 10th
neighbor and distances. We can plot values based on the first neighbor,
which corresponds to using e.g.~\href{https://r-spatial.github.io/sf/reference/geos_unary.html}{Voronoi polygons}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{meuse.gridF }\OtherTok{=}\NormalTok{ meuse.grid}
\NormalTok{meuse.gridF}\SpecialCharTok{@}\NormalTok{data }\OtherTok{=}\NormalTok{ nearest\_obs}
\FunctionTok{spplot}\NormalTok{(meuse.gridF[}\DecValTok{11}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{spatial-interpolation_files/figure-latex/map-ob1-1} 

}

\caption{Values of first neighbor for meuse dataset.}\label{fig:map-ob1}
\end{figure}

Next, we can estimate the same values for training points, but this time
we remove any duplicates using \texttt{rm.dupl\ =\ TRUE}:

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\# training points}
\NormalTok{nearest\_obs.dev }\OtherTok{\textless{}{-}}\NormalTok{ meteo}\SpecialCharTok{::}\FunctionTok{near.obs}\NormalTok{(}\AttributeTok{locations =}\NormalTok{ meuse, }
                                   \AttributeTok{locations.x.y =} \FunctionTok{c}\NormalTok{(}\StringTok{"x"}\NormalTok{,}\StringTok{"y"}\NormalTok{), }
                                   \AttributeTok{observations =}\NormalTok{ meuse, }
                                   \AttributeTok{observations.x.y=}\FunctionTok{c}\NormalTok{(}\StringTok{"x"}\NormalTok{,}\StringTok{"y"}\NormalTok{), }
                                   \AttributeTok{zcol =} \StringTok{"zinc"}\NormalTok{, }\AttributeTok{n.obs =} \DecValTok{10}\NormalTok{, }\AttributeTok{rm.dupl =} \ConstantTok{TRUE}\NormalTok{)}
\CommentTok{\#\textgreater{} Warning in if (class(knn1$nn.idx) != "integer") \{: the condition has length \textgreater{} 1}
\CommentTok{\#\textgreater{} and only the first element will be used}
\NormalTok{meuse}\SpecialCharTok{@}\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{cbind}\NormalTok{(meuse}\SpecialCharTok{@}\NormalTok{data, nearest\_obs.dev)}
\end{Highlighting}
\end{Shaded}

Finally, we can fit a model to predict values purely based on spatial
autocorrelation between values (1st to 10th nearest neighbour):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fm.RFSI }\OtherTok{\textless{}{-}} \FunctionTok{as.formula}\NormalTok{(}\FunctionTok{paste}\NormalTok{(}\StringTok{"zinc \textasciitilde{} "}\NormalTok{, }\FunctionTok{paste}\NormalTok{(}\FunctionTok{paste0}\NormalTok{(}\StringTok{"dist"}\NormalTok{, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{), }\AttributeTok{collapse=}\StringTok{"+"}\NormalTok{), }\StringTok{"+"}\NormalTok{, }\FunctionTok{paste}\NormalTok{(}\FunctionTok{paste0}\NormalTok{(}\StringTok{"obs"}\NormalTok{, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{), }\AttributeTok{collapse=}\StringTok{"+"}\NormalTok{)))}
\NormalTok{fm.RFSI}
\CommentTok{\#\textgreater{} zinc \textasciitilde{} dist1 + dist2 + dist3 + dist4 + dist5 + dist6 + dist7 + }
\CommentTok{\#\textgreater{}     dist8 + dist9 + dist10 + obs1 + obs2 + obs3 + obs4 + obs5 + }
\CommentTok{\#\textgreater{}     obs6 + obs7 + obs8 + obs9 + obs10}
\NormalTok{rf\_RFSI }\OtherTok{\textless{}{-}} \FunctionTok{ranger}\NormalTok{(fm.RFSI, }\AttributeTok{data=}\NormalTok{meuse}\SpecialCharTok{@}\NormalTok{data, }\AttributeTok{importance =} \StringTok{"impurity"}\NormalTok{, }\AttributeTok{num.trees =} \DecValTok{85}\NormalTok{, }\AttributeTok{keep.inbag =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{rf\_RFSI}
\CommentTok{\#\textgreater{} Ranger result}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{}  ranger(fm.RFSI, data = meuse@data, importance = "impurity", num.trees = 85,      keep.inbag = TRUE) }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Type:                             Regression }
\CommentTok{\#\textgreater{} Number of trees:                  85 }
\CommentTok{\#\textgreater{} Sample size:                      155 }
\CommentTok{\#\textgreater{} Number of independent variables:  20 }
\CommentTok{\#\textgreater{} Mtry:                             4 }
\CommentTok{\#\textgreater{} Target node size:                 5 }
\CommentTok{\#\textgreater{} Variable importance mode:         impurity }
\CommentTok{\#\textgreater{} Splitrule:                        variance }
\CommentTok{\#\textgreater{} OOB prediction error (MSE):       65997.22 }
\CommentTok{\#\textgreater{} R squared (OOB):                  0.5101999}
\end{Highlighting}
\end{Shaded}

To produce predictions we can run:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{out }\OtherTok{=} \FunctionTok{predict}\NormalTok{(rf\_RFSI, meuse.gridF}\SpecialCharTok{@}\NormalTok{data)}
\NormalTok{meuse.grid}\SpecialCharTok{$}\NormalTok{zinc.rfsi }\OtherTok{=}\NormalTok{ out}\SpecialCharTok{$}\NormalTok{predictions}
\NormalTok{op }\OtherTok{\textless{}{-}} \FunctionTok{par}\NormalTok{(}\AttributeTok{oma=}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{), }\AttributeTok{mar=}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{3}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(}\FunctionTok{raster}\NormalTok{(meuse.grid[}\StringTok{"zinc.rfsi"}\NormalTok{]), }\AttributeTok{col=}\NormalTok{R\_pal[[}\StringTok{"rainbow\_75"}\NormalTok{]][}\DecValTok{4}\SpecialCharTok{:}\DecValTok{20}\NormalTok{],}
     \AttributeTok{main=}\StringTok{"Predictions RFSI"}\NormalTok{, }\AttributeTok{axes=}\ConstantTok{FALSE}\NormalTok{, }\AttributeTok{box=}\ConstantTok{FALSE}\NormalTok{)}
\FunctionTok{points}\NormalTok{(meuse, }\AttributeTok{pch=}\StringTok{"+"}\NormalTok{, }\AttributeTok{cex=}\NormalTok{.}\DecValTok{8}\NormalTok{)}
\FunctionTok{par}\NormalTok{(op)}
\CommentTok{\#dev.off()}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{spatial-interpolation_files/figure-latex/map-r-1} 

}

\caption{Values of first neighbor for meuse dataset.}\label{fig:map-r}
\end{figure}

In summary, based on the Figs. \ref{fig:map-buff} and \ref{fig:map-r},
we can conclude that predictions produced using nearest neighbors (Fig. \ref{fig:map-r}) show quite different patterns than
predictions based on buffer distances (Fig. \ref{fig:map-buff}). The method by \citet{sekulic2020random}
(\textbf{Random Forest Spatial Interpolation}) RFSI is probably more interesting for general applications as it could be
also added to spatiotemporal data problems. It also reflects closely idea of using
spatial autocorrelation of values as used in kriging since both values of neighbors and
distances to neighbors are used as covariates. On the other hand, RFSI seem to
produce predictions that contain also short range variability (more noisy) and as
such predictions might appear to look more like geostatistical simulations.

\hypertarget{interpolation-of-numeric-values-using-spatial-regression}{%
\section{Interpolation of numeric values using spatial regression}\label{interpolation-of-numeric-values-using-spatial-regression}}

We load the packages that will be used in this tutorial:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(landmap)}
\FunctionTok{library}\NormalTok{(rgdal)}
\FunctionTok{library}\NormalTok{(geoR)}
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{}  Analysis of Geostatistical Data}
\CommentTok{\#\textgreater{}  For an Introduction to geoR go to http://www.leg.ufpr.br/geoR}
\CommentTok{\#\textgreater{}  geoR version 1.8{-}1 (built on 2020{-}02{-}08) is now loaded}
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\FunctionTok{library}\NormalTok{(plotKML)}
\FunctionTok{library}\NormalTok{(raster)}
\FunctionTok{library}\NormalTok{(glmnet)}
\FunctionTok{library}\NormalTok{(xgboost)}
\FunctionTok{library}\NormalTok{(kernlab)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Attaching package: \textquotesingle{}kernlab\textquotesingle{}}
\CommentTok{\#\textgreater{} The following objects are masked from \textquotesingle{}package:raster\textquotesingle{}:}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}     buffer, rotated}
\FunctionTok{library}\NormalTok{(deepnet)}
\FunctionTok{library}\NormalTok{(forestError)}
\FunctionTok{library}\NormalTok{(mlr)}
\end{Highlighting}
\end{Shaded}

For testing we use meuse data set. We can fit a 2D model to interpolate zinc
concentration based on sampling points, distance to the river and flooding frequency
maps by using:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{demo}\NormalTok{(meuse, }\AttributeTok{echo=}\ConstantTok{FALSE}\NormalTok{)}
\NormalTok{m }\OtherTok{\textless{}{-}} \FunctionTok{train.spLearner}\NormalTok{(meuse[}\StringTok{"zinc"}\NormalTok{], }\AttributeTok{covariates=}\NormalTok{meuse.grid[,}\FunctionTok{c}\NormalTok{(}\StringTok{"dist"}\NormalTok{,}\StringTok{"ffreq"}\NormalTok{)], }
                     \AttributeTok{lambda =} \DecValTok{1}\NormalTok{, }\AttributeTok{parallel=}\ConstantTok{FALSE}\NormalTok{)}
\CommentTok{\#\textgreater{} \# weights:  103}
\CommentTok{\#\textgreater{} initial  value 51302358.935975 }
\CommentTok{\#\textgreater{} final  value 19491244.345324 }
\CommentTok{\#\textgreater{} converged}
\CommentTok{\#\textgreater{} \# weights:  103}
\CommentTok{\#\textgreater{} initial  value 45220763.375836 }
\CommentTok{\#\textgreater{} final  value 16169762.129496 }
\CommentTok{\#\textgreater{} converged}
\CommentTok{\#\textgreater{} \# weights:  103}
\CommentTok{\#\textgreater{} initial  value 50723533.722266 }
\CommentTok{\#\textgreater{} final  value 18557431.400000 }
\CommentTok{\#\textgreater{} converged}
\CommentTok{\#\textgreater{} \# weights:  103}
\CommentTok{\#\textgreater{} initial  value 52179393.837805 }
\CommentTok{\#\textgreater{} final  value 19673925.525180 }
\CommentTok{\#\textgreater{} converged}
\CommentTok{\#\textgreater{} \# weights:  103}
\CommentTok{\#\textgreater{} initial  value 48966129.905012 }
\CommentTok{\#\textgreater{} final  value 19237393.850000 }
\CommentTok{\#\textgreater{} converged}
\CommentTok{\#\textgreater{} \# weights:  103}
\CommentTok{\#\textgreater{} initial  value 50327217.343373 }
\CommentTok{\#\textgreater{} final  value 19238858.992857 }
\CommentTok{\#\textgreater{} converged}
\CommentTok{\#\textgreater{} \# weights:  103}
\CommentTok{\#\textgreater{} initial  value 50106999.812372 }
\CommentTok{\#\textgreater{} final  value 19072846.949640 }
\CommentTok{\#\textgreater{} converged}
\CommentTok{\#\textgreater{} \# weights:  103}
\CommentTok{\#\textgreater{} initial  value 48513819.381025 }
\CommentTok{\#\textgreater{} final  value 18339886.345324 }
\CommentTok{\#\textgreater{} converged}
\CommentTok{\#\textgreater{} \# weights:  103}
\CommentTok{\#\textgreater{} initial  value 48861791.596477 }
\CommentTok{\#\textgreater{} final  value 18454493.171429 }
\CommentTok{\#\textgreater{} converged}
\CommentTok{\#\textgreater{} \# weights:  103}
\CommentTok{\#\textgreater{} initial  value 48476787.002316 }
\CommentTok{\#\textgreater{} final  value 18430903.600000 }
\CommentTok{\#\textgreater{} converged}
\CommentTok{\#\textgreater{} \# weights:  103}
\CommentTok{\#\textgreater{} initial  value 54933965.304156 }
\CommentTok{\#\textgreater{} final  value 20750447.509677 }
\CommentTok{\#\textgreater{} converged}
\end{Highlighting}
\end{Shaded}

This runs number of steps including derivation of geographical distances \citep{moller2020oblique},
derivation of principal components (to make sure all features are numeric and complete),
fitting of variogram using the \textbf{geoR} package \citep{Diggle2007Springer}, spatial overlay,
training of individual learners and training of the super learner. In principle, the only
parameter we need to set manually in the \texttt{train.spLearner} is the \texttt{lambda\ =\ 1}
which is required to estimate variogram: in this case the target variable is
log-normally distributed, and hence the geoR package needs the transformation
parameter set at \texttt{lambda\ =\ 1}.

Note that the default meta-learner in \texttt{train.spLearner} is a linear model from
five independently fitted learners \texttt{c("regr.ranger",\ "regr.xgboost",\ "regr.ksvm",\ "regr.nnet",\ "regr.cvglmnet")}. We can check the success of training based on the 5-fold
spatial Cross-Validation using:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(m}\SpecialCharTok{@}\NormalTok{spModel}\SpecialCharTok{$}\NormalTok{learner.model}\SpecialCharTok{$}\NormalTok{super.model}\SpecialCharTok{$}\NormalTok{learner.model)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} stats::lm(formula = f, data = d)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residuals:}
\CommentTok{\#\textgreater{}     Min      1Q  Median      3Q     Max }
\CommentTok{\#\textgreater{} {-}470.94  {-}97.78  {-}15.73   64.59 1049.38 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}                Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept)   2146.8982   968.9798   2.216 0.028234 *  }
\CommentTok{\#\textgreater{} regr.ranger      0.6822     0.1912   3.569 0.000483 ***}
\CommentTok{\#\textgreater{} regr.xgboost     0.5249     0.4071   1.289 0.199244    }
\CommentTok{\#\textgreater{} regr.nnet       {-}4.5017     2.0482  {-}2.198 0.029499 *  }
\CommentTok{\#\textgreater{} regr.ksvm        0.4241     0.2148   1.975 0.050145 .  }
\CommentTok{\#\textgreater{} regr.cvglmnet   {-}0.2757     0.1648  {-}1.673 0.096441 .  }
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residual standard error: 199.9 on 149 degrees of freedom}
\CommentTok{\#\textgreater{} Multiple R{-}squared:  0.7132, Adjusted R{-}squared:  0.7036 }
\CommentTok{\#\textgreater{} F{-}statistic:  74.1 on 5 and 149 DF,  p{-}value: \textless{} 2.2e{-}16}
\end{Highlighting}
\end{Shaded}

Which shows that the model explains about 65\% of variability in target variable
and that \texttt{regr.ranger} learner \citep{wright2017ranger} is the strongest learner. Average
mapping error RMSE = 213, hence the models is somewhat more accurate than if we
only used buffer distances.

To predict values at all grids we use:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{meuse.y }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(m)}
\CommentTok{\#\textgreater{} Predicting values using \textquotesingle{}getStackedBaseLearnerPredictions\textquotesingle{}...TRUE}
\CommentTok{\#\textgreater{} Deriving model errors using forestError package...TRUE}
\end{Highlighting}
\end{Shaded}

Note that, by default, we will predict two outputs:

\begin{itemize}
\tightlist
\item
  Mean prediction: i.e.~the best unbiased prediction of response;\\
\item
  Prediction errors: usually predicted as lower and upper 67\% quantiles (1 std.) based on the \href{https://cran.r-project.org/package=forestError}{forestError} \citep{lu2021unified};
\end{itemize}

If not otherwise specified, derivation of the prediction error (\textbf{Root Mean Square
Prediction Error}), bias and lower and upper prediction intervals is implemented
by default via the \href{https://cran.r-project.org/package=forestError}{forestError}
algorithm. The method is explained in detail in \citet{lu2021unified}.

We could also produce the prediction intervals by using the \textbf{quantreg} Random Forest
algorithm \citep{meinshausen2006quantile} as implemented in the ranger package, or as
a standard deviation of the bootstraped models, although using the method by \citet{lu2021unified} is recommended.

To determine the prediction errors without drastically increasing computing time,
we basically fit an independent random forest model using the five base-learners
with setting \texttt{quantreg\ =\ TRUE}:

\begin{verbatim}
zinc ~ regr.ranger + regr.xgboost + regr.nnet + regr.ksvm + regr.cvglmnet
\end{verbatim}

The prediction error methods are non-parameteric and users can choose any
probability in the output via the \texttt{quantiles} argument. For example, the default
\texttt{quantiles} are set to produce prediction intervals for the .682 range, which
is the 1-standard-deviation range in the case of a Gaussian distribution.
Deriving prediction errors, however, can be come computational for large number
of features and trees in the random forest, so have in mind that EML comes with
exponentially increased computing time.

We can plot the predictions and prediction errors next to each other by using:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{), }\AttributeTok{oma=}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{), }\AttributeTok{mar=}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{3}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(}\FunctionTok{raster}\NormalTok{(meuse.y}\SpecialCharTok{$}\NormalTok{pred[}\StringTok{"response"}\NormalTok{]), }\AttributeTok{col=}\NormalTok{R\_pal[[}\StringTok{"rainbow\_75"}\NormalTok{]][}\DecValTok{4}\SpecialCharTok{:}\DecValTok{20}\NormalTok{],}
     \AttributeTok{main=}\StringTok{"Predictions spLearner"}\NormalTok{, }\AttributeTok{axes=}\ConstantTok{FALSE}\NormalTok{, }\AttributeTok{box=}\ConstantTok{FALSE}\NormalTok{)}
\FunctionTok{points}\NormalTok{(meuse, }\AttributeTok{pch=}\StringTok{"+"}\NormalTok{, }\AttributeTok{cex=}\NormalTok{.}\DecValTok{8}\NormalTok{)}
\FunctionTok{plot}\NormalTok{(}\FunctionTok{raster}\NormalTok{(meuse.y}\SpecialCharTok{$}\NormalTok{pred[}\StringTok{"model.error"}\NormalTok{]), }\AttributeTok{col=}\FunctionTok{rev}\NormalTok{(}\FunctionTok{bpy.colors}\NormalTok{()),}
     \AttributeTok{main=}\StringTok{"Prediction errors"}\NormalTok{, }\AttributeTok{axes=}\ConstantTok{FALSE}\NormalTok{, }\AttributeTok{box=}\ConstantTok{FALSE}\NormalTok{)}
\FunctionTok{points}\NormalTok{(meuse, }\AttributeTok{pch=}\StringTok{"+"}\NormalTok{, }\AttributeTok{cex=}\NormalTok{.}\DecValTok{8}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{spatial-interpolation_files/figure-latex/map-zinc-1} 

}

\caption{Predicted zinc content based on meuse data set.}\label{fig:map-zinc}
\end{figure}

This shows that the prediction errors (right plot) are the highest:

\begin{itemize}
\tightlist
\item
  where the model is getting further away from the training points (spatial extrapolation),\\
\item
  where individual points with high values can not be explained by covariates,\\
\item
  where measured values of the response variable are in general high,
\end{itemize}

We can also plot the lower and upper prediction intervals for the .682
probability range using:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pts }\OtherTok{=} \FunctionTok{list}\NormalTok{(}\StringTok{"sp.points"}\NormalTok{, meuse, }\AttributeTok{pch =} \StringTok{"+"}\NormalTok{, }\AttributeTok{col=}\StringTok{"black"}\NormalTok{)}
\FunctionTok{spplot}\NormalTok{(meuse.y}\SpecialCharTok{$}\NormalTok{pred[,}\FunctionTok{c}\NormalTok{(}\StringTok{"q.lwr"}\NormalTok{,}\StringTok{"q.upr"}\NormalTok{)], }\AttributeTok{col.regions=}\NormalTok{R\_pal[[}\StringTok{"rainbow\_75"}\NormalTok{]][}\DecValTok{4}\SpecialCharTok{:}\DecValTok{20}\NormalTok{],}
       \AttributeTok{sp.layout =} \FunctionTok{list}\NormalTok{(pts),}
       \AttributeTok{main=}\StringTok{"Prediction intervals (alpha = 0.318)"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{spatial-interpolation_files/figure-latex/map-zinc-interval-1} 

}

\caption{Lower (q.lwr) and upper (q.upr) prediction intervals for zinc content based on meuse data set.}\label{fig:map-zinc-interval}
\end{figure}

\hypertarget{model-fine-tuning-and-feature-selection}{%
\section{Model fine-tuning and feature selection}\label{model-fine-tuning-and-feature-selection}}

The function \texttt{tune.spLearner} can be used to further optimize spLearner object by:

\begin{itemize}
\tightlist
\item
  fine-tuning model parameters, especially the ranger \texttt{mtry} and XGBoost parameters,\\
\item
  reduce number of features by running feature selection via the \texttt{mlr::makeFeatSelWrapper} function,
\end{itemize}

The package landmap currently requires that two base learners used include \texttt{regr.ranger} and
\texttt{regr.xgboost}, and that there are at least 3 base learners in total. The model from above can be optimized using:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m0 }\OtherTok{\textless{}{-}} \FunctionTok{tune.spLearner}\NormalTok{(m, }\AttributeTok{xg.skip=}\ConstantTok{TRUE}\NormalTok{, }\AttributeTok{parallel=}\ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

which reports RMSE for different \texttt{mtry} and reports which features have been left and which removed. Note that we turn off the fine-tuning of XGboost using \texttt{xg.skip\ =\ TRUE} as it takes at the order of magnitude more time. In summary, in this specific case, the fine-tuned model is not much more accurate, but it comes with the less features:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str}\NormalTok{(m0}\SpecialCharTok{@}\NormalTok{spModel}\SpecialCharTok{$}\NormalTok{features)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
chr [1:11] "PC2" "PC3" "PC4" "rX_0" "rY_0" "rY_0.2" "rX_0.5" "rY_1" "rY_1.4" "rY_2.9" "rY_3.1"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(m0}\SpecialCharTok{@}\NormalTok{spModel}\SpecialCharTok{$}\NormalTok{learner.model}\SpecialCharTok{$}\NormalTok{super.model}\SpecialCharTok{$}\NormalTok{learner.model)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Residuals:
    Min      1Q  Median      3Q     Max 
-404.09 -139.03  -42.05   64.69 1336.47 

Coefficients:
                Estimate Std. Error t value Pr(>|t|)   
(Intercept)   2091.87119  661.70995   3.161  0.00190 **
regr.ranger      0.14278    0.24177   0.591  0.55570   
regr.xgboost     0.92283    0.53131   1.737  0.08448 . 
regr.nnet       -4.34961    1.38703  -3.136  0.00206 **
regr.ksvm        0.66590    0.25027   2.661  0.00865 **
regr.cvglmnet   -0.08703    0.13808  -0.630  0.52944   
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 245.2 on 149 degrees of freedom
Multiple R-squared:  0.5683,    Adjusted R-squared:  0.5538 
F-statistic: 39.23 on 5 and 149 DF,  p-value: < 2.2e-16
\end{verbatim}

Note that fine-tuning and feature selection can be quite computational and it is
highly recommended to start with smaller subsets of data and then measure processing
time. Note that the function \texttt{mlr::makeFeatSelWrapper} can result in errors if
the covariates have a low variance or follow a zero-inflated distribution.
Reducing the number of features via feature selection and fine-tuning of the Random
Forest \texttt{mtry} and XGboost parameters, however, can result in significantly higher
prediction speed and can also help improve accuracy.

\hypertarget{estimation-of-prediction-intervals}{%
\section{Estimation of prediction intervals}\label{estimation-of-prediction-intervals}}

We can also print the lower and upper \href{http://www.sthda.com/english/articles/40-regression-analysis/166-predict-in-r-model-predictions-and-confidence-intervals/}{prediction interval} for every location using e.g.:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sp}\SpecialCharTok{::}\FunctionTok{over}\NormalTok{(meuse[}\DecValTok{1}\NormalTok{,], meuse.y}\SpecialCharTok{$}\NormalTok{pred)}
\CommentTok{\#\textgreater{}   response model.error model.bias    q.lwr    q.upr}
\CommentTok{\#\textgreater{} 1 999.8759    214.1839   27.39722 749.5775 1289.188}
\end{Highlighting}
\end{Shaded}

where \texttt{q.lwr} is the lower and \texttt{q.upr} is the 68\% probability upper quantile value. This shows that the 68\% probability interval for the location \texttt{x=181072,\ y=333611} is about 734--1241 which means that the prediction error (±1 s.d.), at that location, is about 250. Compare with the actual value sampled at that location:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{meuse}\SpecialCharTok{@}\NormalTok{data[}\DecValTok{1}\NormalTok{,}\StringTok{"zinc"}\NormalTok{]}
\CommentTok{\#\textgreater{} [1] 1022}
\end{Highlighting}
\end{Shaded}

The average prediction error for the whole area is:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(meuse.y}\SpecialCharTok{$}\NormalTok{pred}\SpecialCharTok{$}\NormalTok{model.error)}
\CommentTok{\#\textgreater{}    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. }
\CommentTok{\#\textgreater{}   44.63   77.04  167.69  159.58  215.06  480.11}
\end{Highlighting}
\end{Shaded}

which is somewhat lower than the RMSE derived by cross-validation, but this is
also because most of the predicted values are in fact low (skewed distribution),
and EML seems not have many problems predicting low values.

Note also, from the example above, if we refit a model using exactly the same
settings we might get somewhat different maps and different values. This is to
be expected as the number of training points and covariates is low, the stacking
is done by using (random) 5-fold Cross-validation, and hence results will always
be slightly different. The resulting models and maps, however, should not be
significantly different as this would indicate that the Ensemble ML is \emph{unstable}.
In the case of larger datasets (≫1000 points), differences between predictions
should become less and less visible.

\hypertarget{predictions-using-log-transformed-target-variable}{%
\section{Predictions using log-transformed target variable}\label{predictions-using-log-transformed-target-variable}}

If the purpose of spatial prediction to make a more accurate predictions of low(er)
values of the response, then we can train a model with the transformed variable:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{meuse}\SpecialCharTok{$}\NormalTok{log.zinc }\OtherTok{=} \FunctionTok{log1p}\NormalTok{(meuse}\SpecialCharTok{$}\NormalTok{zinc)}
\NormalTok{m2 }\OtherTok{\textless{}{-}} \FunctionTok{train.spLearner}\NormalTok{(meuse[}\StringTok{"log.zinc"}\NormalTok{], }\AttributeTok{covariates=}\NormalTok{meuse.grid[,}\FunctionTok{c}\NormalTok{(}\StringTok{"dist"}\NormalTok{,}\StringTok{"ffreq"}\NormalTok{)], }\AttributeTok{parallel=}\ConstantTok{FALSE}\NormalTok{)}
\CommentTok{\#\textgreater{} \# weights:  103}
\CommentTok{\#\textgreater{} initial  value 4998.882579 }
\CommentTok{\#\textgreater{} final  value 73.222851 }
\CommentTok{\#\textgreater{} converged}
\CommentTok{\#\textgreater{} \# weights:  103}
\CommentTok{\#\textgreater{} initial  value 5683.969732 }
\CommentTok{\#\textgreater{} final  value 68.561330 }
\CommentTok{\#\textgreater{} converged}
\CommentTok{\#\textgreater{} \# weights:  103}
\CommentTok{\#\textgreater{} initial  value 4834.044628 }
\CommentTok{\#\textgreater{} final  value 68.897172 }
\CommentTok{\#\textgreater{} converged}
\CommentTok{\#\textgreater{} \# weights:  103}
\CommentTok{\#\textgreater{} initial  value 5706.570659 }
\CommentTok{\#\textgreater{} final  value 72.649748 }
\CommentTok{\#\textgreater{} converged}
\CommentTok{\#\textgreater{} \# weights:  103}
\CommentTok{\#\textgreater{} initial  value 2349.857144 }
\CommentTok{\#\textgreater{} final  value 73.103072 }
\CommentTok{\#\textgreater{} converged}
\CommentTok{\#\textgreater{} \# weights:  103}
\CommentTok{\#\textgreater{} initial  value 6184.275769 }
\CommentTok{\#\textgreater{} final  value 74.169414 }
\CommentTok{\#\textgreater{} converged}
\CommentTok{\#\textgreater{} \# weights:  103}
\CommentTok{\#\textgreater{} initial  value 4451.463365 }
\CommentTok{\#\textgreater{} final  value 69.440176 }
\CommentTok{\#\textgreater{} converged}
\CommentTok{\#\textgreater{} \# weights:  103}
\CommentTok{\#\textgreater{} initial  value 4852.414287 }
\CommentTok{\#\textgreater{} final  value 72.774801 }
\CommentTok{\#\textgreater{} converged}
\CommentTok{\#\textgreater{} \# weights:  103}
\CommentTok{\#\textgreater{} initial  value 5902.359343 }
\CommentTok{\#\textgreater{} final  value 72.079999 }
\CommentTok{\#\textgreater{} converged}
\CommentTok{\#\textgreater{} \# weights:  103}
\CommentTok{\#\textgreater{} initial  value 6689.386512 }
\CommentTok{\#\textgreater{} final  value 72.803948 }
\CommentTok{\#\textgreater{} converged}
\CommentTok{\#\textgreater{} \# weights:  103}
\CommentTok{\#\textgreater{} initial  value 5607.506170 }
\CommentTok{\#\textgreater{} final  value 79.790191 }
\CommentTok{\#\textgreater{} converged}
\end{Highlighting}
\end{Shaded}

The summary model will usually have a somewhat higher R-square, but the best learners should stay about the same:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(m2}\SpecialCharTok{@}\NormalTok{spModel}\SpecialCharTok{$}\NormalTok{learner.model}\SpecialCharTok{$}\NormalTok{super.model}\SpecialCharTok{$}\NormalTok{learner.model)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} stats::lm(formula = f, data = d)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residuals:}
\CommentTok{\#\textgreater{}      Min       1Q   Median       3Q      Max }
\CommentTok{\#\textgreater{} {-}1.00361 {-}0.18873 {-}0.05022  0.14092  1.41474 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}               Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept)   25.75549   10.10511   2.549 0.011822 *  }
\CommentTok{\#\textgreater{} regr.ranger    0.74984    0.19969   3.755 0.000248 ***}
\CommentTok{\#\textgreater{} regr.xgboost   0.31617    0.33255   0.951 0.343264    }
\CommentTok{\#\textgreater{} regr.nnet     {-}4.44329    1.70769  {-}2.602 0.010206 *  }
\CommentTok{\#\textgreater{} regr.ksvm      0.28476    0.19541   1.457 0.147146    }
\CommentTok{\#\textgreater{} regr.cvglmnet {-}0.07384    0.15905  {-}0.464 0.643137    }
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residual standard error: 0.3574 on 149 degrees of freedom}
\CommentTok{\#\textgreater{} Multiple R{-}squared:  0.7615, Adjusted R{-}squared:  0.7535 }
\CommentTok{\#\textgreater{} F{-}statistic: 95.14 on 5 and 149 DF,  p{-}value: \textless{} 2.2e{-}16}
\end{Highlighting}
\end{Shaded}

We can next predict and then back-transform the values:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{meuse.y2 }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(m2)}
\CommentTok{\#\textgreater{} Predicting values using \textquotesingle{}getStackedBaseLearnerPredictions\textquotesingle{}...TRUE}
\CommentTok{\#\textgreater{} Deriving model errors using forestError package...TRUE}
\DocumentationTok{\#\# back{-}transform:}
\NormalTok{meuse.y2}\SpecialCharTok{$}\NormalTok{pred}\SpecialCharTok{$}\NormalTok{response.t }\OtherTok{=} \FunctionTok{expm1}\NormalTok{(meuse.y2}\SpecialCharTok{$}\NormalTok{pred}\SpecialCharTok{$}\NormalTok{response)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{spatial-interpolation_files/figure-latex/map-zinc2-1} 

}

\caption{Predicted zinc content based on meuse data set after log-transformation.}\label{fig:map-zinc2}
\end{figure}

The predictions (Figs. \ref{fig:map-zinc} and \ref{fig:map-zinc2}) show similar
patterns but the prediction error maps are quite different in this case. Nevertheless,
the problem areas seem to match in both maps (see Figs. \ref{fig:map-zinc} and \ref{fig:map-zinc2} right part).
If we compare distributions of two predictions we can also see that the predictions do not differ much:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggridges)}
\FunctionTok{library}\NormalTok{(viridis)}
\CommentTok{\#\textgreater{} Loading required package: viridisLite}
\FunctionTok{library}\NormalTok{(ggplot2)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Attaching package: \textquotesingle{}ggplot2\textquotesingle{}}
\CommentTok{\#\textgreater{} The following object is masked from \textquotesingle{}package:kernlab\textquotesingle{}:}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}     alpha}
\NormalTok{zinc.df }\OtherTok{=} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{zinc=}\FunctionTok{c}\NormalTok{(sp}\SpecialCharTok{::}\FunctionTok{over}\NormalTok{(meuse, meuse.y}\SpecialCharTok{$}\NormalTok{pred[}\StringTok{"response"}\NormalTok{])[,}\DecValTok{1}\NormalTok{], }
\NormalTok{                            sp}\SpecialCharTok{::}\FunctionTok{over}\NormalTok{(meuse, meuse.y2}\SpecialCharTok{$}\NormalTok{pred[}\StringTok{"response.t"}\NormalTok{])[,}\DecValTok{1}\NormalTok{],}
\NormalTok{                            meuse}\SpecialCharTok{$}\NormalTok{zinc}
\NormalTok{))}
\NormalTok{zinc.df}\SpecialCharTok{$}\NormalTok{type }\OtherTok{=} \FunctionTok{as.vector}\NormalTok{(}\FunctionTok{sapply}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{"predicted"}\NormalTok{, }\StringTok{"log.predicted"}\NormalTok{, }\StringTok{"observed"}\NormalTok{), }\ControlFlowTok{function}\NormalTok{(i)\{}\FunctionTok{rep}\NormalTok{(i, }\FunctionTok{nrow}\NormalTok{(meuse))\}))}
\FunctionTok{ggplot}\NormalTok{(zinc.df, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ zinc, }\AttributeTok{y =}\NormalTok{ type, }\AttributeTok{fill =}\NormalTok{ ..x..)) }\SpecialCharTok{+}
  \FunctionTok{geom\_density\_ridges\_gradient}\NormalTok{(}\AttributeTok{scale =} \FloatTok{0.95}\NormalTok{, }\AttributeTok{rel\_min\_height =} \FloatTok{0.01}\NormalTok{, }\AttributeTok{gradient\_lwd =} \FloatTok{1.}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_x\_continuous}\NormalTok{(}\AttributeTok{expand =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.01}\NormalTok{, }\DecValTok{0}\NormalTok{)) }\SpecialCharTok{+}
  \DocumentationTok{\#\# scale\_x\_continuous(trans=\textquotesingle{}log2\textquotesingle{}) +}
  \FunctionTok{scale\_y\_discrete}\NormalTok{(}\AttributeTok{expand =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.01}\NormalTok{, }\FloatTok{0.01}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{scale\_fill\_viridis}\NormalTok{(}\AttributeTok{name =} \StringTok{"Zinc"}\NormalTok{, }\AttributeTok{option =} \StringTok{"C"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Distributions comparison"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_ridges}\NormalTok{(}\AttributeTok{font\_size =} \DecValTok{13}\NormalTok{, }\AttributeTok{grid =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{+} \FunctionTok{theme}\NormalTok{(}\AttributeTok{axis.title.y =} \FunctionTok{element\_blank}\NormalTok{())}
\CommentTok{\#\textgreater{} Picking joint bandwidth of 110}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{spatial-interpolation_files/figure-latex/hist-zinc2-1} 

}

\caption{Difference in distributions observed and predicted.}\label{fig:hist-zinc2}
\end{figure}

The observed very high values are somewhat smoothed out but the median value is
about the same, hence we can conclude that the two EML models predict the target
variable without a bias. To estimate the prediction intervals using the log-transformed
variable we can use:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\OtherTok{=}\NormalTok{ sp}\SpecialCharTok{::}\FunctionTok{over}\NormalTok{(meuse[}\DecValTok{1}\NormalTok{,], meuse.y2}\SpecialCharTok{$}\NormalTok{pred)}
\FunctionTok{expm1}\NormalTok{(x}\SpecialCharTok{$}\NormalTok{q.lwr); }\FunctionTok{expm1}\NormalTok{(x}\SpecialCharTok{$}\NormalTok{q.upr)}
\CommentTok{\#\textgreater{} [1] 837.8133}
\CommentTok{\#\textgreater{} [1] 1638.359}
\end{Highlighting}
\end{Shaded}

Note that the log-transformation is not needed for a non-linear learner such
ranger and/or Xgboost, but it is often a good idea if the focus of prediction is
to get a better accuracy for lower values \citep{hengl2021african}. For example, if the objective of spatial
interpolation is to map soil nutrient deficiencies, then log-transformation is a
good idea as it will produce slightly better accuracy for lower values.

Another advantage of using log-transformation for log-normal variables is that
the prediction intervals would most likely be symmetric, so that derivation of
prediction error (±1 s.d.) can be derived by:

\begin{verbatim}
pe = (q.upr - q.lwr)/2
\end{verbatim}

\hypertarget{spatial-prediction-of-soil-types-factor-variable}{%
\section{Spatial prediction of soil types (factor-variable)}\label{spatial-prediction-of-soil-types-factor-variable}}

Ensemble Machine Learning can also be used to interpolate factor type variables
e.g.~soil types. This is an example with the Ebergotzen dataset available from
the package plotKML \citep{hengl2015plotkml}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(plotKML)}
\FunctionTok{data}\NormalTok{(eberg\_grid)}
\FunctionTok{gridded}\NormalTok{(eberg\_grid) }\OtherTok{\textless{}{-}} \ErrorTok{\textasciitilde{}}\NormalTok{x}\SpecialCharTok{+}\NormalTok{y}
\FunctionTok{proj4string}\NormalTok{(eberg\_grid) }\OtherTok{\textless{}{-}} \FunctionTok{CRS}\NormalTok{(}\StringTok{"+init=epsg:31467"}\NormalTok{)}
\FunctionTok{data}\NormalTok{(eberg)}
\FunctionTok{coordinates}\NormalTok{(eberg) }\OtherTok{\textless{}{-}} \ErrorTok{\textasciitilde{}}\NormalTok{X}\SpecialCharTok{+}\NormalTok{Y}
\FunctionTok{proj4string}\NormalTok{(eberg) }\OtherTok{\textless{}{-}} \FunctionTok{CRS}\NormalTok{(}\StringTok{"+init=epsg:31467"}\NormalTok{)}
\FunctionTok{summary}\NormalTok{(eberg}\SpecialCharTok{$}\NormalTok{TAXGRSC)}
\CommentTok{\#\textgreater{}     Auenboden     Braunerde          Gley         HMoor    Kolluvisol }
\CommentTok{\#\textgreater{}            71           790            86             1           186 }
\CommentTok{\#\textgreater{}          Moor Parabraunerde  Pararendzina       Pelosol    Pseudogley }
\CommentTok{\#\textgreater{}             1           704           215           252           487 }
\CommentTok{\#\textgreater{}        Ranker       Regosol      Rendzina          NA\textquotesingle{}s }
\CommentTok{\#\textgreater{}            20           376            23           458}
\end{Highlighting}
\end{Shaded}

In this case the target variable is \texttt{TAXGRSC} soil types based on the German soil
classification system. This changes the modeling problem from regression to
classification. We recommend using the following learners here:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sl.c }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"classif.ranger"}\NormalTok{, }\StringTok{"classif.xgboost"}\NormalTok{, }\StringTok{"classif.nnTrain"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The model training and prediction however looks the same as for the regression:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{X }\OtherTok{\textless{}{-}}\NormalTok{ eberg\_grid[}\FunctionTok{c}\NormalTok{(}\StringTok{"PRMGEO6"}\NormalTok{,}\StringTok{"DEMSRT6"}\NormalTok{,}\StringTok{"TWISRT6"}\NormalTok{,}\StringTok{"TIRAST6"}\NormalTok{)]}
\ControlFlowTok{if}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{exists}\NormalTok{(}\StringTok{"mF"}\NormalTok{))\{}
\NormalTok{  mF }\OtherTok{\textless{}{-}} \FunctionTok{train.spLearner}\NormalTok{(eberg[}\StringTok{"TAXGRSC"}\NormalTok{], }\AttributeTok{covariates=}\NormalTok{X, }\AttributeTok{parallel=}\ConstantTok{FALSE}\NormalTok{)}
\NormalTok{\}}
\CommentTok{\#\textgreater{} Converting PRMGEO6 to indicators...}
\CommentTok{\#\textgreater{} Converting covariates to principal components...}
\CommentTok{\#\textgreater{} Deriving oblique coordinates...TRUE}
\CommentTok{\#\textgreater{} Subsetting observations to 79\% complete cases...TRUE}
\CommentTok{\#\textgreater{} Skipping variogram modeling...TRUE}
\CommentTok{\#\textgreater{} Estimating block size ID for spatial Cross Validation...TRUE}
\CommentTok{\#\textgreater{} Using learners: classif.ranger, classif.xgboost, classif.nnTrain...TRUE}
\CommentTok{\#\textgreater{} Fitting a spatial learner using \textquotesingle{}mlr::makeClassifTask\textquotesingle{}...TRUE}
\end{Highlighting}
\end{Shaded}

To generate predictions we use:

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{if}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{exists}\NormalTok{(}\StringTok{"TAXGRSC"}\NormalTok{))\{}
\NormalTok{  TAXGRSC }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(mF)}
\NormalTok{\}}
\CommentTok{\#\textgreater{} Predicting values using \textquotesingle{}getStackedBaseLearnerPredictions\textquotesingle{}...TRUE}
\CommentTok{\#\textgreater{} Deriving model errors using sd of sign. learners...TRUE}
\end{Highlighting}
\end{Shaded}

\hypertarget{classification-accuracy}{%
\section{Classification accuracy}\label{classification-accuracy}}

By default landmap package will predict both hard classes and probabilities per class. We can check the average accuracy of classification by using:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{newdata }\OtherTok{=}\NormalTok{ mF}\SpecialCharTok{@}\NormalTok{vgmModel}\SpecialCharTok{$}\NormalTok{observations}\SpecialCharTok{@}\NormalTok{data}
\NormalTok{sel.e }\OtherTok{=} \FunctionTok{complete.cases}\NormalTok{(newdata[,mF}\SpecialCharTok{@}\NormalTok{spModel}\SpecialCharTok{$}\NormalTok{features])}
\NormalTok{newdata }\OtherTok{=}\NormalTok{ newdata[sel.e, mF}\SpecialCharTok{@}\NormalTok{spModel}\SpecialCharTok{$}\NormalTok{features]}
\NormalTok{pred }\OtherTok{=} \FunctionTok{predict}\NormalTok{(mF}\SpecialCharTok{@}\NormalTok{spModel, }\AttributeTok{newdata=}\NormalTok{newdata)}
\NormalTok{pred}\SpecialCharTok{$}\NormalTok{data}\SpecialCharTok{$}\NormalTok{truth }\OtherTok{=}\NormalTok{ mF}\SpecialCharTok{@}\NormalTok{vgmModel}\SpecialCharTok{$}\NormalTok{observations}\SpecialCharTok{@}\NormalTok{data[sel.e, }\StringTok{"TAXGRSC"}\NormalTok{]}
\FunctionTok{print}\NormalTok{(}\FunctionTok{calculateConfusionMatrix}\NormalTok{(pred))}
\CommentTok{\#\textgreater{}                predicted}
\CommentTok{\#\textgreater{} true            Auenboden Braunerde Gley Kolluvisol Parabraunerde Pararendzina}
\CommentTok{\#\textgreater{}   Auenboden            34         6    0          0             5            0}
\CommentTok{\#\textgreater{}   Braunerde             0       623    0          1            17            7}
\CommentTok{\#\textgreater{}   Gley                  4         9   38          3             9            0}
\CommentTok{\#\textgreater{}   Kolluvisol            0        13    1         99            17            1}
\CommentTok{\#\textgreater{}   Parabraunerde         0        34    0          3           460            0}
\CommentTok{\#\textgreater{}   Pararendzina          0        19    0          0             3          147}
\CommentTok{\#\textgreater{}   Pelosol               0        11    0          1             1            2}
\CommentTok{\#\textgreater{}   Pseudogley            0        54    2          4            24            3}
\CommentTok{\#\textgreater{}   Ranker                0        10    0          0             6            0}
\CommentTok{\#\textgreater{}   Regosol               0        66    0          0            10            1}
\CommentTok{\#\textgreater{}   Rendzina              0         2    0          0             0            0}
\CommentTok{\#\textgreater{}   {-}err.{-}                4       224    3         12            92           14}
\CommentTok{\#\textgreater{}                predicted}
\CommentTok{\#\textgreater{} true            Pelosol Pseudogley Ranker Regosol Rendzina {-}err.{-}}
\CommentTok{\#\textgreater{}   Auenboden           0          3      0       0        0     14}
\CommentTok{\#\textgreater{}   Braunerde           9          5      0       6        1     46}
\CommentTok{\#\textgreater{}   Gley                0          5      0       0        0     30}
\CommentTok{\#\textgreater{}   Kolluvisol          1          3      0       3        0     39}
\CommentTok{\#\textgreater{}   Parabraunerde       2         10      0       4        0     53}
\CommentTok{\#\textgreater{}   Pararendzina        6          1      0       0        0     29}
\CommentTok{\#\textgreater{}   Pelosol           157          4      0       1        0     20}
\CommentTok{\#\textgreater{}   Pseudogley          4        307      0      13        0    104}
\CommentTok{\#\textgreater{}   Ranker              1          0      0       0        0     17}
\CommentTok{\#\textgreater{}   Regosol             4         12      0     220        0     93}
\CommentTok{\#\textgreater{}   Rendzina            0          0      0       0       20      2}
\CommentTok{\#\textgreater{}   {-}err.{-}             27         43      0      27        1    447}
\end{Highlighting}
\end{Shaded}

which shows that about 25\% of classes are miss-classified and the classification
confusion is especially high for the \texttt{Braunerde} class. Note the result above is
based only on the internal training. Normally one should repeat the process
several times using 5-fold or similar (i.e.~fit EML, predict errors using resampled
values only, then repeat).

Predicted probabilities, however, are more interesting because they also show
where EML possibly has problems and which are the transition zones between multiple classes:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(}\FunctionTok{stack}\NormalTok{(TAXGRSC}\SpecialCharTok{$}\NormalTok{pred[}\FunctionTok{grep}\NormalTok{(}\StringTok{"prob."}\NormalTok{, }\FunctionTok{names}\NormalTok{(TAXGRSC}\SpecialCharTok{$}\NormalTok{pred))]),}
     \AttributeTok{col=}\NormalTok{SAGA\_pal[[}\StringTok{"SG\_COLORS\_YELLOW\_RED"}\NormalTok{]], }\AttributeTok{zlim=}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{spatial-interpolation_files/figure-latex/map-tax-1} 

}

\caption{Predicted soil types based on EML.}\label{fig:map-tax}
\end{figure}

The maps show that also in this case geographical distances play a role, but
overall, the features (DTM derivatives and parnt material) seem to be most important.

In addition to map of probabilities per class, we have also derived errors per
probability, which in this case can be computed as the standard deviation between
probabilities produced by individual learners (note: for classification problems
techniques such as quantreg random forest currently do not exist):

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(}\FunctionTok{stack}\NormalTok{(TAXGRSC}\SpecialCharTok{$}\NormalTok{pred[}\FunctionTok{grep}\NormalTok{(}\StringTok{"error."}\NormalTok{, }\FunctionTok{names}\NormalTok{(TAXGRSC}\SpecialCharTok{$}\NormalTok{pred))]),}
     \AttributeTok{col=}\NormalTok{SAGA\_pal[[}\StringTok{"SG\_COLORS\_YELLOW\_BLUE"}\NormalTok{]], }\AttributeTok{zlim=}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\FloatTok{0.45}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{spatial-interpolation_files/figure-latex/map-tax-error-1} 

}

\caption{Predicted errors per soil types based on s.d. between individual learners.}\label{fig:map-tax-error}
\end{figure}

In probability space, instead of using RMSE or similar measures, it is often
recommended to use the measures such as the \href{https://www.rdocumentation.org/packages/MLmetrics/versions/1.1.1/topics/LogLoss}{log-loss} which
correctly quantifies the difference between the observed and predicted probability.
As a rule of thumb, log-loss values above 0.35 indicate poor accuracy of predictions,
but the threshold number for critically low log-loss also depends on the number
of classes. In the plot above we can note that, in general, the average error in
maps is relatively low e.g.~about 0.07:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(TAXGRSC}\SpecialCharTok{$}\NormalTok{pred}\SpecialCharTok{$}\NormalTok{error.Parabraunerde)}
\CommentTok{\#\textgreater{}     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. }
\CommentTok{\#\textgreater{} 0.001558 0.029074 0.041220 0.066121 0.064179 0.339019}
\end{Highlighting}
\end{Shaded}

but there are still many pixels where confusion between classes and prediction
errors are high. Recommended strategy to improve this map is to generate \href{https://opengeohub.github.io/spatial-sampling-ml/}{a sampling
plan using the average prediction error} and/or Confusion Index map, then collect
new observations \& measurements and refit the prediction models.

\hypertarget{spatial-interpolation-in-3d-using-ensemble-ml}{%
\chapter{Spatial interpolation in 3D using Ensemble ML}\label{spatial-interpolation-in-3d-using-ensemble-ml}}

You are reading the work-in-progress Spatial and spatiotemporal interpolation using Ensemble Machine Learning. This chapter is currently draft version, a peer-review publication is pending. You can find the polished first edition at \url{https://opengeohub.github.io/spatial-prediction-eml/}.

\hypertarget{mapping-concentrations-of-geochemical-elements}{%
\section{Mapping concentrations of geochemical elements}\label{mapping-concentrations-of-geochemical-elements}}

Ensemble ML can also be used for mapping soil variables in 3D. Consider for example
the Geochemical and minerological data set for USA48 \citep{smith2014geochemical}. This is a public data set
produced and maintained by the \href{https://mrdata.usgs.gov/ngdb/soil/}{USA Geological Survey} and contains laboratory measurements
of chemical elements and minerals in soil at 4,857 sites (for three depths 0 to 5 cm,
A horizon and C horizon; Fig. \ref{fig:ds801-example}).

We have previously imported and overlaid the sampling points vs large stack of
covariate layers using e.g.:

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\# Training data {-}{-}{-}{-}}
\NormalTok{ngs.m }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"./training\_data/ds\_801\_all.csv"}\NormalTok{)}
\NormalTok{ngs.m}\SpecialCharTok{$}\NormalTok{olc\_id }\OtherTok{=}\NormalTok{ olctools}\SpecialCharTok{::}\FunctionTok{encode\_olc}\NormalTok{(ngs.m}\SpecialCharTok{$}\NormalTok{latitude, ngs.m}\SpecialCharTok{$}\NormalTok{longitude, }\DecValTok{11}\NormalTok{)}
\DocumentationTok{\#\# Spatial overlay {-}{-}{-}{-}}
\NormalTok{sel.pnts }\OtherTok{=} \SpecialCharTok{!}\FunctionTok{duplicated}\NormalTok{(ngs.m}\SpecialCharTok{$}\NormalTok{olc\_id)}
\FunctionTok{summary}\NormalTok{(sel.pnts)}
\NormalTok{ngs.m.pnts }\OtherTok{=}\NormalTok{ ngs.m[}\FunctionTok{which}\NormalTok{(sel.pnts), }\FunctionTok{c}\NormalTok{(}\StringTok{"olc\_id"}\NormalTok{, }\StringTok{"longitude"}\NormalTok{, }\StringTok{"latitude"}\NormalTok{)]}
\FunctionTok{coordinates}\NormalTok{(ngs.m.pnts) }\OtherTok{\textless{}{-}} \ErrorTok{\textasciitilde{}}\NormalTok{ longitude }\SpecialCharTok{+}\NormalTok{ latitude}
\FunctionTok{proj4string}\NormalTok{(ngs.m.pnts) }\OtherTok{\textless{}{-}} \StringTok{"+init=epsg:4326"}
\NormalTok{ngs.xy }\OtherTok{=} \FunctionTok{spTransform}\NormalTok{(ngs.m.pnts, }\FunctionTok{crs}\NormalTok{(mask))}
\NormalTok{tif.lst }\OtherTok{=} \FunctionTok{list.files}\NormalTok{(}\StringTok{"./1km"}\NormalTok{, }\FunctionTok{glob2rx}\NormalTok{(}\StringTok{"*.tif$"}\NormalTok{), }\AttributeTok{full.names =} \ConstantTok{TRUE}\NormalTok{)}
\DocumentationTok{\#\# 262 layers}
\NormalTok{ov.tmp }\OtherTok{=}\NormalTok{ parallel}\SpecialCharTok{::}\FunctionTok{mclapply}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(tif.lst), }\ControlFlowTok{function}\NormalTok{(j)\{ terra}\SpecialCharTok{::}\FunctionTok{extract}\NormalTok{(terra}\SpecialCharTok{::}\FunctionTok{rast}\NormalTok{(tif.lst[j]), terra}\SpecialCharTok{::}\FunctionTok{vect}\NormalTok{(ngs.xy)) \}, }\AttributeTok{mc.cores =} \DecValTok{80}\NormalTok{)}
\NormalTok{ov.tmp }\OtherTok{=}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{bind\_cols}\NormalTok{(}\FunctionTok{lapply}\NormalTok{(ov.tmp, }\ControlFlowTok{function}\NormalTok{(i)\{i[,}\DecValTok{2}\NormalTok{]\}))}
\FunctionTok{names}\NormalTok{(ov.tmp) }\OtherTok{=}\NormalTok{ tools}\SpecialCharTok{::}\FunctionTok{file\_path\_sans\_ext}\NormalTok{(}\FunctionTok{basename}\NormalTok{(tif.lst))}
\NormalTok{ov.tmp}\SpecialCharTok{$}\NormalTok{olc\_id }\OtherTok{=}\NormalTok{ ngs.xy}\SpecialCharTok{$}\NormalTok{olc\_id}
\NormalTok{reg.matrix }\OtherTok{=}\NormalTok{ plyr}\SpecialCharTok{::}\FunctionTok{join}\NormalTok{(ngs.m, ov.tmp)}
\FunctionTok{saveRDS.gz}\NormalTok{(reg.matrix, }\StringTok{"./input/ds801\_geochem1km.rds"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We can load the regression matrix which contains all target variables and covariates:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ds801 }\OtherTok{=} \FunctionTok{readRDS}\NormalTok{(}\StringTok{"./input/ds801\_geochem1km.rds"}\NormalTok{)}
\FunctionTok{dim}\NormalTok{(ds801)}
\CommentTok{\#\textgreater{} [1] 14275   407}
\end{Highlighting}
\end{Shaded}

which has a total of 4818 unique locations:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str}\NormalTok{(}\FunctionTok{levels}\NormalTok{(}\FunctionTok{as.factor}\NormalTok{(ds801}\SpecialCharTok{$}\NormalTok{olc\_id)))}
\CommentTok{\#\textgreater{}  chr [1:4818] "75WXFVQG+WH6" "75WXJHVG+62X" "75WXJVM9+995" "75WXRG2G+5PV" ...}
\end{Highlighting}
\end{Shaded}

The individual records can be browsed directly via \url{https://mrdata.usgs.gov/ngdb/soil/},
for example a single record includes:

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{./img/Fig_geochemical_USGS_record} 

}

\caption{Example of a geochemical sample with observations and measurements and coordiates of site.}\label{fig:ds801-example}
\end{figure}

Covariates prepared to help interpolation of the geochemicals include:

\begin{itemize}
\tightlist
\item
  Distance to cities \citep{nelson2019suite};\\
\item
  \href{https://doi.org/10.5281/zenodo.1420114}{MODIS LST} (monthly daytime and nighttime);\\
\item
  \href{https://lpdaac.usgs.gov/products/mod13q1v006/}{MODIS EVI} (long-term monthly values);\\
\item
  \href{https://eogdata.mines.edu/products/dmsp/}{Lights at night images};\\
\item
  \href{https://climate.esa.int/en/odp/\#/project/snow}{Snow occurrence probability};\\
\item
  Soil property and class maps for USA48 \citep{ramcharan2018soil};\\
\item
  Terrain / hydrological indices;
\end{itemize}

We can focus on predict concentration of lead (Pb). Pb seems to change
with depth and this is seems to be consistent for different land cover classes:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{openair}\SpecialCharTok{::}\FunctionTok{scatterPlot}\NormalTok{(ds801[ds801}\SpecialCharTok{$}\NormalTok{pb\_ppm}\SpecialCharTok{\textless{}}\DecValTok{140}\NormalTok{,], }\AttributeTok{x =} \StringTok{"hzn\_depth"}\NormalTok{, }\AttributeTok{y =} \StringTok{"pb\_ppm"}\NormalTok{, }\AttributeTok{method =} \StringTok{"hexbin"}\NormalTok{, }\AttributeTok{col =} \StringTok{"increment"}\NormalTok{, }\AttributeTok{log.x=}\ConstantTok{TRUE}\NormalTok{, }\AttributeTok{log.y=}\ConstantTok{TRUE}\NormalTok{, }\AttributeTok{xlab=}\StringTok{"Depth"}\NormalTok{, }\AttributeTok{ylab=}\StringTok{"Pb [ppm]"}\NormalTok{, }\AttributeTok{z.lim=}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{100}\NormalTok{), }\AttributeTok{type=}\StringTok{"landcover1"}\NormalTok{)}
\CommentTok{\#\textgreater{} Warning: removing 11 missing rows due to landcover1}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{spatial-3D_files/figure-latex/cor-depth-1} 

}

\caption{Distribution of Pb as a function of land cover classes.}\label{fig:cor-depth}
\end{figure}

Because we are aiming at producing predictions of geochemical elements for different
depths in soil, we can also use depth of the soil sample as one of the covariates.
This makes the prediction system 3D and the model can thus be used to predict at
any new 3D location (\(X, Y, d\)). To fit a RF model for this data we can use:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ds801}\SpecialCharTok{$}\NormalTok{log.pb }\OtherTok{=} \FunctionTok{log1p}\NormalTok{(ds801}\SpecialCharTok{$}\NormalTok{pb\_ppm)}
\NormalTok{pr.vars }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\FunctionTok{readRDS}\NormalTok{(}\StringTok{"./input/pb.pr.vars.rds"}\NormalTok{), }\StringTok{"hzn\_depth"}\NormalTok{)}
\NormalTok{sel.pb }\OtherTok{=} \FunctionTok{complete.cases}\NormalTok{(ds801[,}\FunctionTok{c}\NormalTok{(}\StringTok{"log.pb"}\NormalTok{, pr.vars)])}
\NormalTok{mrf }\OtherTok{=}\NormalTok{ ranger}\SpecialCharTok{::}\FunctionTok{ranger}\NormalTok{(}\AttributeTok{y=}\NormalTok{ds801}\SpecialCharTok{$}\NormalTok{log.pb[sel.pb], }\AttributeTok{x=}\NormalTok{ds801[sel.pb, pr.vars], }
            \AttributeTok{num.trees =} \DecValTok{85}\NormalTok{, }\AttributeTok{importance =} \StringTok{\textquotesingle{}impurity\textquotesingle{}}\NormalTok{)}
\NormalTok{mrf}
\CommentTok{\#\textgreater{} Ranger result}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{}  ranger::ranger(y = ds801$log.pb[sel.pb], x = ds801[sel.pb, pr.vars],      num.trees = 85, importance = "impurity") }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Type:                             Regression }
\CommentTok{\#\textgreater{} Number of trees:                  85 }
\CommentTok{\#\textgreater{} Sample size:                      14264 }
\CommentTok{\#\textgreater{} Number of independent variables:  188 }
\CommentTok{\#\textgreater{} Mtry:                             13 }
\CommentTok{\#\textgreater{} Target node size:                 5 }
\CommentTok{\#\textgreater{} Variable importance mode:         impurity }
\CommentTok{\#\textgreater{} Splitrule:                        variance }
\CommentTok{\#\textgreater{} OOB prediction error (MSE):       0.09636483 }
\CommentTok{\#\textgreater{} R squared (OOB):                  0.670695}
\end{Highlighting}
\end{Shaded}

which results in R-square of about 0.67. Because many training samples have exactly
the same coordinates (same site, three depths), we assume that this model is
over-fitting i.e.~that the out-of-bag accuracy is \href{https://opengeohub.github.io/spatial-sampling-ml/}{probably over-optimistic}.
Instead we can fit an Ensemble model where we block points within 30 by 30-km blocks:

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{if}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{exists}\NormalTok{(}\StringTok{"eml.pb"}\NormalTok{))\{}
\NormalTok{  lrn.rf }\OtherTok{=}\NormalTok{ mlr}\SpecialCharTok{::}\FunctionTok{makeLearner}\NormalTok{(}\StringTok{"regr.ranger"}\NormalTok{, }\AttributeTok{num.trees=}\DecValTok{85}\NormalTok{, }\AttributeTok{importance=}\StringTok{"impurity"}\NormalTok{,}
                            \AttributeTok{num.threads =}\NormalTok{ parallel}\SpecialCharTok{::}\FunctionTok{detectCores}\NormalTok{())}
\NormalTok{  lrns.pb }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(lrn.rf, mlr}\SpecialCharTok{::}\FunctionTok{makeLearner}\NormalTok{(}\StringTok{"regr.xgboost"}\NormalTok{), mlr}\SpecialCharTok{::}\FunctionTok{makeLearner}\NormalTok{(}\StringTok{"regr.cvglmnet"}\NormalTok{))}
\NormalTok{  tsk0.pb }\OtherTok{\textless{}{-}}\NormalTok{ mlr}\SpecialCharTok{::}\FunctionTok{makeRegrTask}\NormalTok{(}\AttributeTok{data =}\NormalTok{ ds801[sel.pb, }\FunctionTok{c}\NormalTok{(}\StringTok{"log.pb"}\NormalTok{, pr.vars)], }
                               \AttributeTok{target =} \StringTok{"log.pb"}\NormalTok{, }\AttributeTok{blocking =} \FunctionTok{as.factor}\NormalTok{(ds801}\SpecialCharTok{$}\NormalTok{ID[sel.pb]))}
\NormalTok{  init.pb }\OtherTok{\textless{}{-}}\NormalTok{ mlr}\SpecialCharTok{::}\FunctionTok{makeStackedLearner}\NormalTok{(lrns.pb, }\AttributeTok{method=}\StringTok{"stack.cv"}\NormalTok{, }\AttributeTok{super.learner=}\StringTok{"regr.lm"}\NormalTok{, }
                                      \AttributeTok{resampling=}\NormalTok{mlr}\SpecialCharTok{::}\FunctionTok{makeResampleDesc}\NormalTok{(}\AttributeTok{method=}\StringTok{"CV"}\NormalTok{, }\AttributeTok{blocking.cv=}\ConstantTok{TRUE}\NormalTok{))}
\NormalTok{  parallelMap}\SpecialCharTok{::}\FunctionTok{parallelStartSocket}\NormalTok{(parallel}\SpecialCharTok{::}\FunctionTok{detectCores}\NormalTok{())}
\NormalTok{  eml.pb }\OtherTok{=} \FunctionTok{train}\NormalTok{(init.pb, tsk0.pb)}
\NormalTok{  parallelMap}\SpecialCharTok{::}\FunctionTok{parallelStop}\NormalTok{()}
\NormalTok{\}}
\CommentTok{\#\textgreater{} [18:37:08] }\AlertTok{WARNING}\CommentTok{: amalgamation/../src/objective/regression\_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(eml.pb}\SpecialCharTok{$}\NormalTok{learner.model}\SpecialCharTok{$}\NormalTok{super.model}\SpecialCharTok{$}\NormalTok{learner.model)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} stats::lm(formula = f, data = d)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residuals:}
\CommentTok{\#\textgreater{}     Min      1Q  Median      3Q     Max }
\CommentTok{\#\textgreater{} {-}2.3979 {-}0.1990 {-}0.0117  0.1699  6.2948 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}               Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept)   {-}0.64745    0.04296 {-}15.069  \textless{} 2e{-}16 ***}
\CommentTok{\#\textgreater{} regr.ranger    0.85552    0.02152  39.755  \textless{} 2e{-}16 ***}
\CommentTok{\#\textgreater{} regr.xgboost   0.26550    0.05337   4.974 6.62e{-}07 ***}
\CommentTok{\#\textgreater{} regr.cvglmnet  0.25631    0.02024  12.665  \textless{} 2e{-}16 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residual standard error: 0.4075 on 14260 degrees of freedom}
\CommentTok{\#\textgreater{} Multiple R{-}squared:  0.4328, Adjusted R{-}squared:  0.4327 }
\CommentTok{\#\textgreater{} F{-}statistic:  3627 on 3 and 14260 DF,  p{-}value: \textless{} 2.2e{-}16}
\end{Highlighting}
\end{Shaded}

which shows somewhat lower R-square of 0.44, this time that whole sites have
been taken out and hence this seems to be somewhat more realistic estimate of the
mapping accuracy \citep{roberts2017cross}. The accuracy plot shows that the model has
some problems with predicting higher values, but overall matches the observed values:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{t.pb }\OtherTok{=} \FunctionTok{quantile}\NormalTok{(ds801}\SpecialCharTok{$}\NormalTok{log.pb, }\FunctionTok{c}\NormalTok{(}\FloatTok{0.001}\NormalTok{, }\FloatTok{0.01}\NormalTok{, }\FloatTok{0.999}\NormalTok{), }\AttributeTok{na.rm=}\ConstantTok{TRUE}\NormalTok{)}
\FunctionTok{plot\_hexbin}\NormalTok{(}\AttributeTok{varn=}\StringTok{"log.pb"}\NormalTok{, }\AttributeTok{breaks=}\FunctionTok{c}\NormalTok{(t.pb[}\DecValTok{1}\NormalTok{], }\FunctionTok{seq}\NormalTok{(t.pb[}\DecValTok{2}\NormalTok{], t.pb[}\DecValTok{3}\NormalTok{], }\AttributeTok{length=}\DecValTok{25}\NormalTok{)), }
      \AttributeTok{meas=}\NormalTok{eml.pb}\SpecialCharTok{$}\NormalTok{learner.model}\SpecialCharTok{$}\NormalTok{super.model}\SpecialCharTok{$}\NormalTok{learner.model}\SpecialCharTok{$}\NormalTok{model}\SpecialCharTok{$}\NormalTok{log.pb, }
      \AttributeTok{pred=}\NormalTok{eml.pb}\SpecialCharTok{$}\NormalTok{learner.model}\SpecialCharTok{$}\NormalTok{super.model}\SpecialCharTok{$}\NormalTok{learner.model}\SpecialCharTok{$}\NormalTok{fitted.values,}
      \AttributeTok{main=}\StringTok{"Pb [EML]"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{./img/plot_CV_log.pb} 

}

\caption{Accuracy plot for Pb concentration in soil fitted using Ensemble ML.}\label{fig:ac-pb1}
\end{figure}

Variables most important for explaining distribution of the target variable (based on the variable importance)
seem to be soil depth (\texttt{hnz\_depth}), soil type maps, annual day time temperature and travel time to cities:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggplot2)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Attaching package: \textquotesingle{}ggplot2\textquotesingle{}}
\CommentTok{\#\textgreater{} The following object is masked from \textquotesingle{}package:latticeExtra\textquotesingle{}:}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}     layer}
\NormalTok{xl.pb }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(mlr}\SpecialCharTok{::}\FunctionTok{getFeatureImportance}\NormalTok{(eml.pb[[}\StringTok{"learner.model"}\NormalTok{]][[}\StringTok{"base.models"}\NormalTok{]][[}\DecValTok{1}\NormalTok{]])}\SpecialCharTok{$}\NormalTok{res)}
\NormalTok{xl.pb}\SpecialCharTok{$}\NormalTok{relative\_importance }\OtherTok{=} \DecValTok{100}\SpecialCharTok{*}\NormalTok{xl.pb}\SpecialCharTok{$}\NormalTok{importance}\SpecialCharTok{/}\FunctionTok{sum}\NormalTok{(xl.pb}\SpecialCharTok{$}\NormalTok{importance)}
\NormalTok{xl.pb }\OtherTok{=}\NormalTok{ xl.pb[}\FunctionTok{order}\NormalTok{(xl.pb}\SpecialCharTok{$}\NormalTok{relative\_importance, }\AttributeTok{decreasing =}\NormalTok{ T),]}
\NormalTok{xl.pb}\SpecialCharTok{$}\NormalTok{variable }\OtherTok{=} \FunctionTok{paste0}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\FunctionTok{nrow}\NormalTok{(xl.pb)), }\StringTok{". "}\NormalTok{, xl.pb}\SpecialCharTok{$}\NormalTok{variable)}
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ xl.pb[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{20}\NormalTok{,], }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =} \FunctionTok{reorder}\NormalTok{(variable, relative\_importance), }\AttributeTok{y =}\NormalTok{ relative\_importance)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{fill =} \StringTok{"steelblue"}\NormalTok{,}
           \AttributeTok{stat =} \StringTok{"identity"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{coord\_flip}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Variable importance"}\NormalTok{,}
       \AttributeTok{x =} \ConstantTok{NULL}\NormalTok{,}
       \AttributeTok{y =} \ConstantTok{NULL}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{theme}\NormalTok{(}\AttributeTok{text =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size=}\DecValTok{15}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{spatial-3D_files/figure-latex/varimp-pb-1} 

}

\caption{Variable importance for 3D prediction model for Pb concentrations.}\label{fig:varimp-pb}
\end{figure}

If we plot the travel time to cities vs Pb concentrations, we can clearly see that
Pb is negatively correlated with travel time to cities (following a log-log linear relationship):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{openair}\SpecialCharTok{::}\FunctionTok{scatterPlot}\NormalTok{(ds801[ds801}\SpecialCharTok{$}\NormalTok{pb\_ppm}\SpecialCharTok{\textless{}}\DecValTok{140}\NormalTok{,], }\AttributeTok{x =} \StringTok{"travel\_time\_to\_cities\_1\_usa48"}\NormalTok{, }\AttributeTok{y =} \StringTok{"pb\_ppm"}\NormalTok{, }\AttributeTok{method =} \StringTok{"hexbin"}\NormalTok{, }\AttributeTok{col =} \StringTok{"increment"}\NormalTok{, }\AttributeTok{log.x=}\ConstantTok{TRUE}\NormalTok{, }\AttributeTok{log.y=}\ConstantTok{TRUE}\NormalTok{, }\AttributeTok{xlab=}\StringTok{"Travel time to cities 1"}\NormalTok{, }\AttributeTok{ylab=}\StringTok{"Pb [ppm]"}\NormalTok{, }\AttributeTok{type=}\StringTok{"hzn\_depth"}\NormalTok{)}
\CommentTok{\#\textgreater{} Warning: removing 11 missing rows due to hzn\_depth}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{spatial-3D_files/figure-latex/cor-cities-1} 

}

\caption{Distribution of Pb as a function of travel time to cities for different depths.}\label{fig:cor-cities}
\end{figure}

\hypertarget{predictions-in-3d}{%
\section{Predictions in 3D}\label{predictions-in-3d}}

To produce predictions we can focus on area around Chicago conglomeration. We can
load the covariate layers by using:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{g1km }\OtherTok{=} \FunctionTok{readRDS}\NormalTok{(}\StringTok{"./input/chicago\_grid1km.rds"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

This contains all layers we used for training. We can generate predictions by
adding a depth column, then write predictions to GeoTIFFs:

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{for}\NormalTok{(k }\ControlFlowTok{in} \FunctionTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{30}\NormalTok{, }\DecValTok{60}\NormalTok{))\{}
\NormalTok{  out.tif }\OtherTok{=} \FunctionTok{paste0}\NormalTok{(}\StringTok{"./output/pb\_ppm\_"}\NormalTok{, k, }\StringTok{"cm\_1km.tif"}\NormalTok{)}
  \ControlFlowTok{if}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{file.exists}\NormalTok{(out.tif))\{}
\NormalTok{    g1km}\SpecialCharTok{$}\NormalTok{hzn\_depth }\OtherTok{=}\NormalTok{ k}
\NormalTok{    sel.na }\OtherTok{=} \FunctionTok{complete.cases}\NormalTok{(g1km)}
\NormalTok{    newdata }\OtherTok{=}\NormalTok{ g1km[sel.na, eml.pb}\SpecialCharTok{$}\NormalTok{features]}
\NormalTok{    pred }\OtherTok{=} \FunctionTok{predict}\NormalTok{(eml.pb, }\AttributeTok{newdata=}\NormalTok{newdata)}
\NormalTok{    g1km.sp }\OtherTok{=} \FunctionTok{SpatialPixelsDataFrame}\NormalTok{(}\FunctionTok{as.matrix}\NormalTok{(g1km[sel.na,}\FunctionTok{c}\NormalTok{(}\StringTok{"x"}\NormalTok{,}\StringTok{"y"}\NormalTok{)]), }
                \AttributeTok{data=}\NormalTok{pred}\SpecialCharTok{$}\NormalTok{data, }\AttributeTok{proj4string=}\FunctionTok{CRS}\NormalTok{(}\StringTok{"EPSG:5070"}\NormalTok{))}
\NormalTok{    g1km.sp}\SpecialCharTok{$}\NormalTok{pred }\OtherTok{=} \FunctionTok{expm1}\NormalTok{(g1km.sp}\SpecialCharTok{$}\NormalTok{response)}
\NormalTok{    rgdal}\SpecialCharTok{::}\FunctionTok{writeGDAL}\NormalTok{(g1km.sp[}\StringTok{"pred"}\NormalTok{], out.tif, }\AttributeTok{type=}\StringTok{"Int16"}\NormalTok{, }\AttributeTok{mvFlag=}\SpecialCharTok{{-}}\DecValTok{32768}\NormalTok{, }\AttributeTok{options=}\FunctionTok{c}\NormalTok{(}\StringTok{"COMPRESS=DEFLATE"}\NormalTok{))}
    \CommentTok{\#gc()}
\NormalTok{  \}}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

This finally gives the following pattern:

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{./img/Pb_predictions_d1} \includegraphics[width=0.9\linewidth]{./img/Pb_predictions_d2} \includegraphics[width=0.9\linewidth]{./img/Pb_predictions_d3} 

}

\caption{Predictions of Pb concentration for different soil depths based on Ensemble ML. Points indicate training points used to build the predictive mapping model. Red color indicates high values. Values of Pb clearly drop with soil depth.}\label{fig:pred-pb1}
\end{figure}

Based on these results, it can be said that in general:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Distribution of Pb across USA seem to be controlled by a mixture of factors
  including climatic factors, soil-terrain units and anthropogenic factors (travel distance to cities);\\
\item
  Overall big urban areas show significantly higher concentrations for some heavy
  metals and this relationship if log-log linear;\\
\item
  Soil depth for some geochemical elements comes as the overall most important
  covariate hence mapping soil variables in 3D is fully justified;
\end{enumerate}

\hypertarget{advantages-and-limitations-of-running-3d-predictive-mapping}{%
\section{Advantages and limitations of running 3D predictive mapping}\label{advantages-and-limitations-of-running-3d-predictive-mapping}}

In summary 3D soil mapping is relatively straight forward to implement especially
for mapping soil variables from soil profiles (multiple samples per soil layer).
Before modeling the target variable with depth, it is a good idea to plot
relationship between target variable and depth under different settings
(as in Fig. \ref{fig:cor-depth}). 3D soil mapping based on Machine Learning is
now increasingly common \citep{hengl2019predictive, sothe2022large}.

A limitation for 3D predictive mapping is the size of data i.e.~data volumes
increasing proportionally to number of slices we need to predict (in this case three).
Also, we show that points with exactly the same coordinates might result in e.g.~
Random Forest overfitting emphasizing some covariate layers that are possibly
less important, hence it is again important to use the blocking parameter that
separates training and validation points.

Soil depth is for many soil variables most important explanatory variables, but
in the cases it does not correlate with the target variable, there is probably
also no need for 3D soil mapping. In the case depth is not significantly
correlated, one could simply first aggregate all values to fixed block depth and
convert modeling from 3D to 2D.

\hypertarget{spatiotemporal-interpolation-using-ensemble-ml}{%
\chapter{Spatiotemporal interpolation using Ensemble ML}\label{spatiotemporal-interpolation-using-ensemble-ml}}

You are reading the work-in-progress Spatial and spatiotemporal interpolation using Ensemble Machine Learning. This chapter is currently draft version, a peer-review publication is pending. You can find the polished first edition at \url{https://opengeohub.github.io/spatial-prediction-eml/}.

\hypertarget{spatiotemporal-interpolation-of-daily-temperatures}{%
\section{Spatiotemporal interpolation of daily temperatures}\label{spatiotemporal-interpolation-of-daily-temperatures}}

In previous examples we have demonstrated effects of over-fitting and
how Ensemble ML helps decrease overfitting and extrapolation problems
using synthetic data. We can now look at some real-life cases for example
the daily temperatures measured for several years for Croatia described in \citet{hengl2012spatio}.
This data sets consist of two parts: (1) measurements of daily temperatures at
meteo stations, (2) list of gridded covariates (Fig. \ref{fig:croatia-meteo}).

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{./img/Fig_stations_meteo_croatia} 

}

\caption{Temporal dynamics of mean-daily temperatures at sample meteorological stations. This shows seasonality effects (smoothed line) and daily oscillations.}\label{fig:croatia-meteo}
\end{figure}

We can load the point data by using:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(rgdal)}
\NormalTok{hrmeteo }\OtherTok{=} \FunctionTok{readRDS}\NormalTok{(}\StringTok{"input/hrtemp2006\_meteo.rds"}\NormalTok{)}
\FunctionTok{str}\NormalTok{(hrmeteo)}
\CommentTok{\#\textgreater{} List of 2}
\CommentTok{\#\textgreater{}  $ meteo   :\textquotesingle{}data.frame\textquotesingle{}:    44895 obs. of  5 variables:}
\CommentTok{\#\textgreater{}   ..$ IDSTA : chr [1:44895] "GL001" "GL001" "GL001" "GL001" ...}
\CommentTok{\#\textgreater{}   ..$ DATE  : chr [1:44895] "2006{-}1{-}1" "2006{-}1{-}2" "2006{-}1{-}3" "2006{-}1{-}4" ...}
\CommentTok{\#\textgreater{}   ..$ MDTEMP: num [1:44895] 1.6 0.7 1.5 0.3 {-}0.1 1 0.3 {-}1.9 {-}5.4 {-}3.6 ...}
\CommentTok{\#\textgreater{}   ..$ cday  : num [1:44895] 13148 13149 13150 13151 13152 ...}
\CommentTok{\#\textgreater{}   .. ..{-} attr(*, "tzone")= chr ""}
\CommentTok{\#\textgreater{}   ..$ x     : num [1:44895] NA NA NA NA NA NA NA NA NA NA ...}
\CommentTok{\#\textgreater{}  $ stations:\textquotesingle{}data.frame\textquotesingle{}:    152 obs. of  3 variables:}
\CommentTok{\#\textgreater{}   ..$ IDSTA: chr [1:152] "GL001" "GL002" "GL003" "GL004" ...}
\CommentTok{\#\textgreater{}   ..$ X    : num [1:152] 670760 643073 673778 752344 767729 ...}
\CommentTok{\#\textgreater{}   ..$ Y    : num [1:152] 5083464 5086417 5052001 4726567 4717878 ...}
\NormalTok{idsta.pnts }\OtherTok{=}\NormalTok{ hrmeteo}\SpecialCharTok{$}\NormalTok{stations}
\FunctionTok{coordinates}\NormalTok{(idsta.pnts) }\OtherTok{=} \ErrorTok{\textasciitilde{}}\NormalTok{ X }\SpecialCharTok{+}\NormalTok{ Y}
\end{Highlighting}
\end{Shaded}

This is a typical format for spatiotemporal meteorological data with locations
of stations in one table, and measurements of daily temperatures (\texttt{MDTEMP}) in
other. The column \texttt{cday} here is the cumulative day since 1970, which allows us
to present time on a linear scale i.e.~by using a numeric value instead of dates.

The gridded data includes: (a) static covariates (relief data), and
(b) dynamic time-series data (MODIS LST). To load the static covariates we use:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{hrgrid1km }\OtherTok{=} \FunctionTok{readRDS}\NormalTok{(}\StringTok{"input/hrgrid1km.rds"}\NormalTok{)}
\CommentTok{\#plot(hrgrid1km[1])}
\FunctionTok{proj4string}\NormalTok{(idsta.pnts) }\OtherTok{=} \FunctionTok{proj4string}\NormalTok{(hrgrid1km)}
\FunctionTok{str}\NormalTok{(hrgrid1km}\SpecialCharTok{@}\NormalTok{data)}
\CommentTok{\#\textgreater{} \textquotesingle{}data.frame\textquotesingle{}:    238630 obs. of  4 variables:}
\CommentTok{\#\textgreater{}  $ HRdem : int  1599 1426 1440 1764 1917 1912 1707 1550 1518 1516 ...}
\CommentTok{\#\textgreater{}  $ HRdsea: num  93 89.6 89.8 93.6 95 ...}
\CommentTok{\#\textgreater{}  $ Lat   : num  46.5 46.5 46.5 46.5 46.5 ...}
\CommentTok{\#\textgreater{}  $ Lon   : num  13.2 13.2 13.2 13.2 13.2 ...}
\end{Highlighting}
\end{Shaded}

The dynamic time-series data is stored in a local folder (\texttt{input/LST2006HR}) as
individual files that we can list by using:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{LST.listday }\OtherTok{\textless{}{-}} \FunctionTok{dir}\NormalTok{(}\StringTok{"input/LST2006HR"}\NormalTok{, }\AttributeTok{pattern=}\FunctionTok{glob2rx}\NormalTok{(}\StringTok{"LST2006\_**\_**.LST\_Day\_1km.tif"}\NormalTok{), }\AttributeTok{full.names =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{LST.listnight }\OtherTok{\textless{}{-}} \FunctionTok{dir}\NormalTok{(}\StringTok{"input/LST2006HR"}\NormalTok{, }\AttributeTok{pattern=}\FunctionTok{glob2rx}\NormalTok{(}\StringTok{"LST2006\_**\_**.LST\_Night\_1km.tif"}\NormalTok{), }\AttributeTok{full.names =} \ConstantTok{TRUE}\NormalTok{)}
\FunctionTok{str}\NormalTok{(LST.listday)}
\CommentTok{\#\textgreater{}  chr [1:46] "input/LST2006HR/LST2006\_01\_01.LST\_Day\_1km.tif" ...}
\end{Highlighting}
\end{Shaded}

Here we see there are 46 images for year 2006 with daytime and 46 images for
night time estimates of LST. We do not want to load all these rasters to R
because we might experience RAM problems. We can first overlay points and see
which variables can help with mapping daily temperatures.

For the static covariates we only have to run the overlay once:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{idsta.ov }\OtherTok{\textless{}{-}}\NormalTok{ sp}\SpecialCharTok{::}\FunctionTok{over}\NormalTok{(idsta.pnts, hrgrid1km)}
\NormalTok{idsta.ov}\SpecialCharTok{$}\NormalTok{IDSTA }\OtherTok{=}\NormalTok{ idsta.pnts}\SpecialCharTok{$}\NormalTok{IDSTA}
\FunctionTok{str}\NormalTok{(idsta.ov)}
\CommentTok{\#\textgreater{} \textquotesingle{}data.frame\textquotesingle{}:    152 obs. of  5 variables:}
\CommentTok{\#\textgreater{}  $ HRdem : int  161 134 202 31 205 563 80 96 116 228 ...}
\CommentTok{\#\textgreater{}  $ HRdsea: num  198.5 181.7 192.9 0 1.5 ...}
\CommentTok{\#\textgreater{}  $ Lat   : num  45.9 45.9 45.6 42.7 42.6 ...}
\CommentTok{\#\textgreater{}  $ Lon   : num  17.2 16.8 17.2 18.1 18.3 ...}
\CommentTok{\#\textgreater{}  $ IDSTA : chr  "GL001" "GL002" "GL003" "GL004" ...}
\end{Highlighting}
\end{Shaded}

For the spatiotemporal data (MODIS LST time-series) we need to run overlay as in
a spacetime cube. This means that we need to match points using \texttt{x,y,t}
coordinates with grids covering the same \texttt{x,y,t} fields. To speed up spacetime overlay
we use our custom function \texttt{extract\_st}, which basically builds on top of the
\texttt{terra} package. First, we need to define begin, end times for each GeoTIFF:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(terra)}
\FunctionTok{source}\NormalTok{(}\StringTok{"mlst\_functions.R"}\NormalTok{)}
\NormalTok{hrmeteo}\SpecialCharTok{$}\NormalTok{meteo}\SpecialCharTok{$}\NormalTok{x }\OtherTok{=}\NormalTok{ plyr}\SpecialCharTok{::}\FunctionTok{join}\NormalTok{(hrmeteo}\SpecialCharTok{$}\NormalTok{meteo, hrmeteo}\SpecialCharTok{$}\NormalTok{stations, }\AttributeTok{by=}\StringTok{"IDSTA"}\NormalTok{)}\SpecialCharTok{$}\NormalTok{X}
\NormalTok{hrmeteo}\SpecialCharTok{$}\NormalTok{meteo}\SpecialCharTok{$}\NormalTok{y }\OtherTok{=}\NormalTok{ plyr}\SpecialCharTok{::}\FunctionTok{join}\NormalTok{(hrmeteo}\SpecialCharTok{$}\NormalTok{meteo, hrmeteo}\SpecialCharTok{$}\NormalTok{stations, }\AttributeTok{by=}\StringTok{"IDSTA"}\NormalTok{)}\SpecialCharTok{$}\NormalTok{Y}
\DocumentationTok{\#\# generate row ID:}
\NormalTok{hrmeteo}\SpecialCharTok{$}\NormalTok{meteo}\SpecialCharTok{$}\NormalTok{row.id }\OtherTok{=} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{nrow}\NormalTok{(hrmeteo}\SpecialCharTok{$}\NormalTok{meteo)}
\NormalTok{hrmeteo}\SpecialCharTok{$}\NormalTok{meteo}\SpecialCharTok{$}\NormalTok{Date }\OtherTok{=} \FunctionTok{as.Date}\NormalTok{(hrmeteo}\SpecialCharTok{$}\NormalTok{meteo}\SpecialCharTok{$}\NormalTok{DATE, }\AttributeTok{format =} \StringTok{"\%Y{-}\%m{-}\%d"}\NormalTok{)}
\DocumentationTok{\#\# strip dates from filename:}
\NormalTok{begin.tif1.lst }\OtherTok{=} \FunctionTok{as.Date}\NormalTok{(}\FunctionTok{paste0}\NormalTok{(}\StringTok{"2006{-}"}\NormalTok{, }\FunctionTok{substr}\NormalTok{(}\FunctionTok{basename}\NormalTok{(LST.listday), }\DecValTok{9}\NormalTok{, }\DecValTok{10}\NormalTok{), }
                                \StringTok{"{-}"}\NormalTok{, }\FunctionTok{substr}\NormalTok{(}\FunctionTok{basename}\NormalTok{(LST.listday), }\DecValTok{12}\NormalTok{, }\DecValTok{13}\NormalTok{)))}\SpecialCharTok{{-}}\DecValTok{4}
\NormalTok{end.tif1.lst }\OtherTok{=} \FunctionTok{as.Date}\NormalTok{(}\FunctionTok{paste0}\NormalTok{(}\StringTok{"2006{-}"}\NormalTok{, }\FunctionTok{substr}\NormalTok{(}\FunctionTok{basename}\NormalTok{(LST.listday), }\DecValTok{9}\NormalTok{, }\DecValTok{10}\NormalTok{), }
                              \StringTok{"{-}"}\NormalTok{, }\FunctionTok{substr}\NormalTok{(}\FunctionTok{basename}\NormalTok{(LST.listday), }\DecValTok{12}\NormalTok{, }\DecValTok{13}\NormalTok{)))}\SpecialCharTok{+}\DecValTok{4}
\end{Highlighting}
\end{Shaded}

now that we know spacetime coordinates for both points and grids, we can run
overlay in parallel to speed up computing:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mc.cores }\OtherTok{=}\NormalTok{ parallel}\SpecialCharTok{::}\FunctionTok{detectCores}\NormalTok{()}
\NormalTok{ov.pnts }\OtherTok{\textless{}{-}}\NormalTok{ parallel}\SpecialCharTok{::}\FunctionTok{mclapply}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(LST.listday), }\ControlFlowTok{function}\NormalTok{(i)\{ }
  \FunctionTok{extract\_st}\NormalTok{(}\AttributeTok{tif=}\NormalTok{LST.listday[i], hrmeteo}\SpecialCharTok{$}\NormalTok{meteo, }\AttributeTok{date=}\StringTok{"Date"}\NormalTok{, }
             \AttributeTok{crs =} \FunctionTok{proj4string}\NormalTok{(hrgrid1km),        }
             \AttributeTok{date.tif.begin=}\NormalTok{begin.tif1.lst[i], }
             \AttributeTok{date.tif.end=}\NormalTok{end.tif1.lst[i], }
             \AttributeTok{coords=}\FunctionTok{c}\NormalTok{(}\StringTok{"x"}\NormalTok{,}\StringTok{"y"}\NormalTok{), }\AttributeTok{variable.name=}\StringTok{"LST.day"}\NormalTok{) \}, }
  \AttributeTok{mc.cores=}\NormalTok{mc.cores)}
\NormalTok{ov.pnts }\OtherTok{=}\NormalTok{ ov.pnts[}\SpecialCharTok{!}\FunctionTok{sapply}\NormalTok{(ov.pnts, is.null)]}
\NormalTok{ov.tifs1 }\OtherTok{=}\NormalTok{ plyr}\SpecialCharTok{::}\FunctionTok{join\_all}\NormalTok{(ov.pnts, }\AttributeTok{by=}\StringTok{"row.id"}\NormalTok{, }\AttributeTok{type=}\StringTok{"full"}\NormalTok{)}
\FunctionTok{str}\NormalTok{(ov.tifs1)}
\CommentTok{\#\textgreater{} \textquotesingle{}data.frame\textquotesingle{}:    44895 obs. of  2 variables:}
\CommentTok{\#\textgreater{}  $ LST.day: num  0 0 0 0 0 0 0 0 0 0 ...}
\CommentTok{\#\textgreater{}  $ row.id : int  1 2 3 4 5 366 367 368 369 370 ...}
\NormalTok{ov.tifs1}\SpecialCharTok{$}\NormalTok{LST.day }\OtherTok{=} \FunctionTok{ifelse}\NormalTok{(ov.tifs1}\SpecialCharTok{$}\NormalTok{LST.day }\SpecialCharTok{==} \DecValTok{0}\NormalTok{, }\ConstantTok{NA}\NormalTok{, ov.tifs1}\SpecialCharTok{$}\NormalTok{LST.day)}
\end{Highlighting}
\end{Shaded}

In this case we also exclude the values of \texttt{LST.day} are equal to 0 as these
are basically missing values in the GeoTIFFs. We repeat the same overlay operation
for the night light images:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{begin.tif2.lst }\OtherTok{=} \FunctionTok{as.Date}\NormalTok{(}\FunctionTok{paste0}\NormalTok{(}\StringTok{"2006{-}"}\NormalTok{, }\FunctionTok{substr}\NormalTok{(}\FunctionTok{basename}\NormalTok{(LST.listnight), }\DecValTok{9}\NormalTok{, }\DecValTok{10}\NormalTok{), }
                                \StringTok{"{-}"}\NormalTok{, }\FunctionTok{substr}\NormalTok{(}\FunctionTok{basename}\NormalTok{(LST.listnight), }\DecValTok{12}\NormalTok{, }\DecValTok{13}\NormalTok{)))}\SpecialCharTok{{-}}\DecValTok{4}
\NormalTok{end.tif2.lst }\OtherTok{=} \FunctionTok{as.Date}\NormalTok{(}\FunctionTok{paste0}\NormalTok{(}\StringTok{"2006{-}"}\NormalTok{, }\FunctionTok{substr}\NormalTok{(}\FunctionTok{basename}\NormalTok{(LST.listnight), }\DecValTok{9}\NormalTok{, }\DecValTok{10}\NormalTok{), }
                              \StringTok{"{-}"}\NormalTok{, }\FunctionTok{substr}\NormalTok{(}\FunctionTok{basename}\NormalTok{(LST.listnight), }\DecValTok{12}\NormalTok{, }\DecValTok{13}\NormalTok{)))}\SpecialCharTok{+}\DecValTok{4}
\NormalTok{ov.pnts }\OtherTok{\textless{}{-}}\NormalTok{ parallel}\SpecialCharTok{::}\FunctionTok{mclapply}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(LST.listnight), }\ControlFlowTok{function}\NormalTok{(i)\{ }
  \FunctionTok{extract\_st}\NormalTok{(}\AttributeTok{tif=}\NormalTok{LST.listnight[i], hrmeteo}\SpecialCharTok{$}\NormalTok{meteo, }\AttributeTok{date=}\StringTok{"Date"}\NormalTok{, }
             \AttributeTok{crs =} \FunctionTok{proj4string}\NormalTok{(hrgrid1km),        }
             \AttributeTok{date.tif.begin=}\NormalTok{begin.tif2.lst[i], }
             \AttributeTok{date.tif.end=}\NormalTok{end.tif2.lst[i], }
             \AttributeTok{coords=}\FunctionTok{c}\NormalTok{(}\StringTok{"x"}\NormalTok{,}\StringTok{"y"}\NormalTok{), }\AttributeTok{variable.name=}\StringTok{"LST.night"}\NormalTok{) \}, }
  \AttributeTok{mc.cores=}\NormalTok{mc.cores)}
\NormalTok{ov.pnts }\OtherTok{=}\NormalTok{ ov.pnts[}\SpecialCharTok{!}\FunctionTok{sapply}\NormalTok{(ov.pnts, is.null)]}
\NormalTok{ov.tifs2 }\OtherTok{=}\NormalTok{ plyr}\SpecialCharTok{::}\FunctionTok{join\_all}\NormalTok{(ov.pnts, }\AttributeTok{by=}\StringTok{"row.id"}\NormalTok{, }\AttributeTok{type=}\StringTok{"full"}\NormalTok{)}
\FunctionTok{str}\NormalTok{(ov.tifs2)}
\CommentTok{\#\textgreater{} \textquotesingle{}data.frame\textquotesingle{}:    44895 obs. of  2 variables:}
\CommentTok{\#\textgreater{}  $ LST.night: num  13344 13344 13344 13344 13344 ...}
\CommentTok{\#\textgreater{}  $ row.id   : int  1 2 3 4 5 366 367 368 369 370 ...}
\NormalTok{ov.tifs2}\SpecialCharTok{$}\NormalTok{LST.night }\OtherTok{=} \FunctionTok{ifelse}\NormalTok{(ov.tifs2}\SpecialCharTok{$}\NormalTok{LST.night }\SpecialCharTok{==} \DecValTok{0}\NormalTok{, }\ConstantTok{NA}\NormalTok{, ov.tifs2}\SpecialCharTok{$}\NormalTok{LST.night)}
\end{Highlighting}
\end{Shaded}

The result of spacetime overlay is a simple long table matching exactly the meteo-data table.
We next bind results of overlay using static and dynamic covariates:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{hrmeteo.rm }\OtherTok{=}\NormalTok{ plyr}\SpecialCharTok{::}\FunctionTok{join\_all}\NormalTok{(}\FunctionTok{list}\NormalTok{(hrmeteo}\SpecialCharTok{$}\NormalTok{meteo, ov.tifs1, ov.tifs2))}
\CommentTok{\#\textgreater{} Joining by: row.id}
\CommentTok{\#\textgreater{} Joining by: row.id}
\NormalTok{hrmeteo.rm }\OtherTok{=}\NormalTok{ plyr}\SpecialCharTok{::}\FunctionTok{join}\NormalTok{(hrmeteo.rm, idsta.ov)}
\CommentTok{\#\textgreater{} Joining by: IDSTA}
\end{Highlighting}
\end{Shaded}

we also add the geometric component of temperature based on the sphere formulas:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{hrmeteo.rm}\SpecialCharTok{$}\NormalTok{temp.mean }\OtherTok{\textless{}{-}} \FunctionTok{temp.from.geom}\NormalTok{(}\AttributeTok{fi=}\NormalTok{hrmeteo.rm}\SpecialCharTok{$}\NormalTok{Lat, }
                   \FunctionTok{as.numeric}\NormalTok{(}\FunctionTok{strftime}\NormalTok{(hrmeteo.rm}\SpecialCharTok{$}\NormalTok{Date, }\AttributeTok{format =} \StringTok{"\%j"}\NormalTok{)), }
                   \AttributeTok{a=}\FloatTok{37.03043}\NormalTok{, }\AttributeTok{b=}\SpecialCharTok{{-}}\FloatTok{15.43029}\NormalTok{, }\AttributeTok{elev=}\NormalTok{hrmeteo.rm}\SpecialCharTok{$}\NormalTok{HRdem, }\AttributeTok{t.grad=}\FloatTok{0.6}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We have now produced a \textbf{spatiotemporal regression matrix} that can be used to fit
a prediction model for daily temperature. The model is of form:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fm.tmp }\OtherTok{\textless{}{-}}\NormalTok{ MDTEMP }\SpecialCharTok{\textasciitilde{}}\NormalTok{ temp.mean }\SpecialCharTok{+}\NormalTok{ LST.day }\SpecialCharTok{+}\NormalTok{ LST.night }\SpecialCharTok{+}\NormalTok{ HRdsea}
\end{Highlighting}
\end{Shaded}

We next fit an Ensemble ML using the same process described in the previous sections:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(mlr)}
\NormalTok{lrn.rf }\OtherTok{=}\NormalTok{ mlr}\SpecialCharTok{::}\FunctionTok{makeLearner}\NormalTok{(}\StringTok{"regr.ranger"}\NormalTok{, }\AttributeTok{num.trees=}\DecValTok{150}\NormalTok{, }\AttributeTok{importance=}\StringTok{"impurity"}\NormalTok{,}
                          \AttributeTok{num.threads =}\NormalTok{ parallel}\SpecialCharTok{::}\FunctionTok{detectCores}\NormalTok{())}
\NormalTok{lrns.st }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(lrn.rf, mlr}\SpecialCharTok{::}\FunctionTok{makeLearner}\NormalTok{(}\StringTok{"regr.nnet"}\NormalTok{), mlr}\SpecialCharTok{::}\FunctionTok{makeLearner}\NormalTok{(}\StringTok{"regr.gamboost"}\NormalTok{))}
\NormalTok{sel }\OtherTok{=} \FunctionTok{complete.cases}\NormalTok{(hrmeteo.rm[,}\FunctionTok{all.vars}\NormalTok{(fm.tmp)])}
\NormalTok{hrmeteo.rm }\OtherTok{=}\NormalTok{ hrmeteo.rm[sel,]}
\CommentTok{\#summary(sel)}
\NormalTok{subs }\OtherTok{\textless{}{-}} \FunctionTok{runif}\NormalTok{(}\FunctionTok{nrow}\NormalTok{(hrmeteo.rm))}\SpecialCharTok{\textless{}}\NormalTok{.}\DecValTok{2}
\NormalTok{tsk0.st }\OtherTok{\textless{}{-}}\NormalTok{ mlr}\SpecialCharTok{::}\FunctionTok{makeRegrTask}\NormalTok{(}\AttributeTok{data =}\NormalTok{ hrmeteo.rm[subs,}\FunctionTok{all.vars}\NormalTok{(fm.tmp)], }
                             \AttributeTok{target =} \StringTok{"MDTEMP"}\NormalTok{, }\AttributeTok{blocking =} \FunctionTok{as.factor}\NormalTok{(hrmeteo.rm}\SpecialCharTok{$}\NormalTok{IDSTA[subs]))}
\NormalTok{tsk0.st}
\CommentTok{\#\textgreater{} Supervised task: hrmeteo.rm[subs, all.vars(fm.tmp)]}
\CommentTok{\#\textgreater{} Type: regr}
\CommentTok{\#\textgreater{} Target: MDTEMP}
\CommentTok{\#\textgreater{} Observations: 7573}
\CommentTok{\#\textgreater{} Features:}
\CommentTok{\#\textgreater{}    numerics     factors     ordered functionals }
\CommentTok{\#\textgreater{}           4           0           0           0 }
\CommentTok{\#\textgreater{} Missings: FALSE}
\CommentTok{\#\textgreater{} Has weights: FALSE}
\CommentTok{\#\textgreater{} Has blocking: TRUE}
\CommentTok{\#\textgreater{} Has coordinates: FALSE}
\end{Highlighting}
\end{Shaded}

Train model using a subset of points:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{init.TMP }\OtherTok{\textless{}{-}}\NormalTok{ mlr}\SpecialCharTok{::}\FunctionTok{makeStackedLearner}\NormalTok{(lrns.st, }\AttributeTok{method=}\StringTok{"stack.cv"}\NormalTok{, }\AttributeTok{super.learner=}\StringTok{"regr.lm"}\NormalTok{, }
                                    \AttributeTok{resampling=}\NormalTok{mlr}\SpecialCharTok{::}\FunctionTok{makeResampleDesc}\NormalTok{(}\AttributeTok{method=}\StringTok{"CV"}\NormalTok{, }\AttributeTok{blocking.cv=}\ConstantTok{TRUE}\NormalTok{))}
\NormalTok{parallelMap}\SpecialCharTok{::}\FunctionTok{parallelStartSocket}\NormalTok{(parallel}\SpecialCharTok{::}\FunctionTok{detectCores}\NormalTok{())}
\NormalTok{eml.TMP }\OtherTok{=} \FunctionTok{train}\NormalTok{(init.TMP, tsk0.st)}
\CommentTok{\#\textgreater{} \# weights:  19}
\CommentTok{\#\textgreater{} initial  value 1752597.611462 }
\CommentTok{\#\textgreater{} final  value 503633.983959 }
\CommentTok{\#\textgreater{} converged}
\NormalTok{parallelMap}\SpecialCharTok{::}\FunctionTok{parallelStop}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

This shows that daily temperatures can be predicted with relatively high R-square,
although the residual values are still significant (ranging from -1.8 to 1.8 degrees):

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(eml.TMP}\SpecialCharTok{$}\NormalTok{learner.model}\SpecialCharTok{$}\NormalTok{super.model}\SpecialCharTok{$}\NormalTok{learner.model)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} stats::lm(formula = f, data = d)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residuals:}
\CommentTok{\#\textgreater{}      Min       1Q   Median       3Q      Max }
\CommentTok{\#\textgreater{} {-}16.2439  {-}1.8106   0.0204   1.8004  14.9611 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}               Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept)   35.06219    8.10904   4.324 1.55e{-}05 ***}
\CommentTok{\#\textgreater{} regr.ranger    0.70575    0.02770  25.474  \textless{} 2e{-}16 ***}
\CommentTok{\#\textgreater{} regr.nnet     {-}2.72276    0.62920  {-}4.327 1.53e{-}05 ***}
\CommentTok{\#\textgreater{} regr.gamboost  0.29540    0.02799  10.553  \textless{} 2e{-}16 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residual standard error: 2.888 on 7569 degrees of freedom}
\CommentTok{\#\textgreater{} Multiple R{-}squared:  0.8747, Adjusted R{-}squared:  0.8746 }
\CommentTok{\#\textgreater{} F{-}statistic: 1.761e+04 on 3 and 7569 DF,  p{-}value: \textless{} 2.2e{-}16}
\end{Highlighting}
\end{Shaded}

The variable importance analysis shows that the most important variable for predicting
daily temperatures is, in fact, the night-time MODIS LST:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggplot2)}
\NormalTok{xl }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(mlr}\SpecialCharTok{::}\FunctionTok{getFeatureImportance}\NormalTok{(eml.TMP[[}\StringTok{"learner.model"}\NormalTok{]][[}\StringTok{"base.models"}\NormalTok{]][[}\DecValTok{1}\NormalTok{]])}\SpecialCharTok{$}\NormalTok{res)}
\NormalTok{xl}\SpecialCharTok{$}\NormalTok{relative\_importance }\OtherTok{=} \DecValTok{100}\SpecialCharTok{*}\NormalTok{xl}\SpecialCharTok{$}\NormalTok{importance}\SpecialCharTok{/}\FunctionTok{sum}\NormalTok{(xl}\SpecialCharTok{$}\NormalTok{importance)}
\NormalTok{xl }\OtherTok{=}\NormalTok{ xl[}\FunctionTok{order}\NormalTok{(xl}\SpecialCharTok{$}\NormalTok{relative\_importance, }\AttributeTok{decreasing =} \ConstantTok{TRUE}\NormalTok{),]}
\NormalTok{xl}\SpecialCharTok{$}\NormalTok{variable }\OtherTok{=} \FunctionTok{paste0}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(xl}\SpecialCharTok{$}\NormalTok{variable)), }\StringTok{". "}\NormalTok{, xl}\SpecialCharTok{$}\NormalTok{variable)}
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ xl[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{,], }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =} \FunctionTok{reorder}\NormalTok{(variable, relative\_importance), }\AttributeTok{y =}\NormalTok{ relative\_importance)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{fill =} \StringTok{"steelblue"}\NormalTok{,}
           \AttributeTok{stat =} \StringTok{"identity"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{coord\_flip}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Variable importance"}\NormalTok{,}
       \AttributeTok{x =} \ConstantTok{NULL}\NormalTok{,}
       \AttributeTok{y =} \ConstantTok{NULL}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{theme}\NormalTok{(}\AttributeTok{text =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size=}\DecValTok{15}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{spatiotemporal-interpolation_files/figure-latex/var-imptemp-1} 

}

\caption{Variable importance for modeling spacetime daily temperatures.}\label{fig:var-imptemp}
\end{figure}

We can use the \textbf{fitted spacetime EML model} to generate predictions e.g.~for
four consecutive days in August. First, we import MODIS LST for month of interest:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{hrpred1km }\OtherTok{=}\NormalTok{ hrgrid1km}
\NormalTok{sel.tifs1 }\OtherTok{=}\NormalTok{ LST.listday[}\FunctionTok{grep}\NormalTok{(}\StringTok{"\_08\_"}\NormalTok{, LST.listday)]}
\NormalTok{sel.tifs2 }\OtherTok{=}\NormalTok{ LST.listnight[}\FunctionTok{grep}\NormalTok{(}\StringTok{"\_08\_"}\NormalTok{, LST.listnight)]}
\DocumentationTok{\#\# read to R in parallel}
\NormalTok{x1 }\OtherTok{=} \FunctionTok{as.data.frame}\NormalTok{( parallel}\SpecialCharTok{::}\FunctionTok{mclapply}\NormalTok{(sel.tifs1, }
      \ControlFlowTok{function}\NormalTok{(i)\{x }\OtherTok{\textless{}{-}} \FunctionTok{readGDAL}\NormalTok{(i)}\SpecialCharTok{$}\NormalTok{band1; x }\OtherTok{\textless{}{-}} \FunctionTok{ifelse}\NormalTok{(x}\SpecialCharTok{\textless{}}\DecValTok{1}\NormalTok{, }\ConstantTok{NA}\NormalTok{, x); }\FunctionTok{return}\NormalTok{(x)\}, }
                                       \AttributeTok{mc.cores =}\NormalTok{ mc.cores))}
\NormalTok{x2 }\OtherTok{=} \FunctionTok{as.data.frame}\NormalTok{( parallel}\SpecialCharTok{::}\FunctionTok{mclapply}\NormalTok{(sel.tifs2, }
      \ControlFlowTok{function}\NormalTok{(i)\{x }\OtherTok{\textless{}{-}} \FunctionTok{readGDAL}\NormalTok{(i)}\SpecialCharTok{$}\NormalTok{band1; x }\OtherTok{\textless{}{-}} \FunctionTok{ifelse}\NormalTok{(x}\SpecialCharTok{\textless{}}\DecValTok{1}\NormalTok{, }\ConstantTok{NA}\NormalTok{, x); }\FunctionTok{return}\NormalTok{(x)\}, }
                                       \AttributeTok{mc.cores =}\NormalTok{ mc.cores))}
\FunctionTok{names}\NormalTok{(x1)  }\OtherTok{\textless{}{-}} \FunctionTok{basename}\NormalTok{(sel.tifs1); }\FunctionTok{names}\NormalTok{(x2) }\OtherTok{\textless{}{-}} \FunctionTok{basename}\NormalTok{(sel.tifs2)}
\end{Highlighting}
\end{Shaded}

Second, we interpolate values between 8--day periods and fill gaps in EO data
using simple linear interpolation (MODIS images are available only every 8 days):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dates.lst }\OtherTok{=} \FunctionTok{as.Date}\NormalTok{(}\StringTok{"2006{-}08{-}13"}\NormalTok{)}\SpecialCharTok{+}\DecValTok{1}\SpecialCharTok{:}\DecValTok{8}
\NormalTok{in.dates }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\StringTok{"2006{-}08{-}05"}\NormalTok{, }\StringTok{"2006{-}08{-}13"}\NormalTok{, }\StringTok{"2006{-}08{-}21"}\NormalTok{, }\StringTok{"2006{-}08{-}29"}\NormalTok{)}
\NormalTok{in.days }\OtherTok{=} \FunctionTok{as.numeric}\NormalTok{(}\FunctionTok{strftime}\NormalTok{(}\FunctionTok{as.Date}\NormalTok{(}\FunctionTok{c}\NormalTok{(in.dates)), }\AttributeTok{format =} \StringTok{"\%j"}\NormalTok{))}
\DocumentationTok{\#\# interpolate values for missing dates in spacetime}
\FunctionTok{library}\NormalTok{(parallel)}
\NormalTok{cl }\OtherTok{\textless{}{-}} \FunctionTok{makeCluster}\NormalTok{(}\FunctionTok{detectCores}\NormalTok{())}
\FunctionTok{clusterExport}\NormalTok{(cl, }\FunctionTok{c}\NormalTok{(}\StringTok{"in.days"}\NormalTok{, }\StringTok{"dates.lst"}\NormalTok{))}
\NormalTok{t1s }\OtherTok{=}\NormalTok{ parallel}\SpecialCharTok{::}\FunctionTok{parApply}\NormalTok{(cl, x1, }\DecValTok{1}\NormalTok{, }
      \ControlFlowTok{function}\NormalTok{(y) \{ }\FunctionTok{try}\NormalTok{( }\FunctionTok{approx}\NormalTok{(in.days, }\FunctionTok{as.vector}\NormalTok{(y), }\AttributeTok{xout=}\FunctionTok{as.numeric}\NormalTok{(}\FunctionTok{strftime}\NormalTok{(dates.lst, }\AttributeTok{format =} \StringTok{"\%j"}\NormalTok{)))}\SpecialCharTok{$}\NormalTok{y ) \})}
\NormalTok{t2s }\OtherTok{=}\NormalTok{ parallel}\SpecialCharTok{::}\FunctionTok{parApply}\NormalTok{(cl, x2, }\DecValTok{1}\NormalTok{, }
      \ControlFlowTok{function}\NormalTok{(y) \{ }\FunctionTok{try}\NormalTok{( }\FunctionTok{approx}\NormalTok{(in.days, }\FunctionTok{as.vector}\NormalTok{(y), }\AttributeTok{xout=}\FunctionTok{as.numeric}\NormalTok{(}\FunctionTok{strftime}\NormalTok{(dates.lst, }\AttributeTok{format =} \StringTok{"\%j"}\NormalTok{)))}\SpecialCharTok{$}\NormalTok{y ) \})}
\FunctionTok{stopCluster}\NormalTok{(cl)}
\DocumentationTok{\#\# remove missing pixels}
\NormalTok{x.t1s }\OtherTok{=}\NormalTok{ parallel}\SpecialCharTok{::}\FunctionTok{mclapply}\NormalTok{(t1s, length, }\AttributeTok{mc.cores =}\NormalTok{ mc.cores)}
\NormalTok{t1s[}\FunctionTok{which}\NormalTok{(}\SpecialCharTok{!}\NormalTok{x.t1s}\SpecialCharTok{==}\DecValTok{8}\NormalTok{)] }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\ConstantTok{NA}\NormalTok{, }\DecValTok{8}\NormalTok{))}
\NormalTok{t1s }\OtherTok{=} \FunctionTok{do.call}\NormalTok{(rbind.data.frame, t1s)}
\FunctionTok{names}\NormalTok{(t1s) }\OtherTok{=} \FunctionTok{paste0}\NormalTok{(}\StringTok{"LST.day\_"}\NormalTok{, dates.lst)}
\NormalTok{x.t2s }\OtherTok{=}\NormalTok{ parallel}\SpecialCharTok{::}\FunctionTok{mclapply}\NormalTok{(t2s, length, }\AttributeTok{mc.cores =}\NormalTok{ mc.cores)}
\NormalTok{t2s[}\FunctionTok{which}\NormalTok{(}\SpecialCharTok{!}\NormalTok{x.t2s}\SpecialCharTok{==}\DecValTok{8}\NormalTok{)] }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\ConstantTok{NA}\NormalTok{, }\DecValTok{8}\NormalTok{))}
\NormalTok{t2s }\OtherTok{=} \FunctionTok{do.call}\NormalTok{(rbind.data.frame, t2s)}
\FunctionTok{names}\NormalTok{(t2s) }\OtherTok{=} \FunctionTok{paste0}\NormalTok{(}\StringTok{"LST.night\_"}\NormalTok{, dates.lst)}
\end{Highlighting}
\end{Shaded}

Now we can make predictions for the target days in August 2006 by using (we run
this operation in a loop to avoid RAM overload):

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{for}\NormalTok{(j }\ControlFlowTok{in} \FunctionTok{paste}\NormalTok{(dates.lst))\{}
\NormalTok{  out.tif }\OtherTok{=} \FunctionTok{paste0}\NormalTok{(}\StringTok{"output/MDTEMP\_"}\NormalTok{, j, }\StringTok{".tif"}\NormalTok{)}
  \ControlFlowTok{if}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{file.exists}\NormalTok{(out.tif))\{}
\NormalTok{    hrpred1km}\SpecialCharTok{@}\NormalTok{data[,}\StringTok{"LST.day"}\NormalTok{] }\OtherTok{=}\NormalTok{ t1s[,}\FunctionTok{paste0}\NormalTok{(}\StringTok{"LST.day\_"}\NormalTok{, j)]}
\NormalTok{    hrpred1km}\SpecialCharTok{@}\NormalTok{data[,}\StringTok{"LST.night"}\NormalTok{] }\OtherTok{=}\NormalTok{ t2s[,}\FunctionTok{paste0}\NormalTok{(}\StringTok{"LST.night\_"}\NormalTok{, j)]}
\NormalTok{    hrpred1km}\SpecialCharTok{$}\NormalTok{temp.mean }\OtherTok{=} \FunctionTok{temp.from.geom}\NormalTok{(}\AttributeTok{fi=}\NormalTok{hrpred1km}\SpecialCharTok{$}\NormalTok{Lat, }
                     \FunctionTok{as.numeric}\NormalTok{(}\FunctionTok{strftime}\NormalTok{(}\FunctionTok{as.Date}\NormalTok{(j), }\AttributeTok{format =} \StringTok{"\%j"}\NormalTok{)), }
                     \AttributeTok{a=}\FloatTok{37.03043}\NormalTok{, }\AttributeTok{b=}\SpecialCharTok{{-}}\FloatTok{15.43029}\NormalTok{, }\AttributeTok{elev=}\NormalTok{hrpred1km}\SpecialCharTok{$}\NormalTok{HRdem, }\AttributeTok{t.grad=}\FloatTok{0.6}\NormalTok{)}
\NormalTok{    sel.pix }\OtherTok{=} \FunctionTok{complete.cases}\NormalTok{(hrpred1km}\SpecialCharTok{@}\NormalTok{data[,eml.TMP}\SpecialCharTok{$}\NormalTok{features])}
\NormalTok{    out }\OtherTok{=} \FunctionTok{predict}\NormalTok{(eml.TMP, }\AttributeTok{newdata=}\NormalTok{hrpred1km}\SpecialCharTok{@}\NormalTok{data[sel.pix,eml.TMP}\SpecialCharTok{$}\NormalTok{features])}
\NormalTok{    hrpred1km}\SpecialCharTok{@}\NormalTok{data[,}\FunctionTok{paste0}\NormalTok{(}\StringTok{"MDTEMP\_"}\NormalTok{, j)] }\OtherTok{=} \ConstantTok{NA}
\NormalTok{    hrpred1km}\SpecialCharTok{@}\NormalTok{data[sel.pix, }\FunctionTok{make.names}\NormalTok{(}\FunctionTok{paste0}\NormalTok{(}\StringTok{"MDTEMP\_"}\NormalTok{, j))] }\OtherTok{=}\NormalTok{ out}\SpecialCharTok{$}\NormalTok{data}\SpecialCharTok{$}\NormalTok{response }\SpecialCharTok{*} \DecValTok{10}
    \FunctionTok{writeGDAL}\NormalTok{(hrpred1km[}\FunctionTok{make.names}\NormalTok{(}\FunctionTok{paste0}\NormalTok{(}\StringTok{"MDTEMP\_"}\NormalTok{, j))], out.tif, }\AttributeTok{mvFlag =} \SpecialCharTok{{-}}\DecValTok{32768}\NormalTok{,}
              \AttributeTok{type =} \StringTok{"Int16"}\NormalTok{, }\AttributeTok{options =} \FunctionTok{c}\NormalTok{(}\StringTok{"COMPRESS=DEFLATE"}\NormalTok{))}
\NormalTok{  \} }\ControlFlowTok{else}\NormalTok{ \{}
\NormalTok{    hrpred1km}\SpecialCharTok{@}\NormalTok{data[,}\FunctionTok{make.names}\NormalTok{(}\FunctionTok{paste0}\NormalTok{(}\StringTok{"MDTEMP\_"}\NormalTok{, j))] }\OtherTok{=} \FunctionTok{readGDAL}\NormalTok{(out.tif)}\SpecialCharTok{$}\NormalTok{band1}
\NormalTok{  \}}
\NormalTok{\}}
\CommentTok{\#\textgreater{} output/MDTEMP\_2006{-}08{-}14.tif has GDAL driver GTiff }
\CommentTok{\#\textgreater{} and has 487 rows and 490 columns}
\CommentTok{\#\textgreater{} output/MDTEMP\_2006{-}08{-}15.tif has GDAL driver GTiff }
\CommentTok{\#\textgreater{} and has 487 rows and 490 columns}
\CommentTok{\#\textgreater{} output/MDTEMP\_2006{-}08{-}16.tif has GDAL driver GTiff }
\CommentTok{\#\textgreater{} and has 487 rows and 490 columns}
\CommentTok{\#\textgreater{} output/MDTEMP\_2006{-}08{-}17.tif has GDAL driver GTiff }
\CommentTok{\#\textgreater{} and has 487 rows and 490 columns}
\CommentTok{\#\textgreater{} output/MDTEMP\_2006{-}08{-}18.tif has GDAL driver GTiff }
\CommentTok{\#\textgreater{} and has 487 rows and 490 columns}
\CommentTok{\#\textgreater{} output/MDTEMP\_2006{-}08{-}19.tif has GDAL driver GTiff }
\CommentTok{\#\textgreater{} and has 487 rows and 490 columns}
\CommentTok{\#\textgreater{} output/MDTEMP\_2006{-}08{-}20.tif has GDAL driver GTiff }
\CommentTok{\#\textgreater{} and has 487 rows and 490 columns}
\CommentTok{\#\textgreater{} output/MDTEMP\_2006{-}08{-}21.tif has GDAL driver GTiff }
\CommentTok{\#\textgreater{} and has 487 rows and 490 columns}
\end{Highlighting}
\end{Shaded}

To plot these predictions we can either put predictions in the \texttt{spacetime} package
class (see \href{https://cran.r-project.org/web/packages/gstat/vignettes/spatio-temporal-kriging.pdf}{gstat tutorial}), or simply plot them using \texttt{sp} package:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{st.pts }\OtherTok{=} \FunctionTok{list}\NormalTok{(}\StringTok{"sp.points"}\NormalTok{, idsta.pnts, }\AttributeTok{pch =} \StringTok{"+"}\NormalTok{, }\AttributeTok{col=}\StringTok{"black"}\NormalTok{)}
\FunctionTok{spplot}\NormalTok{(hrpred1km[}\FunctionTok{make.names}\NormalTok{(}\FunctionTok{paste0}\NormalTok{(}\StringTok{"MDTEMP\_"}\NormalTok{, dates.lst[}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{8}\NormalTok{)]))], }
       \AttributeTok{col.regions=}\NormalTok{plotKML}\SpecialCharTok{::}\NormalTok{R\_pal[[}\StringTok{"rainbow\_75"}\NormalTok{]][}\DecValTok{4}\SpecialCharTok{:}\DecValTok{20}\NormalTok{],}
       \AttributeTok{at =} \FunctionTok{seq}\NormalTok{(}\DecValTok{143}\NormalTok{, }\DecValTok{239}\NormalTok{, }\AttributeTok{length.out=}\DecValTok{17}\NormalTok{),}
       \AttributeTok{sp.layout =} \FunctionTok{list}\NormalTok{(st.pts),}
       \AttributeTok{main=}\StringTok{"Prediction daily temperature"}\NormalTok{)}
\CommentTok{\#dev.off()}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{spatiotemporal-interpolation_files/figure-latex/st-plottemp-1} 

}

\caption{Predictions spacetime daily temperature for August 2006.}\label{fig:st-plottemp}
\end{figure}

In summary, this example shows how to fit spatiotemporal EML with using
also seasonality component together with the EO data. It can hence
be considered a \emph{complete framework} for spatiotemporal interpolation as both static,
dynamic covariates and latitude / elevation are used for model training.

\hypertarget{spatiotemporal-distribution-of-fagus-sylvatica}{%
\section{Spatiotemporal distribution of Fagus sylvatica}\label{spatiotemporal-distribution-of-fagus-sylvatica}}

In the next example we show how to fit a spatiotemporal model
using biological data: occurrences of \href{https://www.gbif.org/species/2882316}{\emph{Fagus
sylvatica}} over Europe. This is
the domain of \textbf{Species Distribution modeling}, except in this case we
model distribution of target species also in spacetime. The training (point) data has
been compiled for the purpose of the OpenDataScience.eu project, then
cleaned and overlaid vs time-series of \href{https://gitlab.com/geoharmonizer_inea/eumap}{Landsat GLAD} images and climatic
variables \citep{witjes2021spatiotemporal, Bonannella2022}.
For more details about the data refer also to the \href{https://gitlab.com/geoharmonizer_inea/eumap}{eumap repository}.

We can load a snapshot of data by using:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(data.table)}
\FunctionTok{library}\NormalTok{(mlr)}
\FunctionTok{library}\NormalTok{(sp)}
\NormalTok{fs.rm }\OtherTok{=} \FunctionTok{readRDS}\NormalTok{(}\StringTok{\textquotesingle{}input/fagus\_sylvatica\_st.rds\textquotesingle{}}\NormalTok{)}
\NormalTok{occ.pnts }\OtherTok{=}\NormalTok{ fs.rm[,}\FunctionTok{c}\NormalTok{(}\StringTok{"Atlas\_class"}\NormalTok{,}\StringTok{"easting"}\NormalTok{,}\StringTok{"northing"}\NormalTok{)]}
\FunctionTok{coordinates}\NormalTok{(occ.pnts) }\OtherTok{=} \ErrorTok{\textasciitilde{}}\NormalTok{ easting }\SpecialCharTok{+}\NormalTok{ northing}
\FunctionTok{proj4string}\NormalTok{(occ.pnts) }\OtherTok{=} \StringTok{"+init=epsg:3035"}
\NormalTok{occ.pnts }\OtherTok{=} \FunctionTok{spTransform}\NormalTok{(occ.pnts, }\FunctionTok{CRS}\NormalTok{(}\StringTok{"+init=epsg:4326"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

This is a subset of a \href{https://doi.org/10.5281/zenodo.5818021}{larger
dataset} that
has been used to produce predictions of distribution of key forest tree
species for Europe (you can browse the data via \url{https://maps.opendatascience.eu}).
The first columns of this dataset show:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(fs.rm[,}\DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{])}
\CommentTok{\#\textgreater{}          id year postprocess Tile\_ID easting northing Atlas\_class lc1}
\CommentTok{\#\textgreater{} 9   1499539 2015   spacetime    9689 3948500  2431500           1    }
\CommentTok{\#\textgreater{} 38   660325 2008   spacetime    8728 3318500  2283500           1    }
\CommentTok{\#\textgreater{} 56   660325 2002   spacetime    8728 3318500  2283500           1    }
\CommentTok{\#\textgreater{} 68  2044229 2006   spacetime   11017 4294500  2655500           1    }
\CommentTok{\#\textgreater{} 104  586994 2016   spacetime    8724 3204500  2309500           1    }
\CommentTok{\#\textgreater{} 111  622055 2016   spacetime    8349 3231500  2226500           1    }
\CommentTok{\#\textgreater{}     clm\_air.temp\_era5.copernicus\_av\_1km\_200..200cm\_02.01..02.28\_avg\_5yrs\_eumap\_epsg3035\_v0.1.tif}
\CommentTok{\#\textgreater{} 9                                                                                            {-}35}
\CommentTok{\#\textgreater{} 38                                                                                             0}
\CommentTok{\#\textgreater{} 56                                                                                            19}
\CommentTok{\#\textgreater{} 68                                                                                           {-}35}
\CommentTok{\#\textgreater{} 104                                                                                           31}
\CommentTok{\#\textgreater{} 111                                                                                           {-}6}
\CommentTok{\#\textgreater{}     clm\_air.temp\_era5.copernicus\_av\_1km\_200..200cm\_02.01..02.28\_mean\_eumap\_epsg3035\_v0.1.tif}
\CommentTok{\#\textgreater{} 9                                                                                        {-}36}
\CommentTok{\#\textgreater{} 38                                                                                        31}
\CommentTok{\#\textgreater{} 56                                                                                        22}
\CommentTok{\#\textgreater{} 68                                                                                       {-}40}
\CommentTok{\#\textgreater{} 104                                                                                       46}
\CommentTok{\#\textgreater{} 111                                                                                        9}
\end{Highlighting}
\end{Shaded}

The header columns are:

\begin{itemize}
\tightlist
\item
  \texttt{id}: is the unique ID of each point;\\
\item
  \texttt{year}: is the year of obsevation;\\
\item
  \texttt{postprocess}: column can have value yearly or spacetime to identify
  if the temporal reference of an observation comes from the original
  dataset or is the result of post-processing (yearly for originals,
  spacetime for post-processed points);\\
\item
  \texttt{Tile\_ID}: is as extracted from the 30-km tiling system;\\
\item
  \texttt{easting}: is the easting coordinate of the observation point;\\
\item
  \texttt{northing}: is the northing coordinate of the observation point;\\
\item
  \texttt{Atlas\_class}: contains name of the tree species or NULL if it's an
  absence point coming from LUCAS;\\
\item
  \texttt{lc1}: contains original LUCAS land cover classes or NULL if it's a
  presence point;
\end{itemize}

Other columns are the EO and ecological covariates that we use for
modeling distribution of \href{https://www.gbif.org/species/2882316}{\emph{Fagus sylvatica}}. We can plot distribution of points over EU using:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(rnaturalearth)}
\FunctionTok{library}\NormalTok{(raster)}
\NormalTok{europe }\OtherTok{\textless{}{-}}\NormalTok{ rnaturalearth}\SpecialCharTok{::}\FunctionTok{ne\_countries}\NormalTok{(}\AttributeTok{scale=}\DecValTok{10}\NormalTok{, }\AttributeTok{continent =} \StringTok{\textquotesingle{}europe\textquotesingle{}}\NormalTok{)}
\NormalTok{europe }\OtherTok{\textless{}{-}}\NormalTok{ raster}\SpecialCharTok{::}\FunctionTok{crop}\NormalTok{(europe, }\FunctionTok{extent}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{24.8}\NormalTok{,}\FloatTok{35.2}\NormalTok{,}\DecValTok{31}\NormalTok{,}\FloatTok{68.5}\NormalTok{))}
\NormalTok{op }\OtherTok{=} \FunctionTok{par}\NormalTok{(}\AttributeTok{mar=}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(europe, }\AttributeTok{col=}\StringTok{"lightgrey"}\NormalTok{, }\AttributeTok{border=}\StringTok{"darkgrey"}\NormalTok{, }\AttributeTok{axes=}\ConstantTok{FALSE}\NormalTok{)}
\FunctionTok{points}\NormalTok{(occ.pnts[occ.pnts}\SpecialCharTok{$}\NormalTok{Atlas\_class}\SpecialCharTok{==}\DecValTok{1}\NormalTok{,], }\AttributeTok{pch=}\StringTok{"+"}\NormalTok{, }\AttributeTok{cex=}\NormalTok{.}\DecValTok{8}\NormalTok{)}
\FunctionTok{par}\NormalTok{(op)}
\CommentTok{\#dev.off()}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{spatiotemporal-interpolation_files/figure-latex/map-fs-1} 

}

\caption{Distribution of occurrence locations for Fagus Sylvatica. Each training points is also referrenced in time and hence can be used to run spacetime overlay.}\label{fig:map-fs}
\end{figure}

As in previous examples, we first define the target model formula. We
remove from model all other columns that are not used for prediction:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{covs }\OtherTok{=} \FunctionTok{grep}\NormalTok{(}\StringTok{"id|year|postprocess|Tile\_ID|easting|northing|Atlas\_class|lc1"}\NormalTok{, }
          \FunctionTok{colnames}\NormalTok{(fs.rm), }\AttributeTok{value =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{invert =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{fm.fs }\OtherTok{=}\NormalTok{ stats}\SpecialCharTok{::}\FunctionTok{as.formula}\NormalTok{(}\FunctionTok{paste}\NormalTok{(}\StringTok{"Atlas\_class \textasciitilde{} "}\NormalTok{, }\FunctionTok{paste}\NormalTok{(covs, }\AttributeTok{collapse=}\StringTok{"+"}\NormalTok{)))}
\NormalTok{fs.rm}\SpecialCharTok{$}\NormalTok{Atlas\_class }\OtherTok{=} \FunctionTok{factor}\NormalTok{(fs.rm}\SpecialCharTok{$}\NormalTok{Atlas\_class)}
\FunctionTok{all.vars}\NormalTok{(fm.fs)[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{]}
\CommentTok{\#\textgreater{} [1] "Atlas\_class"                                                                                 }
\CommentTok{\#\textgreater{} [2] "clm\_air.temp\_era5.copernicus\_av\_1km\_200..200cm\_02.01..02.28\_avg\_5yrs\_eumap\_epsg3035\_v0.1.tif"}
\CommentTok{\#\textgreater{} [3] "clm\_air.temp\_era5.copernicus\_av\_1km\_200..200cm\_02.01..02.28\_mean\_eumap\_epsg3035\_v0.1.tif"    }
\CommentTok{\#\textgreater{} [4] "clm\_air.temp\_era5.copernicus\_av\_1km\_200..200cm\_02.01..02.28\_std\_eumap\_epsg3035\_v0.1.tif"     }
\CommentTok{\#\textgreater{} [5] "clm\_air.temp\_era5.copernicus\_av\_1km\_200..200cm\_03.01..03.31\_avg\_5yrs\_eumap\_epsg3035\_v0.1.tif"}
\end{Highlighting}
\end{Shaded}

To speed-up fitting of the models we have prepared a function that wraps all modeling steps:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{source}\NormalTok{(}\StringTok{"mlst\_functions.R"}\NormalTok{)}
\NormalTok{fs.rm0 }\OtherTok{=}\NormalTok{ fs.rm[}\FunctionTok{runif}\NormalTok{(}\FunctionTok{nrow}\NormalTok{(fs.rm))}\SpecialCharTok{\textless{}}\NormalTok{.}\DecValTok{2}\NormalTok{,]}
\NormalTok{eml.fs }\OtherTok{=} \FunctionTok{train\_sp\_eml}\NormalTok{(}\AttributeTok{data =}\NormalTok{ fs.rm0, }\AttributeTok{formula =}\NormalTok{ fm.fs, }\AttributeTok{blocking =} \FunctionTok{as.factor}\NormalTok{(fs.rm}\SpecialCharTok{$}\NormalTok{Tile\_ID))}
\end{Highlighting}
\end{Shaded}

This fits an ensemble model of binary classification models
(\texttt{classif.ranger}, \texttt{classif.xgboost}, \texttt{classif.glmnet}) and which basically can
be used to predict probability of any training point being classified \texttt{0}
(not-occurring) or \texttt{1} (occurring).

The intermediate models (fine-tuned RF and XGboost) are written to local
folder \texttt{output} as RDS files. For meta-learner we use a GLM model with binomial link
function:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#eml.fs = readRDS("output/EML\_model.rds")}
\FunctionTok{summary}\NormalTok{(eml.fs}\SpecialCharTok{$}\NormalTok{learner.model}\SpecialCharTok{$}\NormalTok{super.model}\SpecialCharTok{$}\NormalTok{learner.model)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} stats::glm(formula = f, family = "binomial", data = getTaskData(.task, }
\CommentTok{\#\textgreater{}     .subset), weights = .weights, model = FALSE)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Deviance Residuals: }
\CommentTok{\#\textgreater{}     Min       1Q   Median       3Q      Max  }
\CommentTok{\#\textgreater{} {-}3.4025  {-}0.0917   0.0666   0.0701   3.2589  }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}                 Estimate Std. Error z value Pr(\textgreater{}|z|)    }
\CommentTok{\#\textgreater{} (Intercept)       5.6680     0.4800  11.807  \textless{} 2e{-}16 ***}
\CommentTok{\#\textgreater{} classif.ranger   {-}8.4832     0.6452 {-}13.147  \textless{} 2e{-}16 ***}
\CommentTok{\#\textgreater{} classif.xgboost   1.2604     1.2307   1.024    0.306    }
\CommentTok{\#\textgreater{} classif.glmnet   {-}3.4816     0.5708  {-}6.099 1.07e{-}09 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} (Dispersion parameter for binomial family taken to be 1)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}     Null deviance: 8432.56  on 7472  degrees of freedom}
\CommentTok{\#\textgreater{} Residual deviance:  700.97  on 7469  degrees of freedom}
\CommentTok{\#\textgreater{} AIC: 708.97}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Number of Fisher Scoring iterations: 8}
\end{Highlighting}
\end{Shaded}

The variable importance analysis (RF component) shows that the most
important covariates for mapping distribution of Fagus sylvatica are
landsat images (which is expected):

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggplot2)}
\NormalTok{xl }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(mlr}\SpecialCharTok{::}\FunctionTok{getFeatureImportance}\NormalTok{(eml.fs[[}\StringTok{"learner.model"}\NormalTok{]][[}\StringTok{"base.models"}\NormalTok{]][[}\DecValTok{1}\NormalTok{]])}\SpecialCharTok{$}\NormalTok{res)}
\NormalTok{xl}\SpecialCharTok{$}\NormalTok{relative\_importance }\OtherTok{=} \DecValTok{100}\SpecialCharTok{*}\NormalTok{xl}\SpecialCharTok{$}\NormalTok{importance}\SpecialCharTok{/}\FunctionTok{sum}\NormalTok{(xl}\SpecialCharTok{$}\NormalTok{importance)}
\NormalTok{xl }\OtherTok{=}\NormalTok{ xl[}\FunctionTok{order}\NormalTok{(xl}\SpecialCharTok{$}\NormalTok{relative\_importance, }\AttributeTok{decreasing =}\NormalTok{ T),]}
\NormalTok{xl}\SpecialCharTok{$}\NormalTok{variable }\OtherTok{=} \FunctionTok{paste0}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{182}\NormalTok{), }\StringTok{". "}\NormalTok{, xl}\SpecialCharTok{$}\NormalTok{variable)}
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ xl[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{20}\NormalTok{,], }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =} \FunctionTok{reorder}\NormalTok{(variable, relative\_importance), }\AttributeTok{y =}\NormalTok{ relative\_importance)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{fill =} \StringTok{"steelblue"}\NormalTok{,}
           \AttributeTok{stat =} \StringTok{"identity"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{coord\_flip}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Variable importance"}\NormalTok{,}
       \AttributeTok{x =} \ConstantTok{NULL}\NormalTok{,}
       \AttributeTok{y =} \ConstantTok{NULL}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{theme}\NormalTok{(}\AttributeTok{text =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size=}\DecValTok{15}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{spatiotemporal-interpolation_files/figure-latex/varimp-st-1} 

}

\caption{Variable importance for spatiotemporal model used to predict distribution of Fagus sylvatica.}\label{fig:varimp-st}
\end{figure}

To produce spacetime predictions for some tiles (120-m spatial
resolution) we can run:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m1 }\OtherTok{=} \FunctionTok{predict\_tiles}\NormalTok{(}\AttributeTok{input =} \StringTok{"9690.2015"}\NormalTok{,  }\AttributeTok{model =}\NormalTok{ eml.fs)}
\CommentTok{\#\textgreater{} [1] "9690 {-} reading the data"}
\CommentTok{\#\textgreater{} Warning in dir.create(tmp\_folder, recursive = TRUE): \textquotesingle{}output//9690\textquotesingle{} already}
\CommentTok{\#\textgreater{} exists}
\CommentTok{\#\textgreater{} [1] "9690 {-} running predictions"}
\CommentTok{\#\textgreater{} Warning in if (class(probability\_map) == "try{-}error") \{: the condition has}
\CommentTok{\#\textgreater{} length \textgreater{} 1 and only the first element will be used}
\CommentTok{\#\textgreater{} [1] "9690 {-} writing files"}
\NormalTok{m2 }\OtherTok{=} \FunctionTok{predict\_tiles}\NormalTok{(}\AttributeTok{input =} \StringTok{"9690.2017"}\NormalTok{,  }\AttributeTok{model =}\NormalTok{ eml.fs)}
\CommentTok{\#\textgreater{} [1] "9690 {-} reading the data"}
\CommentTok{\#\textgreater{} Warning in dir.create(tmp\_folder, recursive = TRUE): \textquotesingle{}output//9690\textquotesingle{} already}
\CommentTok{\#\textgreater{} exists}
\CommentTok{\#\textgreater{} [1] "9690 {-} running predictions"}
\CommentTok{\#\textgreater{} Warning in if (class(probability\_map) == "try{-}error") \{: the condition has}
\CommentTok{\#\textgreater{} length \textgreater{} 1 and only the first element will be used}
\CommentTok{\#\textgreater{} [1] "9690 {-} writing files"}
\NormalTok{m3 }\OtherTok{=} \FunctionTok{predict\_tiles}\NormalTok{(}\AttributeTok{input =} \StringTok{"9690.2019"}\NormalTok{,  }\AttributeTok{model =}\NormalTok{ eml.fs)}
\CommentTok{\#\textgreater{} [1] "9690 {-} reading the data"}
\CommentTok{\#\textgreater{} Warning in dir.create(tmp\_folder, recursive = TRUE): \textquotesingle{}output//9690\textquotesingle{} already}
\CommentTok{\#\textgreater{} exists}
\CommentTok{\#\textgreater{} [1] "9690 {-} running predictions"}
\CommentTok{\#\textgreater{} Warning in if (class(probability\_map) == "try{-}error") \{: the condition has}
\CommentTok{\#\textgreater{} length \textgreater{} 1 and only the first element will be used}
\CommentTok{\#\textgreater{} [1] "9690 {-} writing files"}
\NormalTok{m1}\SpecialCharTok{$}\NormalTok{Prob}\FloatTok{.2015} \OtherTok{=}\NormalTok{ m1}\SpecialCharTok{$}\NormalTok{Prob}
\NormalTok{m1}\SpecialCharTok{$}\NormalTok{Prob}\FloatTok{.2017} \OtherTok{=}\NormalTok{ m2}\SpecialCharTok{$}\NormalTok{Prob}
\NormalTok{m1}\SpecialCharTok{$}\NormalTok{Prob}\FloatTok{.2019} \OtherTok{=}\NormalTok{ m3}\SpecialCharTok{$}\NormalTok{Prob}
\end{Highlighting}
\end{Shaded}

We can compare predictions of the probability of occurrence of the
target species for two years next to each other by using:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pts }\OtherTok{=} \FunctionTok{list}\NormalTok{(}\StringTok{"sp.points"}\NormalTok{, }\FunctionTok{spTransform}\NormalTok{(occ.pnts[occ.pnts}\SpecialCharTok{$}\NormalTok{Atlas\_class}\SpecialCharTok{==}\DecValTok{1}\NormalTok{,], }\FunctionTok{CRS}\NormalTok{(}\StringTok{"+init=epsg:3035"}\NormalTok{)), }
           \AttributeTok{pch =} \StringTok{"+"}\NormalTok{, }\AttributeTok{col=}\StringTok{"black"}\NormalTok{)}
\FunctionTok{spplot}\NormalTok{(m1[,}\FunctionTok{c}\NormalTok{(}\StringTok{"Prob.2015"}\NormalTok{,}\StringTok{"Prob.2017"}\NormalTok{,}\StringTok{"Prob.2019"}\NormalTok{)], }\AttributeTok{col.regions=}\FunctionTok{rev}\NormalTok{(}\FunctionTok{bpy.colors}\NormalTok{()),}
  \AttributeTok{sp.layout =} \FunctionTok{list}\NormalTok{(pts),}
  \AttributeTok{main=}\StringTok{"Predictions Fagus Sylvatica"}\NormalTok{)}
\CommentTok{\#dev.off()}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{spatiotemporal-interpolation_files/figure-latex/predictions-fs-1} 

}

\caption{Predicted probability of occurrence for Fagus Sylvatica for three periods.}\label{fig:predictions-fs}
\end{figure}

In this case there seems to be no drastic changes in the distribution of
the target species through time, which is also expected because forest species distribution
change on a scale of 50 to 100 year and not on a scale of few years.
Some changes in distribution of the species, however, can be detected nevertheless.
These can be due to abrupt events such as pest-pandemics, fires, floods or landslides
or clear cutting of forests of course.

We can also plot the images using the \texttt{plotKML} package so that we can open and
visualize predictions also in Google Earth or similar:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(plotKML)}
\ControlFlowTok{for}\NormalTok{(j }\ControlFlowTok{in} \FunctionTok{c}\NormalTok{(}\DecValTok{2015}\NormalTok{,}\DecValTok{2017}\NormalTok{,}\DecValTok{2019}\NormalTok{))\{}
  \FunctionTok{kml}\NormalTok{(m1, }\AttributeTok{colour=}\NormalTok{m1}\SpecialCharTok{@}\NormalTok{data[,}\FunctionTok{paste0}\NormalTok{(}\StringTok{"Prob."}\NormalTok{, j)], }\AttributeTok{file.name=}\FunctionTok{paste0}\NormalTok{(}\StringTok{"prob."}\NormalTok{, j, }\StringTok{".kml"}\NormalTok{),}
      \AttributeTok{raster\_name =} \FunctionTok{paste0}\NormalTok{(}\StringTok{"prob."}\NormalTok{, j, }\StringTok{".png"}\NormalTok{),}
      \AttributeTok{colour\_scale =}\NormalTok{ SAGA\_pal[[}\StringTok{"SG\_COLORS\_YELLOW\_BLUE"}\NormalTok{]], }
      \AttributeTok{z.lim=}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{100}\NormalTok{),}
      \AttributeTok{TimeSpan.begin =} \FunctionTok{as.Date}\NormalTok{(}\FunctionTok{paste0}\NormalTok{(j, }\StringTok{"{-}01{-}01"}\NormalTok{)), }\AttributeTok{TimeSpan.end =} \FunctionTok{as.Date}\NormalTok{(}\FunctionTok{paste0}\NormalTok{(j, }\StringTok{"{-}12{-}31"}\NormalTok{)))}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

In this case we attach to each prediction also the \texttt{TimeSpan.begin} and \texttt{TimeSpan.end}
which means that Google Earth will recognize the temporal reference of the predictions.
Opening predictions in Google Earth allows us to do some interpretation of produced maps and also
analyze how much are the changes in vegetation cover connected with relief,
proximity to urban areas, possible fire / flood events and similar.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{./img/Fig_google_earth_prob_ts} 

}

\caption{Spacetime predictions of distribution of Fagus Sylvatica visualized as time-series data in Google Earth.}\label{fig:fagus-ge}
\end{figure}

\hypertarget{summary-notes}{%
\section{Summary notes}\label{summary-notes}}

In this tutorial we have reviewed some aspects of spatial and
spatiotemporal data and demonstrated how to use ML, specifically
Ensemble ML, to train spatiotemporal models and produce time-series of
predictions. We have also shown, using some synthetic and real-life datasets, how
incorrectly setting up training and cross-validation can lead to
over-fitting problems. This was done to help users realize that Machine Learning,
however trivial it might seem, is not a click-a-button process and solid knowledge
and understanding of advanced statistics (regression, hypothesis testing,
sampling and resampling, probability theory) is still required.

For spatiotemporal models, we recommend combining covariates that
can represent both long-term or accumulated effects of climate, together with
covariates that can represent daily to monthly oscillation of variables such as soil
moisture, temperatures and similar. During the design of the modeling system,
we highly recommend \textbf{trying to understand ecology and processes behind
variable of interest} first, then designing the modeling system to best reflect expert
knowledge. The example with RF over-fitting data in \citet{gasch2015spatio} shows how in-depth
understanding of the problem can help design modeling framework and prevent
from over-fitting problems and similar. \citet{witjes2021spatiotemporal} and \citet{Bonannella2022} describe a more comprehensive
framework for spatiotemporal ML which can even be run on large datasets.

Where time-series EO data exists, these can be also incorporated into the
mapping algorithm as shown with three case studies. For spacetime overlays we
recommend using Cloud-Optimized GeoTIFFs and the \texttt{terra} package \citep{hijmans2019spatial} which helps speed-up overlays. Other options for efficient overlay are the \texttt{stars} and \texttt{gdalcubes} package \citep{appel2019demand}.

Spatiotemporal datasets can be at the order of magnitude larger, hence
it is important, when implementing analysis of spatiotemporal data, to
consider \textbf{computing optimization}, which typically implies:

\begin{itemize}
\tightlist
\item
  Running operations in parallel;
\item
  Separating fine-tuning and parameter optimization (best to run on
  subset and save computing time) from predictions,
\item
  Using tiling systems to run overlay, predictions and visualizations,
\end{itemize}

Finally, we recommend following these generic steps to fit spatiotemporal models:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{-1}
\tightlist
\item
  Define target of interest and design the modeling framework by understanding
  ecology and processes behind variable of interest.\\
\item
  Prepare training (points) data and a data cube with all covariates
  referenced in spacetime.\\
\item
  Overlay points in spacetime, create a spatiotemporal
  regression-matrix.\\
\item
  Add seasonal components, fine-tune initial model, reduce complexity
  as much as possible, and produce production-ready spatiotemporal prediction model
  (usually using Ensemble Machine Learning).\\
\item
  Run mapping accuracy assessment and determine prediction uncertainty
  including the per pixel uncertainty.\\
\item
  Generate predictions in spacetime --- create time-series of
  predictions.\\
\item
  (optional) Run change-detection / trend analysis and try to detect
  main drivers of positive / negative trends \citep{witjes2021spatiotemporal}.\\
\item
  Deploy predictions as Cloud-Optimized GeoTIFF and produce the final
  report with mapping accuracy, variable importance.
\end{enumerate}

Ensemble ML framework we used here clearly offers many benefits, but it also comes
at a cost of at the order of magnitude higher computational load. Also
interpretation of such models can be a cumbersome as there a multiple
learners plus a meta-learner, so it often becomes difficult to
track-back individual relationship between variables. To help increase
confidence in the produced models, we recommend studying the \href{https://christophm.github.io/interpretable-ml-book/}{Interpretable Machine
Learning} methods
\citep{molnar2020interpretable}, running additional model diagnostics, and
intensively plotting data in space and spacetime and feature space.

Note that the \texttt{mlr} package is discontinued, so some of the examples
above might become unstable with time. You are advised instead to use
the new \href{https://mlr3.mlr-org.com/}{mlr3 package}.

\hypertarget{multi-scale-spatial-prediction-models}{%
\chapter{Multi-scale spatial prediction models}\label{multi-scale-spatial-prediction-models}}

You are reading the work-in-progress Spatial and spatiotemporal interpolation using Ensemble Machine Learning. This chapter is currently draft version, a peer-review publication is pending. You can find the polished first edition at \url{https://opengeohub.github.io/spatial-prediction-eml/}.

\hypertarget{rationale-for-multiscale-models}{%
\section{Rationale for multiscale models}\label{rationale-for-multiscale-models}}

In the previous examples we have shown how to fit spatial and spatiotemporal models
to generate predictions using multiple covariate layers. In practice spatial layers
used for predictive mapping could come and different spatial scales i.e.~they could
be represent different part of spatial variation. There are at least two scales of
spatial variation \citep{hengl2021african}:

\begin{itemize}
\tightlist
\item
  \textbf{Coarse scale} e.g.~representing effects of planetary climate;
\item
  \textbf{Fine scale} e.g.~representing meso-relief and local conditions;
\end{itemize}

In fact, we can imagine that spatial variation can probably be decomposed into different
scale components, as illustrated in plot below.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{./img/Fig_signal_decomposition} 

}

\caption{Decomposition of a signal of spatial variation into four components plus noise. Based on McBratney (1998).}\label{fig:decomposition-signal}
\end{figure}

The idea of modeling soil spatial variation at different scales can be traced back to the work of \citet{McBratney1998}.
That also suggests that we could produce predictions models of different components
of variation, then sum the components to produce ensemble prediction. The rationale
for this, in the case of large datasets, is that we can (a) significantly reduce size
of the data, (b) separate and better focus modeling based on the component of variation.

\hypertarget{fitting-and-predicting-with-multiscale-models}{%
\section{Fitting and predicting with multiscale models}\label{fitting-and-predicting-with-multiscale-models}}

In the next example we use EML to make spatial predictions using data-set with
two sets of covariates basically at different resolutions 250-m and 100-m. For
this we use the Edgeroi data-set \citep{malone2009mapping} used commonly in the soil
science to demonstrate 3D soil mapping of soil organic carbon (g/kg) based on
samples taken from diagnostic soil horizons (multiple depth intervals):

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(edgeroi)}
\NormalTok{edgeroi.sp }\OtherTok{\textless{}{-}}\NormalTok{ edgeroi}\SpecialCharTok{$}\NormalTok{sites}
\FunctionTok{coordinates}\NormalTok{(edgeroi.sp) }\OtherTok{\textless{}{-}} \ErrorTok{\textasciitilde{}}\NormalTok{ LONGDA94 }\SpecialCharTok{+}\NormalTok{ LATGDA94}
\FunctionTok{proj4string}\NormalTok{(edgeroi.sp) }\OtherTok{\textless{}{-}} \FunctionTok{CRS}\NormalTok{(}\StringTok{"+proj=longlat +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +no\_defs"}\NormalTok{)}
\NormalTok{edgeroi.sp }\OtherTok{\textless{}{-}} \FunctionTok{spTransform}\NormalTok{(edgeroi.sp, }\FunctionTok{CRS}\NormalTok{(}\StringTok{"+init=epsg:28355"}\NormalTok{))}
\NormalTok{out.file }\OtherTok{=} \FunctionTok{paste}\NormalTok{(}\FunctionTok{getwd}\NormalTok{(), }\StringTok{"output/edgeroi/edgeroi\_training\_points.gpkg"}\NormalTok{, }\AttributeTok{sep=}\StringTok{"/"}\NormalTok{)}
\CommentTok{\#if(!file.exists("out.file"))\{}
\CommentTok{\#  writeOGR(edgeroi.sp, out.file, layer="edgeroi\_training\_points", driver = "GPKG")}
\CommentTok{\#\}}
\end{Highlighting}
\end{Shaded}

We can fit two independent EML's using the two sets of covariates and then
produce final predictions by combining them. We will refer to the two models as
coarse and fine-scale models. The fine-scale models will often be much larger
datasets and require serious computing capacity.

\hypertarget{coarse-scale-model}{%
\section{Coarse-scale model}\label{coarse-scale-model}}

First we use the 250-m resolution covariates:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{load}\NormalTok{(}\StringTok{"input/edgeroi.grids.rda"}\NormalTok{)}
\FunctionTok{gridded}\NormalTok{(edgeroi.grids) }\OtherTok{\textless{}{-}} \ErrorTok{\textasciitilde{}}\NormalTok{x}\SpecialCharTok{+}\NormalTok{y}
\FunctionTok{proj4string}\NormalTok{(edgeroi.grids) }\OtherTok{\textless{}{-}} \FunctionTok{CRS}\NormalTok{(}\StringTok{"+init=epsg:28355"}\NormalTok{)}
\NormalTok{ov2 }\OtherTok{\textless{}{-}} \FunctionTok{over}\NormalTok{(edgeroi.sp, edgeroi.grids)}
\NormalTok{ov2}\SpecialCharTok{$}\NormalTok{SOURCEID }\OtherTok{\textless{}{-}}\NormalTok{ edgeroi.sp}\SpecialCharTok{$}\NormalTok{SOURCEID}
\NormalTok{ov2}\SpecialCharTok{$}\NormalTok{x }\OtherTok{=}\NormalTok{ edgeroi.sp}\SpecialCharTok{@}\NormalTok{coords[,}\DecValTok{1}\NormalTok{]}
\NormalTok{ov2}\SpecialCharTok{$}\NormalTok{y }\OtherTok{=}\NormalTok{ edgeroi.sp}\SpecialCharTok{@}\NormalTok{coords[,}\DecValTok{2}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

This is a 3D soil data set, so we also use the horizon \texttt{DEPTH} to explain distribution of SOC in soil:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{source}\NormalTok{(}\StringTok{"PSM\_functions.R"}\NormalTok{)}
\NormalTok{h2 }\OtherTok{\textless{}{-}} \FunctionTok{hor2xyd}\NormalTok{(edgeroi}\SpecialCharTok{$}\NormalTok{horizons)}
\DocumentationTok{\#\# regression matrix:}
\NormalTok{rm2 }\OtherTok{\textless{}{-}}\NormalTok{ plyr}\SpecialCharTok{::}\FunctionTok{join\_all}\NormalTok{(}\AttributeTok{dfs =} \FunctionTok{list}\NormalTok{(edgeroi}\SpecialCharTok{$}\NormalTok{sites, h2, ov2))}
\CommentTok{\#\textgreater{} Joining by: SOURCEID}
\CommentTok{\#\textgreater{} Joining by: SOURCEID}
\NormalTok{formulaStringP2 }\OtherTok{\textless{}{-}}\NormalTok{ ORCDRC }\SpecialCharTok{\textasciitilde{}}\NormalTok{ DEMSRT5}\SpecialCharTok{+}\NormalTok{TWISRT5}\SpecialCharTok{+}\NormalTok{EV1MOD5}\SpecialCharTok{+}\NormalTok{EV2MOD5}\SpecialCharTok{+}\NormalTok{EV3MOD5}\SpecialCharTok{+}\NormalTok{DEPTH}
\NormalTok{rmP2 }\OtherTok{\textless{}{-}}\NormalTok{ rm2[}\FunctionTok{complete.cases}\NormalTok{(rm2[,}\FunctionTok{all.vars}\NormalTok{(formulaStringP2)]),]}
\FunctionTok{str}\NormalTok{(rmP2[,}\FunctionTok{all.vars}\NormalTok{(formulaStringP2)])}
\CommentTok{\#\textgreater{} \textquotesingle{}data.frame\textquotesingle{}:    4972 obs. of  7 variables:}
\CommentTok{\#\textgreater{}  $ ORCDRC : num  8.5 7.3 5 4.7 4.7 ...}
\CommentTok{\#\textgreater{}  $ DEMSRT5: num  198 198 198 198 198 198 185 185 185 185 ...}
\CommentTok{\#\textgreater{}  $ TWISRT5: num  19.5 19.5 19.5 19.5 19.5 19.5 19.2 19.2 19.2 19.2 ...}
\CommentTok{\#\textgreater{}  $ EV1MOD5: num  1.14 1.14 1.14 1.14 1.14 1.14 {-}4.7 {-}4.7 {-}4.7 {-}4.7 ...}
\CommentTok{\#\textgreater{}  $ EV2MOD5: num  1.62 1.62 1.62 1.62 1.62 1.62 3.46 3.46 3.46 3.46 ...}
\CommentTok{\#\textgreater{}  $ EV3MOD5: num  {-}5.74 {-}5.74 {-}5.74 {-}5.74 {-}5.74 {-}5.74 0.01 0.01 0.01 0.01 ...}
\CommentTok{\#\textgreater{}  $ DEPTH  : num  11.5 17.5 26 55 80 ...}
\end{Highlighting}
\end{Shaded}

We can now fit an EML directly by using the derived regression matrix:

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{if}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{exists}\NormalTok{(}\StringTok{"m.oc"}\NormalTok{))\{}
\NormalTok{  m.oc }\OtherTok{=} \FunctionTok{train.spLearner.matrix}\NormalTok{(rmP2, formulaStringP2, edgeroi.grids, }
                        \AttributeTok{parallel=}\ConstantTok{FALSE}\NormalTok{, }\AttributeTok{cov.model=}\StringTok{"nugget"}\NormalTok{, }\AttributeTok{cell.size=}\DecValTok{1000}\NormalTok{)}
\NormalTok{\}}
\CommentTok{\#\textgreater{} as.geodata: 4655 replicated data locations found. }
\CommentTok{\#\textgreater{}  Consider using jitterDupCoords() for jittering replicated locations. }
\CommentTok{\#\textgreater{} }\AlertTok{WARNING}\CommentTok{: there are data at coincident or very closed locations, some of the geoR\textquotesingle{}s functions may not work.}
\CommentTok{\#\textgreater{}  Use function dup.coords() to locate duplicated coordinates.}
\CommentTok{\#\textgreater{}  Consider using jitterDupCoords() for jittering replicated locations }
\CommentTok{\#\textgreater{} \# weights:  25}
\CommentTok{\#\textgreater{} initial  value 253895.808438 }
\CommentTok{\#\textgreater{} iter  10 value 131424.395513}
\CommentTok{\#\textgreater{} iter  20 value 92375.545449}
\CommentTok{\#\textgreater{} iter  30 value 88023.497878}
\CommentTok{\#\textgreater{} iter  40 value 78161.622563}
\CommentTok{\#\textgreater{} iter  50 value 71869.588437}
\CommentTok{\#\textgreater{} iter  60 value 69482.655270}
\CommentTok{\#\textgreater{} iter  70 value 68642.175713}
\CommentTok{\#\textgreater{} iter  80 value 68405.024865}
\CommentTok{\#\textgreater{} iter  90 value 68402.034341}
\CommentTok{\#\textgreater{} final  value 68402.000242 }
\CommentTok{\#\textgreater{} converged}
\CommentTok{\#\textgreater{} \# weights:  25}
\CommentTok{\#\textgreater{} initial  value 254790.970425 }
\CommentTok{\#\textgreater{} final  value 136782.219163 }
\CommentTok{\#\textgreater{} converged}
\CommentTok{\#\textgreater{} \# weights:  25}
\CommentTok{\#\textgreater{} initial  value 285010.126650 }
\CommentTok{\#\textgreater{} final  value 135478.529982 }
\CommentTok{\#\textgreater{} converged}
\CommentTok{\#\textgreater{} \# weights:  25}
\CommentTok{\#\textgreater{} initial  value 253832.562811 }
\CommentTok{\#\textgreater{} final  value 137484.280876 }
\CommentTok{\#\textgreater{} converged}
\CommentTok{\#\textgreater{} \# weights:  25}
\CommentTok{\#\textgreater{} initial  value 254385.881547 }
\CommentTok{\#\textgreater{} iter  10 value 98551.221418}
\CommentTok{\#\textgreater{} iter  20 value 94579.214923}
\CommentTok{\#\textgreater{} iter  30 value 93473.664614}
\CommentTok{\#\textgreater{} iter  40 value 93169.177514}
\CommentTok{\#\textgreater{} iter  50 value 93139.660457}
\CommentTok{\#\textgreater{} iter  60 value 93111.617240}
\CommentTok{\#\textgreater{} iter  70 value 93111.256287}
\CommentTok{\#\textgreater{} iter  80 value 93109.156591}
\CommentTok{\#\textgreater{} iter  90 value 93099.925818}
\CommentTok{\#\textgreater{} iter 100 value 93021.880998}
\CommentTok{\#\textgreater{} final  value 93021.880998 }
\CommentTok{\#\textgreater{} stopped after 100 iterations}
\CommentTok{\#\textgreater{} \# weights:  25}
\CommentTok{\#\textgreater{} initial  value 233465.782262 }
\CommentTok{\#\textgreater{} final  value 134647.206038 }
\CommentTok{\#\textgreater{} converged}
\CommentTok{\#\textgreater{} \# weights:  25}
\CommentTok{\#\textgreater{} initial  value 246624.689888 }
\CommentTok{\#\textgreater{} final  value 138702.343415 }
\CommentTok{\#\textgreater{} converged}
\CommentTok{\#\textgreater{} \# weights:  25}
\CommentTok{\#\textgreater{} initial  value 241227.341802 }
\CommentTok{\#\textgreater{} final  value 138599.168021 }
\CommentTok{\#\textgreater{} converged}
\CommentTok{\#\textgreater{} \# weights:  25}
\CommentTok{\#\textgreater{} initial  value 245735.599010 }
\CommentTok{\#\textgreater{} final  value 131152.446689 }
\CommentTok{\#\textgreater{} converged}
\CommentTok{\#\textgreater{} \# weights:  25}
\CommentTok{\#\textgreater{} initial  value 258267.657849 }
\CommentTok{\#\textgreater{} iter  10 value 97368.003255}
\CommentTok{\#\textgreater{} iter  20 value 91058.331259}
\CommentTok{\#\textgreater{} iter  30 value 88735.472097}
\CommentTok{\#\textgreater{} iter  40 value 78495.790097}
\CommentTok{\#\textgreater{} iter  50 value 72384.348608}
\CommentTok{\#\textgreater{} iter  60 value 69221.579541}
\CommentTok{\#\textgreater{} iter  70 value 68248.107158}
\CommentTok{\#\textgreater{} iter  80 value 68073.306877}
\CommentTok{\#\textgreater{} iter  80 value 68073.306456}
\CommentTok{\#\textgreater{} iter  90 value 68072.812887}
\CommentTok{\#\textgreater{} iter  90 value 68072.812254}
\CommentTok{\#\textgreater{} final  value 68072.784171 }
\CommentTok{\#\textgreater{} converged}
\CommentTok{\#\textgreater{} \# weights:  25}
\CommentTok{\#\textgreater{} initial  value 268786.613045 }
\CommentTok{\#\textgreater{} iter  10 value 128937.282143}
\CommentTok{\#\textgreater{} iter  20 value 105536.706972}
\CommentTok{\#\textgreater{} iter  30 value 100970.717402}
\CommentTok{\#\textgreater{} iter  40 value 89676.958144}
\CommentTok{\#\textgreater{} iter  50 value 80499.715395}
\CommentTok{\#\textgreater{} iter  60 value 76269.748745}
\CommentTok{\#\textgreater{} iter  70 value 74645.990665}
\CommentTok{\#\textgreater{} iter  80 value 74429.114194}
\CommentTok{\#\textgreater{} iter  90 value 74041.488710}
\CommentTok{\#\textgreater{} iter 100 value 73877.378810}
\CommentTok{\#\textgreater{} final  value 73877.378810 }
\CommentTok{\#\textgreater{} stopped after 100 iterations}
\end{Highlighting}
\end{Shaded}

The \textbf{geoR} package here reports problems as the data set is 3D and hence there are spatial
duplicates. We can ignore this problem and use the pre-defined cell size of 1-km
for spatial blocking, although in theory one can also fit 3D variograms and then
determine blocking parameter using training data.

The results show that the EML model is significant:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(m.oc}\SpecialCharTok{@}\NormalTok{spModel}\SpecialCharTok{$}\NormalTok{learner.model}\SpecialCharTok{$}\NormalTok{super.model}\SpecialCharTok{$}\NormalTok{learner.model)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} stats::lm(formula = f, data = d)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residuals:}
\CommentTok{\#\textgreater{}     Min      1Q  Median      3Q     Max }
\CommentTok{\#\textgreater{} {-}17.668  {-}0.984  {-}0.066   0.711  64.291 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}               Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept)    0.11730    0.18365   0.639  0.52302    }
\CommentTok{\#\textgreater{} regr.ranger    1.12706    0.02474  45.553  \textless{} 2e{-}16 ***}
\CommentTok{\#\textgreater{} regr.xgboost  {-}0.37833    0.07448  {-}5.080 3.92e{-}07 ***}
\CommentTok{\#\textgreater{} regr.nnet     {-}0.04227    0.02399  {-}1.762  0.07808 .  }
\CommentTok{\#\textgreater{} regr.ksvm      0.08299    0.02900   2.861  0.00423 ** }
\CommentTok{\#\textgreater{} regr.cvglmnet {-}0.05140    0.03879  {-}1.325  0.18525    }
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residual standard error: 3.06 on 4966 degrees of freedom}
\CommentTok{\#\textgreater{} Multiple R{-}squared:  0.6938, Adjusted R{-}squared:  0.6935 }
\CommentTok{\#\textgreater{} F{-}statistic:  2250 on 5 and 4966 DF,  p{-}value: \textless{} 2.2e{-}16}
\end{Highlighting}
\end{Shaded}

We can now predict values at e.g.~5-cm depth by adding a dummy spatial layer with all fixed values:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{out.tif }\OtherTok{=} \StringTok{"output/edgeroi/pred\_oc\_250m.tif"}
\NormalTok{edgeroi.grids}\SpecialCharTok{$}\NormalTok{DEPTH }\OtherTok{\textless{}{-}} \DecValTok{5}
\ControlFlowTok{if}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{exists}\NormalTok{(}\StringTok{"edgeroi.oc"}\NormalTok{))\{}
\NormalTok{  edgeroi.oc }\OtherTok{=} \FunctionTok{predict}\NormalTok{(m.oc, edgeroi.grids[,m.oc}\SpecialCharTok{@}\NormalTok{spModel}\SpecialCharTok{$}\NormalTok{features])}
\NormalTok{\}}
\CommentTok{\#\textgreater{} Predicting values using \textquotesingle{}getStackedBaseLearnerPredictions\textquotesingle{}...TRUE}
\CommentTok{\#\textgreater{} Deriving model errors using forestError package...TRUE}
\ControlFlowTok{if}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{file.exists}\NormalTok{(out.tif))\{}
  \FunctionTok{writeGDAL}\NormalTok{(edgeroi.oc}\SpecialCharTok{$}\NormalTok{pred[}\StringTok{"response"}\NormalTok{], out.tif, }
            \AttributeTok{options =} \FunctionTok{c}\NormalTok{(}\StringTok{"COMPRESS=DEFLATE"}\NormalTok{))}
  \FunctionTok{writeGDAL}\NormalTok{(edgeroi.oc}\SpecialCharTok{$}\NormalTok{pred[}\StringTok{"model.error"}\NormalTok{], }\StringTok{"output/edgeroi/pred\_oc\_250m\_pe.tif"}\NormalTok{, }
            \AttributeTok{options =} \FunctionTok{c}\NormalTok{(}\StringTok{"COMPRESS=DEFLATE"}\NormalTok{))}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

which shows the following:

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{multiscale-models_files/figure-latex/map-oc250m-1} 

}

\caption{Predicted SOC content using 250-m covariates.}\label{fig:map-oc250m}
\end{figure}

The average prediction error in the map is somewhat higher than the average error from the model fitting:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(edgeroi.oc}\SpecialCharTok{$}\NormalTok{pred}\SpecialCharTok{$}\NormalTok{model.error)}
\CommentTok{\#\textgreater{}    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. }
\CommentTok{\#\textgreater{}   2.323   4.710   6.340   6.929   9.257  15.338}
\end{Highlighting}
\end{Shaded}

This is because we are predicting the top-soil SOC, which is exponentially higher at the soil surface and hence average model errors for top soil should be slightly larger than the mean error for the whole soil.

\hypertarget{fine-scale-model}{%
\section{Fine-scale model}\label{fine-scale-model}}

We can now fit the fine-scale model independently from the coarse-scale model
using the 100-m resolution covariates. In this case the 100-m covariates are
based on Landsat 8 and gamma radiometrics images (see \texttt{?edgeroi} for more details):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{edgeroi.grids100 }\OtherTok{=} \FunctionTok{readRDS}\NormalTok{(}\StringTok{"input/edgeroi.grids.100m.rds"}\NormalTok{)}
\CommentTok{\#gridded(edgeroi.grids100) \textless{}{-} \textasciitilde{}x+y}
\CommentTok{\#proj4string(edgeroi.grids100) \textless{}{-} CRS("+init=epsg:28355")}
\NormalTok{ovF }\OtherTok{\textless{}{-}} \FunctionTok{over}\NormalTok{(edgeroi.sp, edgeroi.grids100)}
\NormalTok{ovF}\SpecialCharTok{$}\NormalTok{SOURCEID }\OtherTok{\textless{}{-}}\NormalTok{ edgeroi.sp}\SpecialCharTok{$}\NormalTok{SOURCEID}
\NormalTok{ovF}\SpecialCharTok{$}\NormalTok{x }\OtherTok{=}\NormalTok{ edgeroi.sp}\SpecialCharTok{@}\NormalTok{coords[,}\DecValTok{1}\NormalTok{]}
\NormalTok{ovF}\SpecialCharTok{$}\NormalTok{y }\OtherTok{=}\NormalTok{ edgeroi.sp}\SpecialCharTok{@}\NormalTok{coords[,}\DecValTok{2}\NormalTok{]}
\NormalTok{rmF }\OtherTok{\textless{}{-}}\NormalTok{ plyr}\SpecialCharTok{::}\FunctionTok{join\_all}\NormalTok{(}\AttributeTok{dfs =} \FunctionTok{list}\NormalTok{(edgeroi}\SpecialCharTok{$}\NormalTok{sites, h2, ovF))}
\CommentTok{\#\textgreater{} Joining by: SOURCEID}
\CommentTok{\#\textgreater{} Joining by: SOURCEID}
\NormalTok{formulaStringPF }\OtherTok{\textless{}{-}}\NormalTok{ ORCDRC }\SpecialCharTok{\textasciitilde{}}\NormalTok{ MVBSRT6}\SpecialCharTok{+}\NormalTok{TI1LAN6}\SpecialCharTok{+}\NormalTok{TI2LAN6}\SpecialCharTok{+}\NormalTok{PCKGAD6}\SpecialCharTok{+}\NormalTok{RUTGAD6}\SpecialCharTok{+}\NormalTok{PCTGAD6}\SpecialCharTok{+}\NormalTok{DEPTH}
\NormalTok{rmPF }\OtherTok{\textless{}{-}}\NormalTok{ rmF[}\FunctionTok{complete.cases}\NormalTok{(rmF[,}\FunctionTok{all.vars}\NormalTok{(formulaStringPF)]),]}
\FunctionTok{str}\NormalTok{(rmPF[,}\FunctionTok{all.vars}\NormalTok{(formulaStringPF)])}
\CommentTok{\#\textgreater{} \textquotesingle{}data.frame\textquotesingle{}:    5001 obs. of  8 variables:}
\CommentTok{\#\textgreater{}  $ ORCDRC : num  8.5 7.3 5 4.7 4.7 ...}
\CommentTok{\#\textgreater{}  $ MVBSRT6: num  5.97 5.97 5.97 5.97 5.97 5.97 6.7 6.7 6.7 6.7 ...}
\CommentTok{\#\textgreater{}  $ TI1LAN6: num  31.8 31.8 31.8 31.8 31.8 31.8 14.3 14.3 14.3 14.3 ...}
\CommentTok{\#\textgreater{}  $ TI2LAN6: num  32.9 32.9 32.9 32.9 32.9 32.9 22.1 22.1 22.1 22.1 ...}
\CommentTok{\#\textgreater{}  $ PCKGAD6: num  1.39 1.39 1.39 1.39 1.39 1.39 1.06 1.06 1.06 1.06 ...}
\CommentTok{\#\textgreater{}  $ RUTGAD6: num  0.14 0.14 0.14 0.14 0.14 0.14 0.16 0.16 0.16 0.16 ...}
\CommentTok{\#\textgreater{}  $ PCTGAD6: num  7.82 7.82 7.82 7.82 7.82 7.82 6.48 6.48 6.48 6.48 ...}
\CommentTok{\#\textgreater{}  $ DEPTH  : num  11.5 17.5 26 55 80 ...}
\end{Highlighting}
\end{Shaded}

We fit the 2nd fine-scale model:

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{if}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{exists}\NormalTok{(}\StringTok{"m.ocF"}\NormalTok{))\{}
\NormalTok{  m.ocF }\OtherTok{=} \FunctionTok{train.spLearner.matrix}\NormalTok{(rmPF, formulaStringPF, edgeroi.grids100, }
                        \AttributeTok{parallel=}\ConstantTok{FALSE}\NormalTok{, }\AttributeTok{cov.model=}\StringTok{"nugget"}\NormalTok{, }\AttributeTok{cell.size=}\DecValTok{1000}\NormalTok{)}
\NormalTok{\}}
\CommentTok{\#\textgreater{} as.geodata: 4655 replicated data locations found. }
\CommentTok{\#\textgreater{}  Consider using jitterDupCoords() for jittering replicated locations. }
\CommentTok{\#\textgreater{} }\AlertTok{WARNING}\CommentTok{: there are data at coincident or very closed locations, some of the geoR\textquotesingle{}s functions may not work.}
\CommentTok{\#\textgreater{}  Use function dup.coords() to locate duplicated coordinates.}
\CommentTok{\#\textgreater{}  Consider using jitterDupCoords() for jittering replicated locations }
\CommentTok{\#\textgreater{} \# weights:  28}
\CommentTok{\#\textgreater{} initial  value 259952.010839 }
\CommentTok{\#\textgreater{} iter  10 value 101252.397395}
\CommentTok{\#\textgreater{} iter  20 value 92849.746212}
\CommentTok{\#\textgreater{} iter  30 value 84138.890538}
\CommentTok{\#\textgreater{} iter  40 value 81171.674547}
\CommentTok{\#\textgreater{} iter  50 value 80244.316068}
\CommentTok{\#\textgreater{} iter  60 value 79942.793909}
\CommentTok{\#\textgreater{} iter  70 value 79368.296567}
\CommentTok{\#\textgreater{} iter  80 value 78323.201957}
\CommentTok{\#\textgreater{} iter  90 value 77181.404075}
\CommentTok{\#\textgreater{} iter 100 value 76708.190347}
\CommentTok{\#\textgreater{} final  value 76708.190347 }
\CommentTok{\#\textgreater{} stopped after 100 iterations}
\CommentTok{\#\textgreater{} \# weights:  28}
\CommentTok{\#\textgreater{} initial  value 226902.901029 }
\CommentTok{\#\textgreater{} iter  10 value 134520.116202}
\CommentTok{\#\textgreater{} iter  20 value 106665.533239}
\CommentTok{\#\textgreater{} iter  30 value 100456.523529}
\CommentTok{\#\textgreater{} iter  40 value 94907.032527}
\CommentTok{\#\textgreater{} iter  50 value 94598.860459}
\CommentTok{\#\textgreater{} iter  60 value 94311.934401}
\CommentTok{\#\textgreater{} iter  70 value 93110.357174}
\CommentTok{\#\textgreater{} iter  80 value 92843.643684}
\CommentTok{\#\textgreater{} iter  90 value 92584.240506}
\CommentTok{\#\textgreater{} iter 100 value 92181.626754}
\CommentTok{\#\textgreater{} final  value 92181.626754 }
\CommentTok{\#\textgreater{} stopped after 100 iterations}
\CommentTok{\#\textgreater{} \# weights:  28}
\CommentTok{\#\textgreater{} initial  value 234725.323531 }
\CommentTok{\#\textgreater{} iter  10 value 99738.688037}
\CommentTok{\#\textgreater{} iter  20 value 95202.777671}
\CommentTok{\#\textgreater{} iter  30 value 93332.714310}
\CommentTok{\#\textgreater{} iter  40 value 84502.258499}
\CommentTok{\#\textgreater{} iter  50 value 81245.631274}
\CommentTok{\#\textgreater{} iter  60 value 80530.199169}
\CommentTok{\#\textgreater{} iter  70 value 79322.812977}
\CommentTok{\#\textgreater{} iter  80 value 78753.418715}
\CommentTok{\#\textgreater{} iter  90 value 78202.739233}
\CommentTok{\#\textgreater{} iter 100 value 76867.515069}
\CommentTok{\#\textgreater{} final  value 76867.515069 }
\CommentTok{\#\textgreater{} stopped after 100 iterations}
\CommentTok{\#\textgreater{} \# weights:  28}
\CommentTok{\#\textgreater{} initial  value 264624.170952 }
\CommentTok{\#\textgreater{} iter  10 value 101566.765280}
\CommentTok{\#\textgreater{} iter  20 value 93105.271953}
\CommentTok{\#\textgreater{} iter  30 value 79221.563953}
\CommentTok{\#\textgreater{} iter  40 value 75437.096559}
\CommentTok{\#\textgreater{} iter  50 value 74819.981899}
\CommentTok{\#\textgreater{} iter  60 value 74258.787761}
\CommentTok{\#\textgreater{} iter  70 value 72481.383976}
\CommentTok{\#\textgreater{} iter  80 value 71415.613349}
\CommentTok{\#\textgreater{} iter  90 value 69310.427661}
\CommentTok{\#\textgreater{} iter 100 value 66134.495814}
\CommentTok{\#\textgreater{} final  value 66134.495814 }
\CommentTok{\#\textgreater{} stopped after 100 iterations}
\CommentTok{\#\textgreater{} \# weights:  28}
\CommentTok{\#\textgreater{} initial  value 269334.603789 }
\CommentTok{\#\textgreater{} iter  10 value 114589.803267}
\CommentTok{\#\textgreater{} iter  20 value 98585.812766}
\CommentTok{\#\textgreater{} iter  30 value 95611.345448}
\CommentTok{\#\textgreater{} iter  40 value 94119.323815}
\CommentTok{\#\textgreater{} iter  50 value 92069.210575}
\CommentTok{\#\textgreater{} iter  60 value 90209.532038}
\CommentTok{\#\textgreater{} iter  70 value 86238.674926}
\CommentTok{\#\textgreater{} iter  80 value 82014.171622}
\CommentTok{\#\textgreater{} iter  90 value 78106.825699}
\CommentTok{\#\textgreater{} iter 100 value 76099.552544}
\CommentTok{\#\textgreater{} final  value 76099.552544 }
\CommentTok{\#\textgreater{} stopped after 100 iterations}
\CommentTok{\#\textgreater{} \# weights:  28}
\CommentTok{\#\textgreater{} initial  value 259884.878897 }
\CommentTok{\#\textgreater{} iter  10 value 103726.379375}
\CommentTok{\#\textgreater{} iter  20 value 94570.752266}
\CommentTok{\#\textgreater{} iter  30 value 76708.677450}
\CommentTok{\#\textgreater{} iter  40 value 73787.991288}
\CommentTok{\#\textgreater{} iter  50 value 73126.652279}
\CommentTok{\#\textgreater{} iter  60 value 72979.797218}
\CommentTok{\#\textgreater{} iter  70 value 72965.952547}
\CommentTok{\#\textgreater{} iter  80 value 72468.280591}
\CommentTok{\#\textgreater{} iter  90 value 72359.464672}
\CommentTok{\#\textgreater{} iter 100 value 71873.895760}
\CommentTok{\#\textgreater{} final  value 71873.895760 }
\CommentTok{\#\textgreater{} stopped after 100 iterations}
\CommentTok{\#\textgreater{} \# weights:  28}
\CommentTok{\#\textgreater{} initial  value 285193.065615 }
\CommentTok{\#\textgreater{} iter  10 value 102340.204856}
\CommentTok{\#\textgreater{} iter  20 value 89532.696472}
\CommentTok{\#\textgreater{} iter  30 value 83391.890007}
\CommentTok{\#\textgreater{} iter  40 value 79203.819730}
\CommentTok{\#\textgreater{} iter  50 value 75520.520094}
\CommentTok{\#\textgreater{} iter  60 value 71641.894828}
\CommentTok{\#\textgreater{} iter  70 value 66583.248718}
\CommentTok{\#\textgreater{} iter  80 value 64316.930097}
\CommentTok{\#\textgreater{} iter  90 value 64067.020769}
\CommentTok{\#\textgreater{} iter 100 value 63723.399424}
\CommentTok{\#\textgreater{} final  value 63723.399424 }
\CommentTok{\#\textgreater{} stopped after 100 iterations}
\CommentTok{\#\textgreater{} \# weights:  28}
\CommentTok{\#\textgreater{} initial  value 209074.598360 }
\CommentTok{\#\textgreater{} iter  10 value 92158.672417}
\CommentTok{\#\textgreater{} iter  20 value 88477.518932}
\CommentTok{\#\textgreater{} iter  30 value 83611.323463}
\CommentTok{\#\textgreater{} iter  40 value 80398.938707}
\CommentTok{\#\textgreater{} iter  50 value 75639.793693}
\CommentTok{\#\textgreater{} iter  60 value 72497.710902}
\CommentTok{\#\textgreater{} iter  70 value 68574.884188}
\CommentTok{\#\textgreater{} iter  80 value 66515.331380}
\CommentTok{\#\textgreater{} iter  90 value 64170.821850}
\CommentTok{\#\textgreater{} iter 100 value 63336.087094}
\CommentTok{\#\textgreater{} final  value 63336.087094 }
\CommentTok{\#\textgreater{} stopped after 100 iterations}
\CommentTok{\#\textgreater{} \# weights:  28}
\CommentTok{\#\textgreater{} initial  value 264913.914581 }
\CommentTok{\#\textgreater{} iter  10 value 90799.203779}
\CommentTok{\#\textgreater{} iter  20 value 83326.307020}
\CommentTok{\#\textgreater{} iter  30 value 71495.281328}
\CommentTok{\#\textgreater{} iter  40 value 66078.704046}
\CommentTok{\#\textgreater{} iter  50 value 64850.594409}
\CommentTok{\#\textgreater{} iter  60 value 64005.390526}
\CommentTok{\#\textgreater{} iter  70 value 63563.194716}
\CommentTok{\#\textgreater{} iter  80 value 62770.259605}
\CommentTok{\#\textgreater{} iter  90 value 62084.800044}
\CommentTok{\#\textgreater{} iter 100 value 60518.371937}
\CommentTok{\#\textgreater{} final  value 60518.371937 }
\CommentTok{\#\textgreater{} stopped after 100 iterations}
\CommentTok{\#\textgreater{} \# weights:  28}
\CommentTok{\#\textgreater{} initial  value 254133.236100 }
\CommentTok{\#\textgreater{} iter  10 value 117010.387846}
\CommentTok{\#\textgreater{} iter  20 value 96496.685965}
\CommentTok{\#\textgreater{} iter  30 value 81754.875719}
\CommentTok{\#\textgreater{} iter  40 value 80702.591180}
\CommentTok{\#\textgreater{} iter  50 value 79637.660106}
\CommentTok{\#\textgreater{} iter  60 value 77195.047203}
\CommentTok{\#\textgreater{} iter  70 value 74660.187056}
\CommentTok{\#\textgreater{} iter  80 value 70931.471910}
\CommentTok{\#\textgreater{} iter  90 value 69014.578845}
\CommentTok{\#\textgreater{} iter 100 value 68409.495245}
\CommentTok{\#\textgreater{} final  value 68409.495245 }
\CommentTok{\#\textgreater{} stopped after 100 iterations}
\CommentTok{\#\textgreater{} \# weights:  28}
\CommentTok{\#\textgreater{} initial  value 325116.468080 }
\CommentTok{\#\textgreater{} iter  10 value 120203.210463}
\CommentTok{\#\textgreater{} iter  20 value 115964.153279}
\CommentTok{\#\textgreater{} iter  30 value 90105.298970}
\CommentTok{\#\textgreater{} iter  40 value 79437.548718}
\CommentTok{\#\textgreater{} iter  50 value 70168.831437}
\CommentTok{\#\textgreater{} iter  60 value 69050.767980}
\CommentTok{\#\textgreater{} iter  70 value 68627.297281}
\CommentTok{\#\textgreater{} iter  80 value 68156.983626}
\CommentTok{\#\textgreater{} iter  90 value 67374.568621}
\CommentTok{\#\textgreater{} iter 100 value 67164.637038}
\CommentTok{\#\textgreater{} final  value 67164.637038 }
\CommentTok{\#\textgreater{} stopped after 100 iterations}
\FunctionTok{summary}\NormalTok{(m.ocF}\SpecialCharTok{@}\NormalTok{spModel}\SpecialCharTok{$}\NormalTok{learner.model}\SpecialCharTok{$}\NormalTok{super.model}\SpecialCharTok{$}\NormalTok{learner.model)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} stats::lm(formula = f, data = d)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residuals:}
\CommentTok{\#\textgreater{}     Min      1Q  Median      3Q     Max }
\CommentTok{\#\textgreater{} {-}19.222  {-}0.952  {-}0.047   0.726  61.162 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}               Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept)   {-}0.47135    0.11510  {-}4.095 4.29e{-}05 ***}
\CommentTok{\#\textgreater{} regr.ranger    1.10204    0.02467  44.667  \textless{} 2e{-}16 ***}
\CommentTok{\#\textgreater{} regr.xgboost   0.03329    0.07566   0.440    0.660    }
\CommentTok{\#\textgreater{} regr.nnet      0.03135    0.02196   1.428    0.153    }
\CommentTok{\#\textgreater{} regr.ksvm     {-}0.02556    0.02965  {-}0.862    0.389    }
\CommentTok{\#\textgreater{} regr.cvglmnet {-}0.03575    0.02887  {-}1.238    0.216    }
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residual standard error: 2.967 on 4995 degrees of freedom}
\CommentTok{\#\textgreater{} Multiple R{-}squared:  0.7146, Adjusted R{-}squared:  0.7143 }
\CommentTok{\#\textgreater{} F{-}statistic:  2501 on 5 and 4995 DF,  p{-}value: \textless{} 2.2e{-}16}
\end{Highlighting}
\end{Shaded}

which shows that the 100-m resolution covariates help make even more accurate
predictions with R-square about 0.7. We can also make predictions at 5-cm depth
by using (note: this takes almost 6x more time to compute predictions than for
250-m resolution data):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{edgeroi.grids100}\SpecialCharTok{$}\NormalTok{DEPTH }\OtherTok{\textless{}{-}} \DecValTok{5}
\NormalTok{sel.grid }\OtherTok{=} \FunctionTok{complete.cases}\NormalTok{(edgeroi.grids100}\SpecialCharTok{@}\NormalTok{data[,m.ocF}\SpecialCharTok{@}\NormalTok{spModel}\SpecialCharTok{$}\NormalTok{features])}
\ControlFlowTok{if}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{exists}\NormalTok{(}\StringTok{"edgeroi.ocF"}\NormalTok{))\{}
\NormalTok{  edgeroi.ocF }\OtherTok{=} \FunctionTok{predict}\NormalTok{(m.ocF, edgeroi.grids100[sel.grid, m.ocF}\SpecialCharTok{@}\NormalTok{spModel}\SpecialCharTok{$}\NormalTok{features])}
\NormalTok{\}}
\CommentTok{\#\textgreater{} Predicting values using \textquotesingle{}getStackedBaseLearnerPredictions\textquotesingle{}...TRUE}
\CommentTok{\#\textgreater{} Deriving model errors using forestError package...TRUE}
\NormalTok{out.tif }\OtherTok{=} \StringTok{"output/edgeroi/pred\_oc\_100m.tif"}
\ControlFlowTok{if}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{file.exists}\NormalTok{(out.tif))\{}
  \FunctionTok{writeGDAL}\NormalTok{(edgeroi.ocF}\SpecialCharTok{$}\NormalTok{pred[}\StringTok{"response"}\NormalTok{], out.tif, }\AttributeTok{options =} \FunctionTok{c}\NormalTok{(}\StringTok{"COMPRESS=DEFLATE"}\NormalTok{))}
  \FunctionTok{writeGDAL}\NormalTok{(edgeroi.ocF}\SpecialCharTok{$}\NormalTok{pred[}\StringTok{"model.error"}\NormalTok{], }\StringTok{"output/edgeroi/pred\_oc\_100m\_pe.tif"}\NormalTok{, }\AttributeTok{options =} \FunctionTok{c}\NormalTok{(}\StringTok{"COMPRESS=DEFLATE"}\NormalTok{))}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

which shows the following:

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{multiscale-models_files/figure-latex/map-oc100m-1} 

}

\caption{Predicted SOC content using 100-m covariates.}\label{fig:map-oc100m}
\end{figure}

\hypertarget{merging-multi-scale-predictions}{%
\section{Merging multi-scale predictions}\label{merging-multi-scale-predictions}}

If we compare the coarse scale and fine scale predictions we see:

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{./img/ebergotzen_two_scale_model_ensemble} 

}

\caption{Coarse-scale and fine-scale predictions of soil organic carbon at 5-cm depth for the Edgeroi study area.}\label{fig:two-scale}
\end{figure}

Overall there is a match between general patterns but there are also differences locally. This is to expect as the two models are fitted independently using completely different covariates. We can merge the two predictions and produce the final ensemble prediction by using the following principles:

\begin{itemize}
\tightlist
\item
  User prediction errors per pixel as weights so that more accurate predictions get higher weights,\\
\item
  Derive propagated error using the pooled variance based on individual predictions and errors,
\end{itemize}

Before we run this operation, we need to downscale the maps to the same grid, best using Cubic-splines in GDAL:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{edgeroi.grids100}\SpecialCharTok{@}\NormalTok{bbox}
\CommentTok{\#\textgreater{}       min     max}
\CommentTok{\#\textgreater{} x  741400  789000}
\CommentTok{\#\textgreater{} y 6646000 6678100}
\NormalTok{outD.file }\OtherTok{=} \StringTok{"output/edgeroi/pred\_oc\_250m\_100m.tif"}
\ControlFlowTok{if}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{file.exists}\NormalTok{(outD.file))\{}
  \FunctionTok{system}\NormalTok{(}\FunctionTok{paste0}\NormalTok{(}\StringTok{\textquotesingle{}gdalwarp output/edgeroi/pred\_oc\_250m.tif \textquotesingle{}}\NormalTok{, outD.file,  }
         \StringTok{\textquotesingle{} {-}r }\SpecialCharTok{\textbackslash{}"}\StringTok{cubicspline}\SpecialCharTok{\textbackslash{}"}\StringTok{ {-}te 741400 6646000 789000 6678100 {-}tr 100 100 {-}overwrite\textquotesingle{}}\NormalTok{))}
  \FunctionTok{system}\NormalTok{(}\FunctionTok{paste0}\NormalTok{(}\StringTok{\textquotesingle{}gdalwarp output/edgeroi/pred\_oc\_250m\_pe.tif output/edgeroi/pred\_oc\_250m\_100m\_pe.tif\textquotesingle{}}\NormalTok{,}
         \StringTok{\textquotesingle{} {-}r }\SpecialCharTok{\textbackslash{}"}\StringTok{cubicspline}\SpecialCharTok{\textbackslash{}"}\StringTok{ {-}te 741400 6646000 789000 6678100 {-}tr 100 100 {-}overwrite\textquotesingle{}}\NormalTok{))}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

We can now read the downscaled predictions, and merge them using the prediction errors as weights (weighted average per pixel):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sel.pix }\OtherTok{=}\NormalTok{ edgeroi.ocF}\SpecialCharTok{$}\NormalTok{pred}\SpecialCharTok{@}\NormalTok{grid.index}
\NormalTok{edgeroi.ocF}\SpecialCharTok{$}\NormalTok{pred}\SpecialCharTok{$}\NormalTok{responseC }\OtherTok{=} \FunctionTok{readGDAL}\NormalTok{(}\StringTok{"output/edgeroi/pred\_oc\_250m\_100m.tif"}\NormalTok{)}\SpecialCharTok{$}\NormalTok{band1[sel.pix]}
\CommentTok{\#\textgreater{} output/edgeroi/pred\_oc\_250m\_100m.tif has GDAL driver GTiff }
\CommentTok{\#\textgreater{} and has 321 rows and 476 columns}
\NormalTok{edgeroi.ocF}\SpecialCharTok{$}\NormalTok{pred}\SpecialCharTok{$}\NormalTok{model.errorC }\OtherTok{=} \FunctionTok{readGDAL}\NormalTok{(}\StringTok{"output/edgeroi/pred\_oc\_250m\_100m\_pe.tif"}\NormalTok{)}\SpecialCharTok{$}\NormalTok{band1[sel.pix]}
\CommentTok{\#\textgreater{} output/edgeroi/pred\_oc\_250m\_100m\_pe.tif has GDAL driver GTiff }
\CommentTok{\#\textgreater{} and has 321 rows and 476 columns}
\NormalTok{X }\OtherTok{=} \FunctionTok{comp.var}\NormalTok{(edgeroi.ocF}\SpecialCharTok{$}\NormalTok{pred}\SpecialCharTok{@}\NormalTok{data, }\AttributeTok{r1=}\StringTok{"response"}\NormalTok{, }\AttributeTok{r2=}\StringTok{"responseC"}\NormalTok{, }\AttributeTok{v1=}\StringTok{"model.error"}\NormalTok{, }\AttributeTok{v2=}\StringTok{"model.errorC"}\NormalTok{)}
\NormalTok{edgeroi.ocF}\SpecialCharTok{$}\NormalTok{pred}\SpecialCharTok{$}\NormalTok{responseF }\OtherTok{=}\NormalTok{ X}\SpecialCharTok{$}\NormalTok{response}
\NormalTok{out.tif }\OtherTok{=} \StringTok{"output/edgeroi/pred\_oc\_100m\_merged.tif"}
\ControlFlowTok{if}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{file.exists}\NormalTok{(out.tif))\{}
  \FunctionTok{writeGDAL}\NormalTok{(edgeroi.ocF}\SpecialCharTok{$}\NormalTok{pred[}\StringTok{"responseF"}\NormalTok{], out.tif, }\AttributeTok{options =} \FunctionTok{c}\NormalTok{(}\StringTok{"COMPRESS=DEFLATE"}\NormalTok{))}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

The final map of predictions is a combination of the two independently produced predictions \citep{hengl2021african}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(}\FunctionTok{raster}\NormalTok{(edgeroi.ocF}\SpecialCharTok{$}\NormalTok{pred[}\StringTok{"responseF"}\NormalTok{]), }\AttributeTok{col=}\NormalTok{R\_pal[[}\StringTok{"rainbow\_75"}\NormalTok{]][}\DecValTok{4}\SpecialCharTok{:}\DecValTok{20}\NormalTok{],}
  \AttributeTok{main=}\StringTok{"Merged predictions spLearner"}\NormalTok{, }\AttributeTok{axes=}\ConstantTok{FALSE}\NormalTok{, }\AttributeTok{box=}\ConstantTok{FALSE}\NormalTok{)}
\FunctionTok{points}\NormalTok{(edgeroi.sp, }\AttributeTok{pch=}\StringTok{"+"}\NormalTok{, }\AttributeTok{cex=}\NormalTok{.}\DecValTok{8}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{multiscale-models_files/figure-latex/map-2scale-1} 

}

\caption{Merged predictions (coarse+fine scale) of SOC content at 100-m.}\label{fig:map-2scale}
\end{figure}

To merge the prediction errors, we use the pooled variance formula \citep{rudmin2010calculating}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{comp.var}
\CommentTok{\#\textgreater{} function (x, r1, r2, v1, v2) }
\CommentTok{\#\textgreater{} \{}
\CommentTok{\#\textgreater{}     r = rowSums(x[, c(r1, r2)] * 1/(x[, c(v1, v2)]\^{}2))/rowSums(1/(x[, }
\CommentTok{\#\textgreater{}         c(v1, v2)]\^{}2))}
\CommentTok{\#\textgreater{}     v = sqrt(rowMeans(x[, c(r1, r2)]\^{}2 + x[, c(v1, v2)]\^{}2) {-} }
\CommentTok{\#\textgreater{}         rowMeans(x[, c(r1, r2)])\^{}2)}
\CommentTok{\#\textgreater{}     return(data.frame(response = r, stdev = v))}
\CommentTok{\#\textgreater{} \}}
\CommentTok{\#\textgreater{} \textless{}bytecode: 0x39f5e3f8\textgreater{}}
\NormalTok{edgeroi.ocF}\SpecialCharTok{$}\NormalTok{pred}\SpecialCharTok{$}\NormalTok{model.errorF }\OtherTok{=}\NormalTok{ X}\SpecialCharTok{$}\NormalTok{stdev}
\NormalTok{out.tif }\OtherTok{=} \StringTok{"output/edgeroi/pred\_oc\_100m\_merged\_pe.tif"}
\ControlFlowTok{if}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{file.exists}\NormalTok{(out.tif))\{}
  \FunctionTok{writeGDAL}\NormalTok{(edgeroi.ocF}\SpecialCharTok{$}\NormalTok{pred[}\StringTok{"model.errorF"}\NormalTok{], out.tif, }\AttributeTok{options =} \FunctionTok{c}\NormalTok{(}\StringTok{"COMPRESS=DEFLATE"}\NormalTok{))}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

So in summary, merging multi-scale predictions is a straight forward process,
but it assumes that the reliable prediction errors are available for both coarse and fine scale predictions.
The pooled variance might show higher errors where predictions between independent
models differ significantly and this is correct. The 2-scale Ensemble Machine
Learning method of Predictive Soil Mapping was used, for example, to produce
predictions of \href{https://www.isda-africa.com/isdasoil/}{soil properties and nutrients of Africa at 30-m spatial resolution} \citep{hengl2021african}.

\hypertarget{summary}{%
\chapter{Summary}\label{summary}}

You are reading the work-in-progress Spatial and spatiotemporal interpolation using Ensemble Machine Learning. This chapter is currently draft version, a peer-review publication is pending. You can find the polished first edition at \url{https://opengeohub.github.io/spatial-prediction-eml/}.

The tutorial above demonstrates how to use Ensemble Machine Learning for predictive
mapping going from numeric 2D, to factor and to 3D variables. Have in mind that
the examples shown are based on relatively small datasets, but can still become
computational if you add even more learners. In principle we do not recommend:

\begin{itemize}
\tightlist
\item
  adding learners that are significantly less accurate than your best learners
  (i.e.~focusing on the top 4--5 best performing learners),\\
\item
  fitting EML for \textless50--100 training points,
\item
  fitting EML for spatial interpolation where points are heavily spatially clustered,
\item
  using landmap package with large datasets,
\end{itemize}

For derivation of prediction error and prediction interval we recommend using the
method of \citet{lu2021unified}. This method by default produces three measures of uncertainty:

\begin{itemize}
\tightlist
\item
  Root Mean Square Prediction Error (RMSPE) = the estimated conditional mean squared prediction errors of the random forest predictions,\\
\item
  bias = the estimated conditional biases of the random forest predictions,\\
\item
  lower and upper bounds / prediction intervals for a given probability e.g.~0.05 for the 95\% probability interval,
\end{itemize}

You can also follow an introduction to Ensemble Machine Learning from the \href{https://av.tib.eu/series/1146/opendatascience+europe+workshop+2021}{Open Data Science Europe workshop video recordings}.

Please note that the mlr package is discontinued, so some of the example above might become unstable with time. We are working on migrating the code in the landmap package to make the \texttt{train.spLearner} function work with the new \href{https://mlr3.mlr-org.com/}{mlr3 package}.

If you have a dataset that you have used to test Ensemble Machine Learning, please come back to us and share your experiences by posting \href{https://github.com/Envirometrix/landmap/issues}{an issue} and/or providing a screenshot of your results.

\hypertarget{references}{%
\chapter{References}\label{references}}

  \bibliography{./tex/refs.bib}

\backmatter
\printindex

\end{document}
